{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method (DGM)\n",
    "### S1 = sigma(w1*x + b1)  Z(l) = sigma(u*x + w*S + b) l=1,...,L  G(l) = sigma(u*x + w*S + b) l=1,...,L  \n",
    "### R(l) = sigma(u*x + w*S + b) l=1,...,L   H(l) = sigma(u*x + w*(S Hadamard R) + b)  l=1,...,L  \n",
    "### S(L+1) = (1-G) Hadamard H + Z Hadamard S  f = w*S(L+1) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of imput data  \n",
    "**output_dim:** number of output for LSTM layer  \n",
    "**trans1, trans2 (str):** activation functions used inside the layer;  \n",
    "one of: \"tanh\"(default), \"relu\" or \"sigmoid\"  \n",
    "**u vectors:** weighting vectors for inputs original inputs x  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**S:** output of previous layer  \n",
    "**X:** data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-like layer used in DGM\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        \n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    def call(self, S, X):\n",
    "\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of input data  \n",
    "**output_dim:** number of outputs for dense layer  \n",
    "**transformation:** activation function used inside the layer; using None is equivalent to the identity map  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**X:** input to layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer(dense)\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        \n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    def call(self,X):\n",
    "        \n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_layers:** number of intermediate LSTM layers  \n",
    "**input_dim:** spaital dimension of input data (excludes time dimension)  \n",
    "**final_trans:** transformation used in final layer\n",
    "define initial layer as fully connected\n",
    "to account for time inputs we use input_dim+1 as the input dimension\n",
    "**t:** sampled time inputs  \n",
    "**x:** sampled space inputs  \n",
    "Run the DGM model and obtain fitted function value at the inputs (t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture used in DGM\n",
    "\n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        \n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1))\n",
    "        \n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    def call(self,t,x):\n",
    "\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process parameters（Ornstein-Uhlenbeck Process）\n",
    "kappa = 0\n",
    "theta = 0.5\n",
    "sigma = 2\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "# tenminal time\n",
    "T = 1.0\n",
    "\n",
    "# bounds of sampling region for space dimension, i.e. sampling will be done on\n",
    "# [multipliter*Xlow, multiplier*Xhigh]\n",
    "Xlow = -4.0\n",
    "Xhigh = 4.0\n",
    "x_multiplier = 2.0\n",
    "t_multiplier = 1.5\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages = 500\n",
    "steps_per_sample = 10\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_t = 5\n",
    "nSim_x_interior = 50\n",
    "nSim_x_initial = 50\n",
    "\n",
    "# Save options\n",
    "saveName = 'FokkerPlanck'\n",
    "saveFigure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OU Simulation function(Ornstein-Uhlenbeck Process)**  \n",
    "Simulate end point of Ornstein-Uhlenbeck process with normally distributed random starting value  \n",
    "**alpha:** mean of random starting value  \n",
    "**beta:** standard deviation of random starting value   \n",
    "**theta:** mean reversion level    \n",
    "**kappa:** mean reversion rate  \n",
    "**sigma:** volatility  \n",
    "**nSim:** number of simulations  \n",
    "**T:** terminal time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(alpha, beta, theta, kappa, sigma, nSim, T):\n",
    "    \n",
    "    # simulate initial point based on normal distribution\n",
    "    X0 = np.random.normal(loc = alpha, scale = beta, size = nSim)\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta + (X0 - theta) * np.exp(-kappa * T)\n",
    "    v = np.sqrt(sigma**2 / (2 * kappa) * (1 - np.exp(-2*kappa*T)))\n",
    "    \n",
    "    # simulate endpoint\n",
    "    Xt = np.random.normal(m,v)    \n",
    "    \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample time-space points from the function's domain;  \n",
    "point are sampled uniformly on the interior of the domain, at the initial/terminal time points  \n",
    "and along the spatial boundary at different time points.  \n",
    "**nSim_t:** number of (interior) time points to sample  \n",
    "**nSim_x_interior:** number of space points in the interior of the function's domain to sample  \n",
    "**nSim_x_initial:** number of space points at initial time to sample (initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function - random sample time-space pairs\n",
    "def sampler(nSim_t, nSim_x_interior, nSim_x_initial):\n",
    "    \n",
    "    # Sampler1: domain interior\n",
    "    t = np.random.uniform(low=0, high=T*t_multiplier, size=[nSim_t, 1])\n",
    "    x_interior = np.random.uniform(low=Xlow*x_multiplier, high=Xhigh*x_multiplier, size=[nSim_x_interior, 1])\n",
    "    \n",
    "    # Sampler: spatial boundary\n",
    "    # no spatial boundary condition for this problem \n",
    "    \n",
    "    # Sampler3: initial/terminal condition\n",
    "    x_initial = np.random.uniform(low=Xlow*1.5, high=Xhigh*1.5, size = [nSim_x_initial, 1])\n",
    "    \n",
    "    return t, x_interior, x_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute total loss for training.     \n",
    "The loss is based o the PDE satisfied by the negative-exponential of the density and NOT the density   \n",
    "itself, i.e. the u(t,x) in p(t,x) = exp(-u(t,x)) / c(t) where p is the density and c is the normalization constant.    \n",
    "**model:** DGM model object   \n",
    "**t:** sampled (interior) time points  \n",
    "**x_interior:** sampled space points in the interior of the function's domain   \n",
    "**x_initial:** sampled space points at initial time   \n",
    "**nSim_t:** number of (interior) time points sampled (size of t)  \n",
    "**alpha:** mean of normal distribution for process staring value  \n",
    "**beta:** standard deviation of normal distribution for process starting value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t, x_interior, x_initial, nSim_t, alpha, beta):\n",
    "    \n",
    "    # Loss term1: PDE\n",
    "    \n",
    "    # initialize vector of losses\n",
    "    losses_u = []\n",
    "    \n",
    "    # for each simulated interior time point\n",
    "    for tIndex in range(nSim_t):\n",
    "        \n",
    "        curr_t = t[tIndex]\n",
    "        t_vector = curr_t * tf.ones_like(x_interior)\n",
    "        \n",
    "        u    = model.call(t_vector, x_interior)\n",
    "        u_t  = tf.gradients(u, t_vector)[0]\n",
    "        u_x  = tf.gradients(u, x_interior)[0]\n",
    "        u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "        psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "        psi = tf.reduce_sum( u_t*tf.exp(-u) ) / psi_denominator\n",
    "\n",
    "        # PDE differential operator\n",
    "        diff_f = -u_t + kappa - kappa*(x_interior- theta)*u_x - 0.5*sigma**2*(-u_xx + u_x**2) + psi\n",
    "        \n",
    "        # compute L2-norm of differential operator and attach to vector of losses\n",
    "        currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "        losses_u.append(currLoss)\n",
    "    \n",
    "    # average losses across sample time points \n",
    "    L1 = tf.add_n(losses_u) / nSim_t\n",
    "    \n",
    "    # Loss term2: boundary condition\n",
    "    # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term3: initial condition\n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0 i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    fitted_pdf = model.call(0*tf.ones_like(x_initial), x_initial)\n",
    "    \n",
    "    target_pdf  = 0.5*(x_initial - alpha)**2 / (beta**2)\n",
    "    \n",
    "    # average L2 error for initial distribution\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "    return L1, L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input(time, space domain interior, space domain at initial time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Set up network\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "t_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_initial_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_tnsr, x_interior_tnsr, x_initial_tnsr, nSim_t, alpha, beta)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "u = model.call(t_tnsr, x_interior_tnsr)\n",
    "p_unnorm = tf.exp(-u)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.5).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.173464 2.6470678 6.526396 0\n",
      "11.255162 1.965284 9.289878 1\n",
      "6.2199855 0.9154019 5.3045835 2\n",
      "16.401844 7.7372546 8.664589 3\n",
      "5.0454373 1.5063931 3.5390441 4\n",
      "3.9453137 0.84130067 3.104013 5\n",
      "9.035099 2.6405306 6.394568 6\n",
      "4.893872 0.7210587 4.172813 7\n",
      "5.4506354 0.9358942 4.5147414 8\n",
      "7.3117437 1.6832255 5.628518 9\n",
      "3.937552 1.109752 2.8277998 10\n",
      "7.664158 1.4594176 6.2047405 11\n",
      "3.100573 0.6282829 2.4722903 12\n",
      "4.223132 1.4949458 2.7281861 13\n",
      "1.181602 0.2501031 0.9314989 14\n",
      "0.5293205 0.11886418 0.4104563 15\n",
      "7.606311 1.801052 5.8052588 16\n",
      "2.9504004 0.32688725 2.623513 17\n",
      "4.3400083 1.2088358 3.1311724 18\n",
      "1.2224001 0.118965864 1.1034342 19\n",
      "10.266661 2.7772803 7.48938 20\n",
      "5.590842 1.3296676 4.261174 21\n",
      "5.7672076 1.450757 4.3164506 22\n",
      "4.3457503 1.7995745 2.5461757 23\n",
      "1.821093 0.23954831 1.5815446 24\n",
      "7.46653 1.9403213 5.5262084 25\n",
      "7.295567 2.4022713 4.893296 26\n",
      "5.0430636 1.3607558 3.682308 27\n",
      "4.156294 0.86611813 3.2901757 28\n",
      "8.530575 3.6486282 4.881946 29\n",
      "3.6811037 0.74570596 2.9353979 30\n",
      "1.4868371 0.25358364 1.2332535 31\n",
      "4.6991754 1.0372916 3.6618836 32\n",
      "3.7220814 0.8579214 2.86416 33\n",
      "9.614775 2.8793297 6.7354455 34\n",
      "1.7885729 0.43968678 1.3488861 35\n",
      "1.9028862 0.67969817 1.2231879 36\n",
      "0.5171396 0.10172083 0.41541877 37\n",
      "5.964843 1.5270215 4.4378214 38\n",
      "1.9411325 0.1081323 1.8330003 39\n",
      "6.615513 1.3948618 5.220651 40\n",
      "3.9193797 0.85521525 3.0641644 41\n",
      "2.0986464 0.44233927 1.6563072 42\n",
      "2.16823 0.5348982 1.6333318 43\n",
      "1.8887564 0.42585382 1.4629025 44\n",
      "3.0799387 0.77959794 2.3003407 45\n",
      "2.8038545 0.6816198 2.1222346 46\n",
      "3.4132063 1.5123363 1.9008701 47\n",
      "2.0571034 0.54091734 1.5161861 48\n",
      "0.9551852 0.18840922 0.76677597 49\n",
      "0.429755 0.10425966 0.32549533 50\n",
      "7.204611 1.366188 5.838423 51\n",
      "3.7109911 0.85377866 2.8572125 52\n",
      "6.9488783 2.8522985 4.09658 53\n",
      "4.126652 0.70237267 3.4242792 54\n",
      "3.7323666 0.8199278 2.9124389 55\n",
      "1.5729077 0.25456357 1.3183441 56\n",
      "2.2110307 0.44830266 1.762728 57\n",
      "3.7777967 0.90497315 2.8728235 58\n",
      "1.2810686 0.29302344 0.9880451 59\n",
      "2.3476155 0.6624543 1.6851611 60\n",
      "3.7981896 1.0662526 2.731937 61\n",
      "1.6111296 0.26914895 1.3419807 62\n",
      "1.2370808 0.31997794 0.9171028 63\n",
      "4.563692 1.5814204 2.9822714 64\n",
      "1.6625072 0.2772586 1.3852485 65\n",
      "3.8763576 0.8355776 3.0407798 66\n",
      "0.6367103 0.074010506 0.5626998 67\n",
      "1.6139868 0.32986966 1.2841172 68\n",
      "1.4263048 0.29029804 1.1360067 69\n",
      "1.1156061 0.23581567 0.8797904 70\n",
      "3.2672098 1.9369967 1.3302131 71\n",
      "2.2026436 0.6204733 1.5821704 72\n",
      "1.1604168 0.21095537 0.94946146 73\n",
      "1.8653929 0.5580596 1.3073334 74\n",
      "0.81665266 0.14595962 0.67069304 75\n",
      "0.67048866 0.15114371 0.5193449 76\n",
      "0.56344426 0.09991896 0.46352533 77\n",
      "1.0840309 0.27190897 0.81212187 78\n",
      "1.9631741 1.0470504 0.91612375 79\n",
      "5.578577 2.8165255 2.7620516 80\n",
      "2.75578 0.43861386 2.317166 81\n",
      "1.4160757 0.23537801 1.1806977 82\n",
      "0.5925133 0.12018182 0.4723315 83\n",
      "0.6840378 0.18429238 0.49974546 84\n",
      "2.9635015 0.38999236 2.5735092 85\n",
      "0.9798833 0.21146193 0.76842135 86\n",
      "0.6558258 0.16615288 0.4896729 87\n",
      "0.5558335 0.07018811 0.4856454 88\n",
      "0.3498859 0.050039686 0.29984623 89\n",
      "2.8423615 0.82347786 2.0188837 90\n",
      "2.181005 0.6323559 1.5486492 91\n",
      "1.6496242 0.3281125 1.3215117 92\n",
      "0.9092226 0.12483716 0.78438544 93\n",
      "1.3942058 0.26598582 1.12822 94\n",
      "0.87737584 0.1554868 0.721889 95\n",
      "0.33736697 0.058438342 0.27892864 96\n",
      "0.9712219 0.2978044 0.67341757 97\n",
      "1.5278199 0.5733017 0.9545182 98\n",
      "0.8130841 0.110541694 0.7025424 99\n",
      "0.42369133 0.11011412 0.3135772 100\n",
      "0.26802075 0.052542735 0.215478 101\n",
      "0.20765936 0.070602655 0.13705671 102\n",
      "0.616104 0.11864324 0.49746078 103\n",
      "4.266257 1.1862293 3.0800276 104\n",
      "2.8579957 0.49938306 2.3586128 105\n",
      "1.10161 0.23369923 0.8679107 106\n",
      "0.7059462 0.12604454 0.5799017 107\n",
      "0.86279637 0.24648528 0.6163111 108\n",
      "0.39258212 0.058605827 0.3339763 109\n",
      "0.34123474 0.07807707 0.26315767 110\n",
      "0.28069246 0.034463346 0.24622913 111\n",
      "0.73383224 0.2802572 0.45357504 112\n",
      "2.6560383 0.5465342 2.109504 113\n",
      "1.3148216 0.30816975 1.0066519 114\n",
      "0.4060208 0.065089755 0.34093103 115\n",
      "0.8835468 0.26107237 0.6224745 116\n",
      "2.3677273 0.7710237 1.5967035 117\n",
      "0.44958663 0.07995311 0.36963353 118\n",
      "0.84034383 0.20853487 0.631809 119\n",
      "0.3689676 0.10917725 0.25979036 120\n",
      "0.25786906 0.037715852 0.22015321 121\n",
      "5.1125197 1.366947 3.7455726 122\n",
      "1.8728626 0.41883874 1.4540238 123\n",
      "1.9144654 0.4261611 1.4883043 124\n",
      "0.99278593 0.19472896 0.79805696 125\n",
      "1.437989 0.35080186 1.0871872 126\n",
      "0.3559303 0.07269144 0.28323886 127\n",
      "3.8364787 1.288884 2.5475948 128\n",
      "2.0082529 0.2682322 1.7400206 129\n",
      "1.1512885 0.34816003 0.8031285 130\n",
      "0.90102106 0.16268773 0.73833334 131\n",
      "0.6198323 0.09388961 0.5259427 132\n",
      "0.35770416 0.047245055 0.3104591 133\n",
      "0.40878505 0.07065126 0.33813378 134\n",
      "0.39758134 0.06287898 0.33470237 135\n",
      "0.51827556 0.19167577 0.32659978 136\n",
      "0.48648074 0.08651256 0.39996818 137\n",
      "0.21262036 0.060580313 0.15204005 138\n",
      "0.15263537 0.034151435 0.11848393 139\n",
      "0.15996927 0.019866606 0.14010267 140\n",
      "0.16211401 0.052075077 0.110038936 141\n",
      "2.3027296 0.3580458 1.9446838 142\n",
      "0.54130584 0.064584725 0.4767211 143\n",
      "0.5231968 0.15065496 0.37254187 144\n",
      "0.20179445 0.019948764 0.18184568 145\n",
      "2.3411338 0.26713714 2.0739968 146\n",
      "1.0689375 0.2265671 0.84237045 147\n",
      "0.6205102 0.10498994 0.5155203 148\n",
      "0.7453913 0.25878143 0.48660988 149\n",
      "0.42719194 0.15611514 0.2710768 150\n",
      "1.8153887 0.93613595 0.8792528 151\n",
      "0.43365616 0.07250811 0.36114803 152\n",
      "0.3538605 0.04763208 0.30622843 153\n",
      "0.39396733 0.11595052 0.2780168 154\n",
      "0.8610488 0.24811931 0.6129295 155\n",
      "0.30204388 0.030264635 0.27177924 156\n",
      "0.15407917 0.027134959 0.12694421 157\n",
      "0.51981217 0.13428672 0.38552544 158\n",
      "0.6932196 0.22832043 0.46489918 159\n",
      "0.20363507 0.04378633 0.15984873 160\n",
      "0.48156756 0.13620289 0.3453647 161\n",
      "1.3837147 0.7824151 0.60129964 162\n",
      "0.33003235 0.059237044 0.27079532 163\n",
      "0.51467556 0.28688288 0.2277927 164\n",
      "0.43886042 0.102209285 0.33665115 165\n",
      "0.44440246 0.14349599 0.30090645 166\n",
      "0.30186182 0.037497487 0.26436433 167\n",
      "0.13484842 0.022992518 0.111855894 168\n",
      "0.22625148 0.061946608 0.16430488 169\n",
      "0.3052774 0.1088578 0.1964196 170\n",
      "0.10926078 0.028070632 0.08119015 171\n",
      "1.0429665 0.34549734 0.6974692 172\n",
      "0.2587647 0.01909732 0.2396674 173\n",
      "0.21767119 0.03856804 0.17910315 174\n",
      "1.2191138 0.25926614 0.9598476 175\n",
      "0.53065896 0.1166894 0.41396958 176\n",
      "1.8517501 1.2441998 0.6075504 177\n",
      "0.45163488 0.08096778 0.3706671 178\n",
      "0.22603866 0.023452284 0.20258638 179\n",
      "0.14849932 0.0098842485 0.13861507 180\n",
      "0.1274986 0.017181149 0.11031745 181\n",
      "2.7211697 0.4193073 2.3018625 182\n",
      "1.1159557 0.13876131 0.9771944 183\n",
      "1.0234807 0.1805042 0.8429764 184\n",
      "0.67254835 0.13229208 0.54025626 185\n",
      "0.73890436 0.21141005 0.5274943 186\n",
      "0.28730777 0.05751381 0.22979395 187\n",
      "1.9611769 0.8838685 1.0773083 188\n",
      "0.49898514 0.020298941 0.4786862 189\n",
      "0.41292992 0.079909675 0.33302024 190\n",
      "0.43477646 0.07048231 0.36429414 191\n",
      "0.40571123 0.10160015 0.3041111 192\n",
      "0.6374193 0.1865768 0.4508425 193\n",
      "0.34412253 0.066648826 0.2774737 194\n",
      "0.2024308 0.041991398 0.1604394 195\n",
      "1.1299689 0.16038802 0.9695808 196\n",
      "0.2979616 0.02935606 0.26860553 197\n",
      "9.279434 1.3882722 7.891162 198\n",
      "4.3860784 0.5991358 3.7869427 199\n",
      "2.8184447 0.34243894 2.4760058 200\n",
      "2.68317 0.52570057 2.1574695 201\n",
      "2.570272 0.564862 2.00541 202\n",
      "0.95524454 0.12409996 0.8311446 203\n",
      "0.72601056 0.12127866 0.6047319 204\n",
      "1.7635589 0.7136742 1.0498847 205\n",
      "0.55049366 0.071829006 0.47866467 206\n",
      "0.33383304 0.03527069 0.29856235 207\n",
      "0.4830865 0.063070856 0.42001563 208\n",
      "0.22137389 0.037889104 0.18348478 209\n",
      "2.207339 0.46105313 1.7462859 210\n",
      "1.2049842 0.20995839 0.9950258 211\n",
      "1.5768718 0.39430574 1.182566 212\n",
      "0.56389296 0.098271236 0.4656217 213\n",
      "0.71541214 0.10726949 0.6081427 214\n",
      "0.46774805 0.054932546 0.4128155 215\n",
      "0.29741913 0.04726931 0.25014982 216\n",
      "0.50312316 0.091201834 0.41192135 217\n",
      "0.9503207 0.32363674 0.626684 218\n",
      "0.2601306 0.062181134 0.19794947 219\n",
      "2.2066715 0.6678154 1.5388561 220\n",
      "2.4751043 0.67671925 1.798385 221\n",
      "1.7136002 0.35502082 1.3585794 222\n",
      "1.2752168 0.2078445 1.0673723 223\n",
      "0.582072 0.10931393 0.4727581 224\n",
      "0.3666741 0.09975954 0.26691455 225\n",
      "0.5642943 0.13401847 0.43027583 226\n",
      "0.36145067 0.081481546 0.27996913 227\n",
      "0.5047966 0.13160396 0.37319267 228\n",
      "0.18682726 0.04985724 0.13697001 229\n",
      "0.7812621 0.16820495 0.61305714 230\n",
      "0.20927416 0.042541306 0.16673285 231\n",
      "0.24358615 0.068497024 0.17508914 232\n",
      "2.3040576 0.7011155 1.6029422 233\n",
      "0.7098923 0.15306996 0.5568223 234\n",
      "0.32694322 0.06914596 0.25779727 235\n",
      "0.23160625 0.025633315 0.20597292 236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14808634 0.017706413 0.13037993 237\n",
      "0.0990921 0.02186186 0.077230245 238\n",
      "0.9030808 0.20159006 0.70149076 239\n",
      "0.4647325 0.08778799 0.3769445 240\n",
      "0.9765459 0.5273525 0.44919336 241\n",
      "0.123984456 0.024420729 0.099563725 242\n",
      "0.057938594 0.008073875 0.04986472 243\n",
      "0.32987005 0.056266714 0.27360332 244\n",
      "0.49350935 0.12677 0.36673936 245\n",
      "0.18897738 0.045283604 0.14369377 246\n",
      "0.298889 0.04711662 0.2517724 247\n",
      "0.14942187 0.046915025 0.102506846 248\n",
      "0.42512187 0.1229826 0.30213928 249\n",
      "0.6118756 0.17902319 0.4328524 250\n",
      "0.27188194 0.030924043 0.24095789 251\n",
      "11.566265 2.7503154 8.815949 252\n",
      "1.9997075 0.27141038 1.7282971 253\n",
      "4.4913945 0.57865614 3.9127383 254\n",
      "2.5340912 0.25232658 2.2817647 255\n",
      "2.729051 0.5500053 2.1790457 256\n",
      "3.1829991 0.7389075 2.4440916 257\n",
      "0.5715744 0.09185977 0.47971463 258\n",
      "0.37330195 0.08271738 0.29058456 259\n",
      "2.2852712 1.1881644 1.0971068 260\n",
      "0.23442855 0.03277577 0.20165278 261\n",
      "0.43016276 0.056225564 0.3739372 262\n",
      "2.992556 0.6407419 2.3518143 263\n",
      "2.026101 0.6712516 1.3548496 264\n",
      "0.27178237 0.055611648 0.21617073 265\n",
      "0.31192893 0.014553166 0.29737577 266\n",
      "0.31640399 0.049029943 0.26737404 267\n",
      "0.38185927 0.03628763 0.34557164 268\n",
      "1.0100065 0.23139839 0.77860814 269\n",
      "0.19580495 0.032041073 0.16376388 270\n",
      "0.32546493 0.06675226 0.25871268 271\n",
      "4.4796634 1.0561322 3.4235313 272\n",
      "1.5576539 0.26410988 1.293544 273\n",
      "1.0055777 0.24722777 0.7583499 274\n",
      "3.1116455 0.7255861 2.3860593 275\n",
      "2.304503 0.29589078 2.0086122 276\n",
      "1.0665326 0.17093487 0.89559776 277\n",
      "1.5918931 0.32170728 1.2701858 278\n",
      "1.5300575 0.9124014 0.6176562 279\n",
      "1.3269064 0.28466454 1.0422419 280\n",
      "0.32869977 0.0598031 0.26889667 281\n",
      "0.27993208 0.07634769 0.2035844 282\n",
      "2.5494862 1.0438884 1.5055978 283\n",
      "0.4643439 0.05087639 0.41346753 284\n",
      "0.224653 0.013544001 0.21110901 285\n",
      "0.71717334 0.23540281 0.48177052 286\n",
      "1.3213218 0.38395277 0.9373691 287\n",
      "0.53990793 0.08730906 0.45259887 288\n",
      "0.4243669 0.04932345 0.37504345 289\n",
      "0.19427773 0.016092926 0.1781848 290\n",
      "0.23847228 0.062556244 0.17591605 291\n",
      "0.1938553 0.024516981 0.16933832 292\n",
      "0.07943371 0.015136743 0.06429697 293\n",
      "1.0459541 0.21663383 0.8293203 294\n",
      "0.31996155 0.09654577 0.22341579 295\n",
      "0.44206637 0.15363573 0.28843066 296\n",
      "0.16043305 0.025679631 0.13475342 297\n",
      "0.08822972 0.019739792 0.06848993 298\n",
      "0.5879906 0.19484694 0.39314362 299\n",
      "0.16790654 0.027287615 0.14061892 300\n",
      "0.2017903 0.059608925 0.14218138 301\n",
      "0.47248363 0.21421154 0.2582721 302\n",
      "0.3062413 0.047074843 0.25916645 303\n",
      "0.20158313 0.027480884 0.17410225 304\n",
      "0.3532369 0.09994741 0.2532895 305\n",
      "0.2945556 0.091916345 0.20263925 306\n",
      "0.15302272 0.012880817 0.1401419 307\n",
      "0.47587922 0.1145281 0.36135113 308\n",
      "0.18481249 0.018119216 0.16669327 309\n",
      "0.1883347 0.05711932 0.13121538 310\n",
      "0.10858034 0.016984357 0.091595985 311\n",
      "2.3234444 0.6859671 1.6374773 312\n",
      "0.5038997 0.10059668 0.40330303 313\n",
      "0.9042508 0.16477989 0.7394709 314\n",
      "0.29847997 0.02972101 0.26875895 315\n",
      "0.2884399 0.056376617 0.2320633 316\n",
      "0.8340661 0.18397872 0.65008736 317\n",
      "0.2396892 0.08373048 0.15595873 318\n",
      "0.20372953 0.023451302 0.18027823 319\n",
      "0.17059802 0.06752964 0.103068374 320\n",
      "0.08234757 0.030655876 0.051691696 321\n",
      "9.088749 1.0391321 8.049617 322\n",
      "4.319357 0.64566183 3.673695 323\n",
      "1.8589942 0.29579845 1.5631958 324\n",
      "1.2527915 0.3172228 0.9355687 325\n",
      "1.225723 0.63293463 0.59278834 326\n",
      "7.1222687 1.5384127 5.583856 327\n",
      "2.2163017 0.31141362 1.904888 328\n",
      "1.7288764 0.31208178 1.4167945 329\n",
      "3.5337238 0.4643464 3.0693774 330\n",
      "1.6741276 0.38280016 1.2913274 331\n",
      "3.5117905 0.8071594 2.704631 332\n",
      "0.65622073 0.14664567 0.50957507 333\n",
      "1.2389187 0.2338203 1.0050983 334\n",
      "0.67133075 0.11636887 0.55496186 335\n",
      "2.2464104 0.4893403 1.7570702 336\n",
      "1.380145 0.2290231 1.1511219 337\n",
      "0.6358309 0.14067924 0.49515164 338\n",
      "0.84978986 0.20163324 0.6481566 339\n",
      "0.5178497 0.08549719 0.43235248 340\n",
      "3.51706 0.63131803 2.885742 341\n",
      "0.368052 0.04240727 0.32564473 342\n",
      "4.6279078 0.8970372 3.7308707 343\n",
      "4.026776 0.7983244 3.2284513 344\n",
      "2.5256004 0.5298115 1.9957889 345\n",
      "0.88752675 0.15635549 0.73117125 346\n",
      "1.6657035 0.31026816 1.3554354 347\n",
      "0.7610646 0.14968653 0.6113781 348\n",
      "0.97637755 0.19734079 0.77903676 349\n",
      "0.6030062 0.09295392 0.51005226 350\n",
      "0.5335068 0.083904654 0.44960216 351\n",
      "0.8943049 0.36793938 0.52636546 352\n",
      "1.3646353 0.28756425 1.0770711 353\n",
      "0.98105717 0.2791371 0.7019201 354\n",
      "0.39700368 0.07581491 0.32118878 355\n",
      "0.2957337 0.075019225 0.22071448 356\n",
      "2.7011 0.5481253 2.1529748 357\n",
      "1.2049867 0.21841784 0.9865688 358\n",
      "0.44774723 0.09635539 0.35139182 359\n",
      "0.97226274 0.1679279 0.8043348 360\n",
      "0.5812342 0.18885162 0.39238262 361\n",
      "2.7984228 1.1996953 1.5987275 362\n",
      "1.1150638 0.31923997 0.7958238 363\n",
      "1.6731768 0.5800496 1.0931273 364\n",
      "2.9471822 1.6770321 1.27015 365\n",
      "0.79420364 0.13628569 0.6579179 366\n",
      "0.3804893 0.06607999 0.3144093 367\n",
      "0.31904408 0.09298643 0.22605766 368\n",
      "1.3316329 0.22417109 1.1074618 369\n",
      "0.5327506 0.086519435 0.4462312 370\n",
      "0.98927665 0.2933058 0.69597083 371\n",
      "0.45519927 0.054876626 0.40032265 372\n",
      "0.45042437 0.065638214 0.38478616 373\n",
      "1.2723389 0.6023799 0.669959 374\n",
      "0.30188206 0.054383297 0.24749877 375\n",
      "0.29120874 0.042515457 0.24869329 376\n",
      "0.3828435 0.09212375 0.29071975 377\n",
      "1.2364433 0.4229506 0.8134927 378\n",
      "0.30360082 0.04010048 0.26350033 379\n",
      "0.45490468 0.097186446 0.35771823 380\n",
      "0.29033023 0.062103774 0.22822647 381\n",
      "0.26223043 0.036523875 0.22570656 382\n",
      "0.23924999 0.03970584 0.19954415 383\n",
      "0.4653945 0.12693273 0.3384618 384\n",
      "0.10808939 0.018888209 0.089201175 385\n",
      "0.25244677 0.10462072 0.14782605 386\n",
      "0.2277902 0.084361695 0.14342852 387\n",
      "1.3223444 0.38797697 0.9343674 388\n",
      "0.31806752 0.07573003 0.2423375 389\n",
      "0.20744649 0.035652816 0.17179367 390\n",
      "0.16370228 0.022993602 0.14070867 391\n",
      "1.7387275 0.56350446 1.175223 392\n",
      "0.48384365 0.07652402 0.40731964 393\n",
      "0.40384564 0.06487652 0.3389691 394\n",
      "1.7911158 0.7368224 1.0542934 395\n",
      "0.65865517 0.13879615 0.519859 396\n",
      "0.5253812 0.06797348 0.4574077 397\n",
      "0.2542456 0.025526231 0.22871937 398\n",
      "0.23430906 0.044060186 0.19024888 399\n",
      "0.47219986 0.09996264 0.3722372 400\n",
      "0.27358258 0.07576215 0.19782044 401\n",
      "0.16575198 0.0248128 0.14093918 402\n",
      "0.41025108 0.07608737 0.3341637 403\n",
      "0.3270973 0.08102823 0.24606907 404\n",
      "0.22364233 0.060597833 0.1630445 405\n",
      "0.21521586 0.07121775 0.1439981 406\n",
      "0.10289934 0.023968546 0.078930795 407\n",
      "0.098713726 0.019170009 0.07954372 408\n",
      "0.36559573 0.16877349 0.19682224 409\n",
      "0.05215881 0.0067112236 0.045447588 410\n",
      "0.18728033 0.0682434 0.119036935 411\n",
      "0.06287223 0.008560828 0.054311406 412\n",
      "0.08757329 0.031521168 0.056052126 413\n",
      "2.1130416 0.8423231 1.2707186 414\n",
      "0.32647657 0.03851641 0.28796017 415\n",
      "0.35294196 0.041963704 0.31097826 416\n",
      "0.27303356 0.059971895 0.21306168 417\n",
      "0.26202458 0.0271603 0.23486428 418\n",
      "0.13959073 0.015297941 0.12429278 419\n",
      "0.23875509 0.06261736 0.17613773 420\n",
      "2.4307613 0.84675467 1.5840065 421\n",
      "0.5651262 0.09015085 0.47497532 422\n",
      "0.4365319 0.089347415 0.34718448 423\n",
      "0.08355807 0.009811406 0.07374666 424\n",
      "0.34510097 0.10982633 0.23527464 425\n",
      "0.37185162 0.058542307 0.3133093 426\n",
      "0.18637848 0.036689818 0.14968866 427\n",
      "0.7960933 0.4646344 0.3314589 428\n",
      "0.109108746 0.024581287 0.08452746 429\n",
      "0.13376732 0.034851726 0.0989156 430\n",
      "2.7663147 2.1394231 0.6268916 431\n",
      "0.16877776 0.033668842 0.13510892 432\n",
      "0.6198451 0.24014135 0.37970376 433\n",
      "1.0755299 0.42643133 0.64909863 434\n",
      "0.13684617 0.020406315 0.11643986 435\n",
      "0.61774665 0.19112663 0.42662004 436\n",
      "0.20051679 0.024048554 0.17646824 437\n",
      "0.06982112 0.01042405 0.059397068 438\n",
      "0.12491239 0.025201371 0.099711016 439\n",
      "0.08793242 0.010744714 0.07718771 440\n",
      "0.06696452 0.007859802 0.05910472 441\n",
      "0.065199494 0.016171409 0.049028087 442\n",
      "0.149308 0.036521327 0.11278667 443\n",
      "1.6091311 0.31660065 1.2925304 444\n",
      "1.1906319 0.31373948 0.8768923 445\n",
      "0.43563548 0.081126474 0.354509 446\n",
      "0.34301156 0.046887923 0.29612362 447\n",
      "0.20457257 0.024408698 0.18016388 448\n",
      "0.15024328 0.027572272 0.122671016 449\n",
      "0.1370432 0.021629045 0.11541415 450\n",
      "0.0844623 0.022154098 0.062308203 451\n",
      "0.10438053 0.029066056 0.07531448 452\n",
      "11.412409 9.133593 2.278816 453\n",
      "1.033129 0.3090416 0.72408736 454\n",
      "1.008168 0.35279346 0.6553745 455\n",
      "0.28217867 0.029666806 0.25251186 456\n",
      "0.22927326 0.04306979 0.18620346 457\n",
      "3.9033723 1.315245 2.5881271 458\n",
      "1.9008636 0.5203094 1.3805542 459\n",
      "0.933719 0.0766576 0.8570614 460\n",
      "1.0192249 0.24018739 0.77903754 461\n",
      "3.0005097 0.6377252 2.3627846 462\n",
      "0.5789646 0.08937154 0.48959306 463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6470654 0.12895301 0.5181124 464\n",
      "0.24155354 0.014679469 0.22687407 465\n",
      "0.39834833 0.041250825 0.3570975 466\n",
      "2.3742876 1.576635 0.79765266 467\n",
      "0.24875887 0.034074154 0.21468471 468\n",
      "0.23333701 0.056341566 0.17699546 469\n",
      "1.2125416 0.2695923 0.94294924 470\n",
      "0.21928906 0.02420593 0.19508314 471\n",
      "0.21011917 0.044455603 0.16566357 472\n",
      "0.4903893 0.114625834 0.37576345 473\n",
      "0.2750396 0.04896253 0.22607708 474\n",
      "1.9529042 1.8160871 0.13681707 475\n",
      "0.35261828 0.082732834 0.26988545 476\n",
      "0.14045355 0.03413061 0.106322944 477\n",
      "0.17518935 0.04376131 0.13142803 478\n",
      "0.12249059 0.019231066 0.103259526 479\n",
      "6.8943157 1.6340439 5.2602715 480\n",
      "2.0837433 0.31152704 1.7722163 481\n",
      "1.7061903 0.3874292 1.3187611 482\n",
      "1.4747242 0.40610227 1.0686219 483\n",
      "3.0986695 0.6971451 2.4015243 484\n",
      "1.0695915 0.21545123 0.85414034 485\n",
      "1.454975 0.23745011 1.2175249 486\n",
      "0.9584254 0.09347867 0.8649467 487\n",
      "0.63642627 0.12454988 0.5118764 488\n",
      "1.3225895 0.2222667 1.1003228 489\n",
      "1.4607971 0.41445714 1.04634 490\n",
      "0.41506538 0.05726105 0.35780433 491\n",
      "0.41941476 0.072934195 0.34648055 492\n",
      "0.47107124 0.09670963 0.3743616 493\n",
      "0.38788015 0.07735282 0.31052732 494\n",
      "0.12705845 0.022776872 0.10428157 495\n",
      "2.4586835 1.0928234 1.3658601 496\n",
      "1.2124527 0.31699932 0.89545333 497\n",
      "0.6027947 0.14547178 0.45732293 498\n",
      "0.5717667 0.14438094 0.42738572 499\n"
     ]
    }
   ],
   "source": [
    "#Train network\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t, x_interior, x_initial = sampler(nSim_t, nSim_x_interior, nSim_x_initial)\n",
    "    \n",
    "    for j in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_tnsr:t, x_interior_tnsr:x_interior, x_initial_tnsr:x_initial})\n",
    "        \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.save(sess, './FokkerPlack/' + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow1.7",
   "language": "python",
   "name": "tensorflow1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
