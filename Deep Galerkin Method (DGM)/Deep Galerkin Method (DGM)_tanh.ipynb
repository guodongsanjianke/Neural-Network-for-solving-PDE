{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method (DGM)\n",
    "### S1 = sigma(w1*x + b1)  Z(l) = sigma(u*x + w*S + b) l=1,...,L  G(l) = sigma(u*x + w*S + b) l=1,...,L  \n",
    "### R(l) = sigma(u*x + w*S + b) l=1,...,L   H(l) = sigma(u*x + w*(S Hadamard R) + b)  l=1,...,L  \n",
    "### S(L+1) = (1-G) Hadamard H + Z Hadamard S  f = w*S(L+1) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of imput data  \n",
    "**output_dim:** number of output for LSTM layer  \n",
    "**trans1, trans2 (str):** activation functions used inside the layer;  \n",
    "one of: \"tanh\"(default), \"relu\" or \"sigmoid\"  \n",
    "**u vectors:** weighting vectors for inputs original inputs x  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**S:** output of previous layer  \n",
    "**X:** data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-like layer used in DGM\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        \n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    def call(self, S, X):\n",
    "\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of input data  \n",
    "**output_dim:** number of outputs for dense layer  \n",
    "**transformation:** activation function used inside the layer; using None is equivalent to the identity map  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**X:** input to layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer(dense)\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        \n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    def call(self,X):\n",
    "        \n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_layers:** number of intermediate LSTM layers  \n",
    "**input_dim:** spaital dimension of input data (excludes time dimension)  \n",
    "**final_trans:** transformation used in final layer\n",
    "define initial layer as fully connected\n",
    "to account for time inputs we use input_dim+1 as the input dimension\n",
    "**t:** sampled time inputs  \n",
    "**x:** sampled space inputs  \n",
    "Run the DGM model and obtain fitted function value at the inputs (t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture used in DGM\n",
    "\n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        \n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1,trans1 = \"tanh\", trans2 = \"tanh\"))\n",
    "        \n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    def call(self,t,x):\n",
    "\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process parameters（Ornstein-Uhlenbeck Process）\n",
    "kappa = 0\n",
    "theta = 0.5\n",
    "sigma = 2\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "# tenminal time\n",
    "T = 1.0\n",
    "\n",
    "# bounds of sampling region for space dimension, i.e. sampling will be done on\n",
    "# [multipliter*Xlow, multiplier*Xhigh]\n",
    "Xlow = -4.0\n",
    "Xhigh = 4.0\n",
    "x_multiplier = 2.0\n",
    "t_multiplier = 1.5\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages = 500\n",
    "steps_per_sample = 10\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_t = 5\n",
    "nSim_x_interior = 50\n",
    "nSim_x_initial = 50\n",
    "\n",
    "# Save options\n",
    "saveName = 'FokkerPlanck'\n",
    "saveFigure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OU Simulation function(Ornstein-Uhlenbeck Process)**  \n",
    "Simulate end point of Ornstein-Uhlenbeck process with normally distributed random starting value  \n",
    "**alpha:** mean of random starting value  \n",
    "**beta:** standard deviation of random starting value   \n",
    "**theta:** mean reversion level    \n",
    "**kappa:** mean reversion rate  \n",
    "**sigma:** volatility  \n",
    "**nSim:** number of simulations  \n",
    "**T:** terminal time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(alpha, beta, theta, kappa, sigma, nSim, T):\n",
    "    \n",
    "    # simulate initial point based on normal distribution\n",
    "    X0 = np.random.normal(loc = alpha, scale = beta, size = nSim)\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta + (X0 - theta) * np.exp(-kappa * T)\n",
    "    v = np.sqrt(sigma**2 / (2 * kappa) * (1 - np.exp(-2*kappa*T)))\n",
    "    \n",
    "    # simulate endpoint\n",
    "    Xt = np.random.normal(m,v)    \n",
    "    \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample time-space points from the function's domain;  \n",
    "point are sampled uniformly on the interior of the domain, at the initial/terminal time points  \n",
    "and along the spatial boundary at different time points.  \n",
    "**nSim_t:** number of (interior) time points to sample  \n",
    "**nSim_x_interior:** number of space points in the interior of the function's domain to sample  \n",
    "**nSim_x_initial:** number of space points at initial time to sample (initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function - random sample time-space pairs\n",
    "def sampler(nSim_t, nSim_x_interior, nSim_x_initial):\n",
    "    \n",
    "    # Sampler1: domain interior\n",
    "    t = np.random.uniform(low=0, high=T*t_multiplier, size=[nSim_t, 1])\n",
    "    x_interior = np.random.uniform(low=Xlow*x_multiplier, high=Xhigh*x_multiplier, size=[nSim_x_interior, 1])\n",
    "    \n",
    "    # Sampler: spatial boundary\n",
    "    # no spatial boundary condition for this problem \n",
    "    \n",
    "    # Sampler3: initial/terminal condition\n",
    "    x_initial = np.random.uniform(low=Xlow*1.5, high=Xhigh*1.5, size = [nSim_x_initial, 1])\n",
    "    \n",
    "    return t, x_interior, x_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute total loss for training.     \n",
    "The loss is based o the PDE satisfied by the negative-exponential of the density and NOT the density   \n",
    "itself, i.e. the u(t,x) in p(t,x) = exp(-u(t,x)) / c(t) where p is the density and c is the normalization constant.    \n",
    "**model:** DGM model object   \n",
    "**t:** sampled (interior) time points  \n",
    "**x_interior:** sampled space points in the interior of the function's domain   \n",
    "**x_initial:** sampled space points at initial time   \n",
    "**nSim_t:** number of (interior) time points sampled (size of t)  \n",
    "**alpha:** mean of normal distribution for process staring value  \n",
    "**beta:** standard deviation of normal distribution for process starting value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t, x_interior, x_initial, nSim_t, alpha, beta):\n",
    "    \n",
    "    # Loss term1: PDE\n",
    "    \n",
    "    # initialize vector of losses\n",
    "    losses_u = []\n",
    "    \n",
    "    # for each simulated interior time point\n",
    "    for tIndex in range(nSim_t):\n",
    "        \n",
    "        curr_t = t[tIndex]\n",
    "        t_vector = curr_t * tf.ones_like(x_interior)\n",
    "        \n",
    "        u    = model.call(t_vector, x_interior)\n",
    "        u_t  = tf.gradients(u, t_vector)[0]\n",
    "        u_x  = tf.gradients(u, x_interior)[0]\n",
    "        u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "        psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "        psi = tf.reduce_sum( u_t*tf.exp(-u) ) / psi_denominator\n",
    "\n",
    "        # PDE differential operator\n",
    "        diff_f = -u_t + kappa - kappa*(x_interior- theta)*u_x - 0.5*sigma**2*(-u_xx + u_x**2) + psi\n",
    "        \n",
    "        # compute L2-norm of differential operator and attach to vector of losses\n",
    "        currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "        losses_u.append(currLoss)\n",
    "    \n",
    "    # average losses across sample time points \n",
    "    L1 = tf.add_n(losses_u) / nSim_t\n",
    "    \n",
    "    # Loss term2: boundary condition\n",
    "    # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term3: initial condition\n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0 i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    fitted_pdf = model.call(0*tf.ones_like(x_initial), x_initial)\n",
    "    \n",
    "    target_pdf  = 0.5*(x_initial - alpha)**2 / (beta**2)\n",
    "    \n",
    "    # average L2 error for initial distribution\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "    return L1, L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input(time, space domain interior, space domain at initial time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Set up network\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "t_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_initial_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_tnsr, x_interior_tnsr, x_initial_tnsr, nSim_t, alpha, beta)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "u = model.call(t_tnsr, x_interior_tnsr)\n",
    "p_unnorm = tf.exp(-u)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.647045 2.4194834 4.2275615 0\n",
      "6.266523 2.2147923 4.0517306 1\n",
      "4.1185074 1.4072833 2.7112243 2\n",
      "4.430644 1.2149463 3.215698 3\n",
      "2.2074668 0.4990591 1.7084078 4\n",
      "1.7181611 0.37702793 1.3411331 5\n",
      "3.3938904 2.0650299 1.3288606 6\n",
      "2.4269183 1.030864 1.3960543 7\n",
      "0.8449986 0.3127013 0.53229725 8\n",
      "0.9292029 0.29220816 0.6369947 9\n",
      "0.4122085 0.1308738 0.2813347 10\n",
      "0.316097 0.09268576 0.22341123 11\n",
      "0.25118518 0.07323962 0.17794557 12\n",
      "0.11736485 0.040881068 0.076483786 13\n",
      "5.8022327 4.1627855 1.6394472 14\n",
      "1.7594676 0.70033497 1.0591327 15\n",
      "1.3304719 0.41092768 0.91954416 16\n",
      "1.9862294 1.1211528 0.86507666 17\n",
      "1.4386858 0.31993982 1.1187459 18\n",
      "0.72758394 0.2825614 0.44502255 19\n",
      "1.3302073 0.7189587 0.6112487 20\n",
      "0.58294755 0.20887528 0.37407225 21\n",
      "0.17020497 0.10647034 0.06373463 22\n",
      "0.4978568 0.2836935 0.2141633 23\n",
      "0.1518138 0.047531568 0.10428224 24\n",
      "0.5037191 0.32729912 0.17641999 25\n",
      "0.35061917 0.08889363 0.26172554 26\n",
      "0.35403 0.105500184 0.24852984 27\n",
      "0.64455736 0.2920912 0.35246614 28\n",
      "0.4488389 0.25552127 0.19331762 29\n",
      "0.36464953 0.21132044 0.1533291 30\n",
      "0.1917884 0.06117163 0.13061677 31\n",
      "0.12509577 0.0193963 0.105699465 32\n",
      "1.7802362 1.3714836 0.40875268 33\n",
      "0.5431999 0.1763041 0.36689577 34\n",
      "0.26361445 0.07092502 0.19268943 35\n",
      "0.10683015 0.026695449 0.080134705 36\n",
      "0.13957337 0.024146566 0.1154268 37\n",
      "0.13949877 0.08100047 0.058498308 38\n",
      "0.4538492 0.19864088 0.2552083 39\n",
      "0.2184559 0.02249446 0.19596143 40\n",
      "0.05590033 0.024651607 0.031248724 41\n",
      "0.22784579 0.08254002 0.14530577 42\n",
      "0.37728208 0.17084913 0.20643297 43\n",
      "0.09031581 0.0153694665 0.074946344 44\n",
      "0.08553603 0.03113985 0.054396186 45\n",
      "0.06379892 0.02275467 0.041044254 46\n",
      "0.2011021 0.13191889 0.0691832 47\n",
      "0.09881342 0.046381086 0.052432336 48\n",
      "0.35865772 0.22097579 0.13768195 49\n",
      "0.19659662 0.12426982 0.07232681 50\n",
      "0.13098623 0.043097492 0.08788874 51\n",
      "0.053452257 0.024074769 0.029377488 52\n",
      "0.34126273 0.19683312 0.14442961 53\n",
      "0.64697254 0.512229 0.13474353 54\n",
      "0.17850736 0.08669908 0.091808274 55\n",
      "0.24371436 0.14319792 0.10051643 56\n",
      "0.08267577 0.044442743 0.038233027 57\n",
      "1.3145666 1.240051 0.07451557 58\n",
      "1.1935569 1.0902437 0.10331318 59\n",
      "0.5327842 0.19113067 0.34165356 60\n",
      "0.21795341 0.04935453 0.16859888 61\n",
      "0.09057291 0.017351488 0.07322142 62\n",
      "0.11774241 0.05593577 0.06180664 63\n",
      "0.09330927 0.0053553227 0.08795395 64\n",
      "0.07229063 0.016156722 0.05613391 65\n",
      "0.02966204 0.013004328 0.01665771 66\n",
      "1.2037896 0.7054107 0.49837887 67\n",
      "0.29996723 0.055766728 0.2442005 68\n",
      "0.3884394 0.15569156 0.23274782 69\n",
      "0.48325288 0.23625 0.24700287 70\n",
      "0.13790135 0.0900543 0.047847044 71\n",
      "0.083214924 0.024528254 0.058686666 72\n",
      "0.9855715 0.6482598 0.3373117 73\n",
      "0.7338959 0.4465191 0.28737676 74\n",
      "0.8417798 0.5997252 0.24205461 75\n",
      "0.5481543 0.2323647 0.3157896 76\n",
      "0.31481826 0.10191401 0.21290424 77\n",
      "0.14095463 0.04379624 0.09715839 78\n",
      "0.4159998 0.32933038 0.086669415 79\n",
      "0.181427 0.07396092 0.10746609 80\n",
      "0.15545166 0.06572681 0.08972484 81\n",
      "0.06006801 0.018329361 0.041738648 82\n",
      "0.3457679 0.23470946 0.11105843 83\n",
      "0.294561 0.067678206 0.2268828 84\n",
      "0.1348692 0.054738414 0.08013079 85\n",
      "0.25008896 0.15423158 0.09585739 86\n",
      "0.08144629 0.02374937 0.057696924 87\n",
      "0.0757304 0.028433967 0.047296427 88\n",
      "0.09024458 0.052822348 0.037422225 89\n",
      "0.1887038 0.13551207 0.05319173 90\n",
      "0.084906176 0.061388046 0.02351813 91\n",
      "0.22119288 0.120815195 0.10037769 92\n",
      "0.063379824 0.027849365 0.035530463 93\n",
      "0.060495846 0.03319608 0.027299764 94\n",
      "0.035763804 0.009493092 0.026270712 95\n",
      "0.39585507 0.25848573 0.13736935 96\n",
      "0.09812566 0.035620034 0.062505625 97\n",
      "0.07984021 0.046304006 0.033536207 98\n",
      "0.092038706 0.036154546 0.05588416 99\n",
      "0.08271357 0.058336493 0.024377076 100\n",
      "0.059497263 0.014404845 0.04509242 101\n",
      "0.1047203 0.07857867 0.026141629 102\n",
      "0.1253229 0.06295348 0.06236941 103\n",
      "0.14992432 0.08814751 0.061776813 104\n",
      "0.16853482 0.11706286 0.051471964 105\n",
      "0.08692286 0.062034786 0.024888076 106\n",
      "0.1442907 0.06688233 0.07740837 107\n",
      "0.13953441 0.10240369 0.037130717 108\n",
      "0.059222892 0.022809876 0.036413014 109\n",
      "0.045834176 0.031883355 0.013950819 110\n",
      "0.04196365 0.0060794638 0.035884187 111\n",
      "0.027434547 0.01722592 0.010208627 112\n",
      "0.061867047 0.04446606 0.017400987 113\n",
      "0.026121324 0.0066904053 0.019430919 114\n",
      "0.08851956 0.07791998 0.010599576 115\n",
      "0.061631363 0.04613432 0.015497045 116\n",
      "0.0285612 0.015277783 0.013283416 117\n",
      "0.015337005 0.006784788 0.008552218 118\n",
      "0.1844265 0.17375283 0.010673667 119\n",
      "0.08725933 0.05794382 0.029315509 120\n",
      "0.07213315 0.056027055 0.016106097 121\n",
      "0.036406152 0.022129174 0.014276979 122\n",
      "0.082809746 0.061623167 0.021186575 123\n",
      "0.09309709 0.060899932 0.032197163 124\n",
      "0.10494858 0.045727324 0.059221253 125\n",
      "0.035869513 0.02079392 0.015075593 126\n",
      "0.9482326 0.7238988 0.22433378 127\n",
      "0.48267514 0.30924687 0.17342825 128\n",
      "0.3595148 0.13809364 0.22142117 129\n",
      "0.20919219 0.05365807 0.15553412 130\n",
      "0.07800421 0.019069023 0.058935184 131\n",
      "0.2605325 0.18090953 0.079622984 132\n",
      "0.107700646 0.0381419 0.06955875 133\n",
      "0.05233454 0.012796012 0.03953853 134\n",
      "0.07892656 0.06319579 0.015730776 135\n",
      "0.18595217 0.14168997 0.0442622 136\n",
      "0.049887136 0.015712155 0.034174982 137\n",
      "0.037520327 0.01942664 0.018093687 138\n",
      "0.1624757 0.0911647 0.07131101 139\n",
      "0.028643895 0.0056464914 0.022997404 140\n",
      "0.20563659 0.1943847 0.011251894 141\n",
      "0.078146264 0.0096209245 0.06852534 142\n",
      "0.06614026 0.033980772 0.032159485 143\n",
      "0.06501222 0.03319755 0.031814672 144\n",
      "0.014643824 0.006566825 0.008076998 145\n",
      "0.267021 0.22897772 0.038043264 146\n",
      "0.10922688 0.07306249 0.03616439 147\n",
      "0.022724103 0.008970204 0.0137539 148\n",
      "0.017358623 0.0103448555 0.007013768 149\n",
      "0.017515164 0.012695662 0.004819502 150\n",
      "0.01740183 0.00908661 0.008315219 151\n",
      "0.06381275 0.020973474 0.042839278 152\n",
      "0.042123545 0.02381887 0.018304676 153\n",
      "0.025948185 0.009707088 0.016241098 154\n",
      "0.011805907 0.0037050005 0.008100906 155\n",
      "0.04004933 0.03476603 0.0052833017 156\n",
      "0.040958874 0.022020878 0.018937994 157\n",
      "0.02644616 0.019445887 0.007000272 158\n",
      "0.024213012 0.014328785 0.009884227 159\n",
      "0.09352101 0.047127318 0.04639369 160\n",
      "0.15204579 0.089721546 0.062324237 161\n",
      "0.10924847 0.07647034 0.03277813 162\n",
      "0.03170971 0.017024267 0.014685443 163\n",
      "0.01970694 0.011634318 0.008072622 164\n",
      "0.020571867 0.011167206 0.009404661 165\n",
      "0.019286133 0.009151862 0.010134271 166\n",
      "0.013235666 0.008121884 0.005113782 167\n",
      "1.2385142 0.72432846 0.51418567 168\n",
      "0.37076432 0.28797352 0.08279078 169\n",
      "0.15081808 0.037248176 0.11356991 170\n",
      "0.59201115 0.41441962 0.17759153 171\n",
      "0.08106196 0.032236382 0.048825573 172\n",
      "0.88575214 0.4888609 0.39689124 173\n",
      "0.73724884 0.34189814 0.3953507 174\n",
      "0.3833688 0.16442546 0.21894334 175\n",
      "0.3131873 0.145193 0.16799432 176\n",
      "0.18049309 0.06470537 0.11578772 177\n",
      "0.061077315 0.0074676396 0.053609677 178\n",
      "0.14165623 0.04267199 0.09898424 179\n",
      "0.08179403 0.064390324 0.017403705 180\n",
      "0.14343257 0.021974284 0.12145829 181\n",
      "0.051307127 0.010444656 0.04086247 182\n",
      "0.11557709 0.048516333 0.06706075 183\n",
      "0.05549266 0.014009088 0.04148357 184\n",
      "0.20291156 0.11930861 0.08360295 185\n",
      "0.050250746 0.019603664 0.030647082 186\n",
      "0.34163982 0.21513395 0.12650585 187\n",
      "0.080266766 0.03971208 0.040554687 188\n",
      "0.25093597 0.12839465 0.122541316 189\n",
      "0.103988275 0.067375764 0.036612514 190\n",
      "0.10971756 0.0713427 0.03837486 191\n",
      "0.0694674 0.020628916 0.048838485 192\n",
      "0.07431565 0.032341883 0.041973773 193\n",
      "0.03589616 0.01207336 0.0238228 194\n",
      "0.020845277 0.011137377 0.009707901 195\n",
      "0.019377103 0.009433695 0.009943408 196\n",
      "0.040077753 0.019086754 0.020991 197\n",
      "0.050016023 0.037248272 0.012767751 198\n",
      "0.05246927 0.007571614 0.044897653 199\n",
      "0.19722128 0.14615016 0.051071115 200\n",
      "0.044657685 0.015487905 0.029169781 201\n",
      "0.07386028 0.05528171 0.018578572 202\n",
      "0.020026255 0.008641806 0.011384449 203\n",
      "0.064812824 0.043212008 0.02160082 204\n",
      "0.05136861 0.0396175 0.011751107 205\n",
      "0.03482208 0.009064731 0.02575735 206\n",
      "0.13565415 0.070526116 0.06512803 207\n",
      "0.09310125 0.04812735 0.0449739 208\n",
      "0.080475524 0.06700819 0.013467333 209\n",
      "0.12408407 0.081405215 0.042678855 210\n",
      "0.09977394 0.041496832 0.05827711 211\n",
      "0.06634988 0.03184356 0.03450632 212\n",
      "0.0384466 0.015828725 0.022617877 213\n",
      "0.068553865 0.04620893 0.02234494 214\n",
      "0.031632032 0.011535302 0.02009673 215\n",
      "0.013259169 0.0035713052 0.009687863 216\n",
      "0.023474079 0.014540267 0.0089338105 217\n",
      "0.015782945 0.0023536368 0.013429308 218\n",
      "0.013244469 0.0075768353 0.005667633 219\n",
      "0.048151173 0.038854234 0.009296937 220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07136992 0.057190634 0.01417929 221\n",
      "0.10283323 0.08636024 0.016472984 222\n",
      "0.053320646 0.04487452 0.008446125 223\n",
      "0.040007506 0.025583839 0.014423666 224\n",
      "0.064950116 0.04467041 0.020279704 225\n",
      "0.019269135 0.009818623 0.009450512 226\n",
      "0.054114476 0.038662355 0.0154521195 227\n",
      "0.030195452 0.016126124 0.014069328 228\n",
      "0.017425193 0.012550406 0.0048747864 229\n",
      "0.030964898 0.021115003 0.009849895 230\n",
      "0.03490246 0.027840454 0.007062007 231\n",
      "0.029330574 0.01696994 0.012360635 232\n",
      "0.9498061 0.8355746 0.11423145 233\n",
      "0.1441867 0.08147082 0.06271588 234\n",
      "0.27628198 0.11349126 0.16279073 235\n",
      "0.06391585 0.022881573 0.041034278 236\n",
      "0.06642775 0.030380607 0.03604715 237\n",
      "0.20345798 0.11610812 0.08734986 238\n",
      "0.074724495 0.032682605 0.042041887 239\n",
      "0.030007048 0.010545582 0.019461466 240\n",
      "0.3674156 0.24914208 0.11827354 241\n",
      "0.050655223 0.019096462 0.031558763 242\n",
      "1.8746686 0.8179623 1.0567063 243\n",
      "0.56444097 0.18447427 0.3799667 244\n",
      "0.23637947 0.08199864 0.15438084 245\n",
      "0.13570228 0.023838965 0.111863315 246\n",
      "0.23835325 0.15692487 0.081428386 247\n",
      "0.38798523 0.28668126 0.10130396 248\n",
      "0.26683837 0.18760319 0.07923519 249\n",
      "0.0911953 0.029024351 0.062170945 250\n",
      "0.0470134 0.013311866 0.03370153 251\n",
      "0.13037118 0.08036166 0.05000952 252\n",
      "0.19179225 0.15270607 0.039086178 253\n",
      "0.060926843 0.027272463 0.03365438 254\n",
      "0.085649796 0.028416773 0.05723302 255\n",
      "0.07009286 0.041312132 0.028780723 256\n",
      "0.029702712 0.011699322 0.018003391 257\n",
      "0.037749343 0.009111706 0.028637636 258\n",
      "0.12273611 0.053053122 0.069682986 259\n",
      "0.06913596 0.045416076 0.02371988 260\n",
      "0.03125003 0.007281605 0.023968423 261\n",
      "0.037426934 0.02078276 0.016644176 262\n",
      "0.03177077 0.009437985 0.022332786 263\n",
      "0.01890824 0.011482358 0.007425883 264\n",
      "0.048659824 0.025576562 0.023083262 265\n",
      "0.07048799 0.040382024 0.030105963 266\n",
      "0.043645978 0.014430729 0.029215248 267\n",
      "0.045288112 0.028985793 0.01630232 268\n",
      "0.02979298 0.025774939 0.0040180413 269\n",
      "0.025412358 0.015827851 0.009584507 270\n",
      "0.035677217 0.013751942 0.021925276 271\n",
      "0.04779605 0.039064784 0.008731263 272\n",
      "0.034436584 0.023407903 0.011028683 273\n",
      "0.058219887 0.035966042 0.022253843 274\n",
      "0.045315955 0.03409899 0.011216964 275\n",
      "0.043612942 0.032711294 0.010901648 276\n",
      "0.10879834 0.098505154 0.0102931885 277\n",
      "0.044990566 0.030029943 0.0149606215 278\n",
      "0.018226935 0.011908487 0.0063184476 279\n",
      "0.023728844 0.017959956 0.0057688886 280\n",
      "0.0047323266 0.001464154 0.0032681727 281\n",
      "0.34893954 0.32675323 0.022186305 282\n",
      "0.061735723 0.027884454 0.03385127 283\n",
      "0.043563925 0.0073865126 0.03617741 284\n",
      "0.04536666 0.038735084 0.0066315765 285\n",
      "0.024217144 0.011743217 0.012473928 286\n",
      "0.1287093 0.08838371 0.040325593 287\n",
      "0.025264293 0.008405912 0.016858382 288\n",
      "0.040213563 0.02953791 0.010675651 289\n",
      "0.05747308 0.028873166 0.02859991 290\n",
      "0.03835908 0.0151551515 0.023203928 291\n",
      "0.027595941 0.013266829 0.014329113 292\n",
      "0.02840452 0.016658105 0.011746416 293\n",
      "0.069382735 0.062350344 0.007032394 294\n",
      "0.031302154 0.023134226 0.008167926 295\n",
      "0.019123547 0.01263614 0.0064874073 296\n",
      "0.020160276 0.016278392 0.0038818838 297\n",
      "0.029622037 0.015761659 0.0138603775 298\n",
      "0.011590639 0.009230513 0.002360126 299\n",
      "0.065727666 0.055035032 0.01069263 300\n",
      "0.01759696 0.010916597 0.0066803633 301\n",
      "0.032072045 0.022002766 0.010069281 302\n",
      "0.019712515 0.012854737 0.006857777 303\n",
      "0.049447324 0.041227303 0.008220022 304\n",
      "0.03538178 0.028921677 0.006460102 305\n",
      "0.006333344 0.0037056326 0.0026277115 306\n",
      "0.010849597 0.008810199 0.0020393983 307\n",
      "0.014457965 0.010554951 0.0039030144 308\n",
      "0.053790618 0.04225243 0.01153819 309\n",
      "0.14884149 0.13667637 0.0121651115 310\n",
      "0.03798502 0.026932482 0.01105254 311\n",
      "0.034513675 0.01399719 0.020516487 312\n",
      "0.08243355 0.046243556 0.036189992 313\n",
      "0.05149045 0.020129051 0.031361397 314\n",
      "0.07588931 0.061960608 0.013928703 315\n",
      "0.023965707 0.009371204 0.014594503 316\n",
      "0.07381234 0.06350357 0.010308772 317\n",
      "0.017518954 0.0046699056 0.012849049 318\n",
      "0.016054677 0.009194138 0.006860539 319\n",
      "0.05089627 0.04164512 0.009251148 320\n",
      "0.032830276 0.021207957 0.011622319 321\n",
      "0.04761778 0.02865052 0.01896726 322\n",
      "0.02806831 0.023593714 0.0044745947 323\n",
      "0.045376908 0.035681304 0.009695605 324\n",
      "0.022931755 0.015336371 0.0075953836 325\n",
      "0.02058846 0.013497104 0.007091356 326\n",
      "0.02093342 0.019796437 0.001136982 327\n",
      "0.012460847 0.0071628354 0.005298012 328\n",
      "0.013013723 0.004412625 0.008601098 329\n",
      "0.0076580327 0.0050089792 0.0026490532 330\n",
      "0.018240582 0.011430255 0.0068103266 331\n",
      "0.006950921 0.005431675 0.0015192459 332\n",
      "0.042532623 0.039796058 0.002736563 333\n",
      "0.02708285 0.008413496 0.018669352 334\n",
      "0.057230197 0.039378565 0.017851632 335\n",
      "0.042383734 0.034286782 0.008096952 336\n",
      "0.017079853 0.008247504 0.008832348 337\n",
      "0.012562608 0.007546313 0.0050162957 338\n",
      "0.004652343 0.0032859538 0.0013663896 339\n",
      "0.014000032 0.008662417 0.0053376155 340\n",
      "0.009358556 0.004787668 0.004570888 341\n",
      "0.00885523 0.006866814 0.0019884158 342\n",
      "0.026418982 0.019369567 0.0070494157 343\n",
      "0.0065687234 0.0039958986 0.002572825 344\n",
      "0.06557581 0.0509769 0.0145989135 345\n",
      "0.023697508 0.018840726 0.0048567825 346\n",
      "0.007179305 0.004536365 0.0026429398 347\n",
      "0.0054044193 0.0035092512 0.001895168 348\n",
      "0.0864398 0.029671943 0.05676786 349\n",
      "0.025946748 0.014075649 0.0118710995 350\n",
      "0.019900598 0.0101678325 0.009732764 351\n",
      "0.020268064 0.017059447 0.003208617 352\n",
      "0.014525483 0.0114442725 0.003081211 353\n",
      "0.008112775 0.006445502 0.0016672728 354\n",
      "0.008022409 0.005363604 0.0026588049 355\n",
      "0.029171642 0.022230316 0.0069413264 356\n",
      "0.008048935 0.005894669 0.0021542658 357\n",
      "0.082353234 0.06434086 0.018012373 358\n",
      "0.034883097 0.022487206 0.012395891 359\n",
      "0.01972918 0.014390473 0.005338707 360\n",
      "0.010844436 0.0054471125 0.0053973235 361\n",
      "0.004494913 0.0027244787 0.001770434 362\n",
      "0.008745415 0.00580373 0.0029416848 363\n",
      "0.0077153463 0.0057133893 0.0020019573 364\n",
      "0.02809908 0.0243196 0.0037794795 365\n",
      "0.009490654 0.006499602 0.0029910516 366\n",
      "0.008525634 0.0069034845 0.00162215 367\n",
      "0.029105041 0.025771132 0.0033339085 368\n",
      "0.023099106 0.01974026 0.0033588472 369\n",
      "0.062048234 0.046236217 0.015812017 370\n",
      "0.031057362 0.027388696 0.0036686668 371\n",
      "0.018813996 0.014924685 0.0038893116 372\n",
      "0.05887589 0.051169343 0.007706544 373\n",
      "0.035992272 0.029748986 0.0062432885 374\n",
      "0.022472411 0.0154342875 0.0070381234 375\n",
      "0.0038895616 0.0019456986 0.0019438629 376\n",
      "0.24604338 0.2271986 0.018844776 377\n",
      "0.04311276 0.021959301 0.021153457 378\n",
      "0.037686273 0.018523145 0.019163126 379\n",
      "0.039830305 0.026854968 0.012975335 380\n",
      "0.034598347 0.019683091 0.014915255 381\n",
      "0.27612764 0.19223706 0.08389057 382\n",
      "0.053348366 0.010552297 0.042796068 383\n",
      "0.83879554 0.28081337 0.55798215 384\n",
      "0.67611194 0.26503035 0.41108155 385\n",
      "0.1898293 0.08025813 0.109571174 386\n",
      "0.32976028 0.10788909 0.22187118 387\n",
      "0.10911107 0.06611014 0.043000933 388\n",
      "0.034863766 0.016067645 0.018796122 389\n",
      "0.34492254 0.20887236 0.13605018 390\n",
      "0.074655354 0.034397196 0.040258154 391\n",
      "0.04924284 0.023800856 0.025441982 392\n",
      "0.03623814 0.014881367 0.021356773 393\n",
      "0.107529685 0.047425777 0.060103904 394\n",
      "0.500396 0.25149748 0.24889852 395\n",
      "0.14497799 0.06672927 0.07824872 396\n",
      "0.10793336 0.05527198 0.05266138 397\n",
      "0.06961732 0.05075217 0.018865155 398\n",
      "0.18499453 0.09879156 0.08620297 399\n",
      "0.030364044 0.012882036 0.017482007 400\n",
      "0.23946148 0.17255262 0.066908866 401\n",
      "0.19780219 0.11155987 0.086242326 402\n",
      "0.15006916 0.060930997 0.089138165 403\n",
      "0.09220768 0.06382574 0.028381936 404\n",
      "0.027838752 0.018682642 0.00915611 405\n",
      "0.025929708 0.012348122 0.013581587 406\n",
      "0.008962594 0.005201883 0.0037607113 407\n",
      "0.11337134 0.071807735 0.04156361 408\n",
      "0.115305126 0.08819758 0.027107546 409\n",
      "0.035243638 0.031499308 0.003744331 410\n",
      "0.02066408 0.013661954 0.007002126 411\n",
      "0.016060408 0.0074838917 0.008576517 412\n",
      "0.34546274 0.18939058 0.15607215 413\n",
      "0.18419513 0.08079862 0.103396505 414\n",
      "0.07964787 0.04257059 0.03707728 415\n",
      "0.14216556 0.097578265 0.044587288 416\n",
      "0.08673511 0.040564615 0.04617049 417\n",
      "0.04691059 0.017140208 0.029770382 418\n",
      "0.05667485 0.014207579 0.04246727 419\n",
      "0.03644328 0.02482531 0.01161797 420\n",
      "0.14126958 0.104046725 0.03722285 421\n",
      "0.0870128 0.045702893 0.041309904 422\n",
      "0.037772235 0.009569267 0.028202968 423\n",
      "0.019322906 0.008655953 0.010666953 424\n",
      "0.015207514 0.008897404 0.0063101095 425\n",
      "0.038745206 0.029447654 0.0092975525 426\n",
      "0.016157214 0.0056821615 0.0104750525 427\n",
      "0.0339811 0.026635433 0.007345667 428\n",
      "0.12799823 0.06309102 0.06490722 429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036255047 0.014054889 0.02220016 430\n",
      "0.26813513 0.20081814 0.06731699 431\n",
      "0.10496172 0.020007337 0.08495439 432\n",
      "0.05478204 0.007739454 0.047042586 433\n",
      "0.1885358 0.09718714 0.091348656 434\n",
      "0.22163376 0.17065488 0.050978877 435\n",
      "0.066049404 0.05053724 0.015512163 436\n",
      "0.091330275 0.05125503 0.040075246 437\n",
      "0.04306897 0.023405222 0.01966375 438\n",
      "0.030295307 0.0131288115 0.017166495 439\n",
      "0.12468628 0.08045317 0.044233106 440\n",
      "0.03211472 0.007230818 0.024883904 441\n",
      "0.06849969 0.026002048 0.042497646 442\n",
      "0.09537386 0.087226935 0.008146924 443\n",
      "0.041550487 0.016846202 0.024704285 444\n",
      "0.056221496 0.04146887 0.014752626 445\n",
      "0.06224779 0.05381469 0.0084331 446\n",
      "0.03920327 0.022786358 0.016416913 447\n",
      "0.030551802 0.022222355 0.008329447 448\n",
      "0.03560619 0.016932212 0.01867398 449\n",
      "0.024729978 0.007860036 0.016869944 450\n",
      "0.017388398 0.00657697 0.010811428 451\n",
      "0.016184567 0.012405605 0.0037789615 452\n",
      "0.44310716 0.38764828 0.05545887 453\n",
      "0.18568614 0.15530221 0.030383935 454\n",
      "0.039950095 0.012333092 0.027617004 455\n",
      "0.012602918 0.003936731 0.0086661875 456\n",
      "0.014306821 0.003190462 0.011116359 457\n",
      "0.12115744 0.102282204 0.018875234 458\n",
      "0.066200435 0.027741855 0.03845858 459\n",
      "0.027520355 0.018977074 0.00854328 460\n",
      "0.041728623 0.02229781 0.019430814 461\n",
      "0.06461166 0.03238229 0.03222937 462\n",
      "0.028469276 0.018801335 0.009667941 463\n",
      "0.04426217 0.033663448 0.010598725 464\n",
      "0.01794794 0.014160112 0.0037878293 465\n",
      "0.03913238 0.02763178 0.011500601 466\n",
      "0.018922783 0.014218836 0.004703948 467\n",
      "0.015530283 0.007321279 0.008209003 468\n",
      "0.06569298 0.061046936 0.004646048 469\n",
      "0.041186355 0.023415102 0.017771255 470\n",
      "0.06505779 0.057792753 0.0072650407 471\n",
      "0.04298269 0.023983104 0.018999584 472\n",
      "0.027015585 0.021957483 0.005058102 473\n",
      "0.041874006 0.030603472 0.011270535 474\n",
      "0.009406114 0.007457343 0.0019487711 475\n",
      "0.013386287 0.0065999455 0.006786342 476\n",
      "0.014280451 0.007975611 0.006304841 477\n",
      "0.024525935 0.022372337 0.002153598 478\n",
      "0.017037539 0.011509038 0.0055285012 479\n",
      "0.028363796 0.021072382 0.0072914143 480\n",
      "0.079494394 0.074582495 0.004911899 481\n",
      "0.022055123 0.01665767 0.005397454 482\n",
      "0.03099949 0.018390862 0.012608626 483\n",
      "0.01252279 0.009999884 0.0025229054 484\n",
      "0.009327714 0.005296054 0.0040316605 485\n",
      "0.04154017 0.030349294 0.0111908745 486\n",
      "0.028320156 0.010755315 0.01756484 487\n",
      "0.015200082 0.0069678375 0.008232245 488\n",
      "0.046472654 0.024710184 0.021762472 489\n",
      "0.025209963 0.015294275 0.009915687 490\n",
      "0.02059067 0.01114314 0.009447531 491\n",
      "0.19000037 0.1533842 0.036616176 492\n",
      "0.11449477 0.06372704 0.05076773 493\n",
      "0.40280762 0.19724086 0.20556676 494\n",
      "0.20580956 0.078724205 0.12708536 495\n",
      "0.8141644 0.45278317 0.36138126 496\n",
      "0.24799001 0.093810074 0.15417993 497\n",
      "0.21909377 0.10115141 0.117942356 498\n",
      "0.22975852 0.18323424 0.04652427 499\n"
     ]
    }
   ],
   "source": [
    "#Train network\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t, x_interior, x_initial = sampler(nSim_t, nSim_x_interior, nSim_x_initial)\n",
    "    \n",
    "    for j in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_tnsr:t, x_interior_tnsr:x_interior, x_initial_tnsr:x_initial})\n",
    "        \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.save(sess, './FokkerPlack/' + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow1.7",
   "language": "python",
   "name": "tensorflow1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
