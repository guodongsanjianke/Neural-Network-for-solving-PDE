{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method (DGM)\n",
    "### S1 = sigma(w1*x + b1)  Z(l) = sigma(u*x + w*S + b) l=1,...,L  G(l) = sigma(u*x + w*S + b) l=1,...,L  \n",
    "### R(l) = sigma(u*x + w*S + b) l=1,...,L   H(l) = sigma(u*x + w*(S Hadamard R) + b)  l=1,...,L  \n",
    "### S(L+1) = (1-G) Hadamard H + Z Hadamard S  f = w*S(L+1) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of imput data  \n",
    "**output_dim:** number of output for LSTM layer  \n",
    "**trans1, trans2 (str):** activation functions used inside the layer;  \n",
    "one of: \"tanh\"(default), \"relu\" or \"sigmoid\"  \n",
    "**u vectors:** weighting vectors for inputs original inputs x  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**S:** output of previous layer  \n",
    "**X:** data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-like layer used in DGM\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        \n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    def call(self, S, X):\n",
    "\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of input data  \n",
    "**output_dim:** number of outputs for dense layer  \n",
    "**transformation:** activation function used inside the layer; using None is equivalent to the identity map  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**X:** input to layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer(dense)\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        \n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    def call(self,X):\n",
    "        \n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_layers:** number of intermediate LSTM layers  \n",
    "**input_dim:** spaital dimension of input data (excludes time dimension)  \n",
    "**final_trans:** transformation used in final layer\n",
    "define initial layer as fully connected\n",
    "to account for time inputs we use input_dim+1 as the input dimension\n",
    "**t:** sampled time inputs  \n",
    "**x:** sampled space inputs  \n",
    "Run the DGM model and obtain fitted function value at the inputs (t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture used in DGM\n",
    "\n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        \n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1,trans1 = \"relu\", trans2 = \"relu\"))\n",
    "        \n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    def call(self,t,x):\n",
    "\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process parameters（Ornstein-Uhlenbeck Process）\n",
    "kappa = 0\n",
    "theta = 0.5\n",
    "sigma = 2\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "# tenminal time\n",
    "T = 1.0\n",
    "\n",
    "# bounds of sampling region for space dimension, i.e. sampling will be done on\n",
    "# [multipliter*Xlow, multiplier*Xhigh]\n",
    "Xlow = -4.0\n",
    "Xhigh = 4.0\n",
    "x_multiplier = 2.0\n",
    "t_multiplier = 1.5\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages = 500\n",
    "steps_per_sample = 10\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_t = 5\n",
    "nSim_x_interior = 50\n",
    "nSim_x_initial = 50\n",
    "\n",
    "# Save options\n",
    "saveName = 'FokkerPlanck'\n",
    "saveFigure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OU Simulation function(Ornstein-Uhlenbeck Process)**  \n",
    "Simulate end point of Ornstein-Uhlenbeck process with normally distributed random starting value  \n",
    "**alpha:** mean of random starting value  \n",
    "**beta:** standard deviation of random starting value   \n",
    "**theta:** mean reversion level    \n",
    "**kappa:** mean reversion rate  \n",
    "**sigma:** volatility  \n",
    "**nSim:** number of simulations  \n",
    "**T:** terminal time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(alpha, beta, theta, kappa, sigma, nSim, T):\n",
    "    \n",
    "    # simulate initial point based on normal distribution\n",
    "    X0 = np.random.normal(loc = alpha, scale = beta, size = nSim)\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta + (X0 - theta) * np.exp(-kappa * T)\n",
    "    v = np.sqrt(sigma**2 / (2 * kappa) * (1 - np.exp(-2*kappa*T)))\n",
    "    \n",
    "    # simulate endpoint\n",
    "    Xt = np.random.normal(m,v)    \n",
    "    \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample time-space points from the function's domain;  \n",
    "point are sampled uniformly on the interior of the domain, at the initial/terminal time points  \n",
    "and along the spatial boundary at different time points.  \n",
    "**nSim_t:** number of (interior) time points to sample  \n",
    "**nSim_x_interior:** number of space points in the interior of the function's domain to sample  \n",
    "**nSim_x_initial:** number of space points at initial time to sample (initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function - random sample time-space pairs\n",
    "def sampler(nSim_t, nSim_x_interior, nSim_x_initial):\n",
    "    \n",
    "    # Sampler1: domain interior\n",
    "    t = np.random.uniform(low=0, high=T*t_multiplier, size=[nSim_t, 1])\n",
    "    x_interior = np.random.uniform(low=Xlow*x_multiplier, high=Xhigh*x_multiplier, size=[nSim_x_interior, 1])\n",
    "    \n",
    "    # Sampler: spatial boundary\n",
    "    # no spatial boundary condition for this problem \n",
    "    \n",
    "    # Sampler3: initial/terminal condition\n",
    "    x_initial = np.random.uniform(low=Xlow*1.5, high=Xhigh*1.5, size = [nSim_x_initial, 1])\n",
    "    \n",
    "    return t, x_interior, x_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute total loss for training.     \n",
    "The loss is based o the PDE satisfied by the negative-exponential of the density and NOT the density   \n",
    "itself, i.e. the u(t,x) in p(t,x) = exp(-u(t,x)) / c(t) where p is the density and c is the normalization constant.    \n",
    "**model:** DGM model object   \n",
    "**t:** sampled (interior) time points  \n",
    "**x_interior:** sampled space points in the interior of the function's domain   \n",
    "**x_initial:** sampled space points at initial time   \n",
    "**nSim_t:** number of (interior) time points sampled (size of t)  \n",
    "**alpha:** mean of normal distribution for process staring value  \n",
    "**beta:** standard deviation of normal distribution for process starting value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t, x_interior, x_initial, nSim_t, alpha, beta):\n",
    "    \n",
    "    # Loss term1: PDE\n",
    "    \n",
    "    # initialize vector of losses\n",
    "    losses_u = []\n",
    "    \n",
    "    # for each simulated interior time point\n",
    "    for tIndex in range(nSim_t):\n",
    "        \n",
    "        curr_t = t[tIndex]\n",
    "        t_vector = curr_t * tf.ones_like(x_interior)\n",
    "        \n",
    "        u    = model.call(t_vector, x_interior)\n",
    "        u_t  = tf.gradients(u, t_vector)[0]\n",
    "        u_x  = tf.gradients(u, x_interior)[0]\n",
    "        u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "        psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "        psi = tf.reduce_sum( u_t*tf.exp(-u) ) / psi_denominator\n",
    "\n",
    "        # PDE differential operator\n",
    "        diff_f = -u_t + kappa - kappa*(x_interior- theta)*u_x - 0.5*sigma**2*(-u_xx + u_x**2) + psi\n",
    "        \n",
    "        # compute L2-norm of differential operator and attach to vector of losses\n",
    "        currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "        losses_u.append(currLoss)\n",
    "    \n",
    "    # average losses across sample time points \n",
    "    L1 = tf.add_n(losses_u) / nSim_t\n",
    "    \n",
    "    # Loss term2: boundary condition\n",
    "    # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term3: initial condition\n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0 i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    fitted_pdf = model.call(0*tf.ones_like(x_initial), x_initial)\n",
    "    \n",
    "    target_pdf  = 0.5*(x_initial - alpha)**2 / (beta**2)\n",
    "    \n",
    "    # average L2 error for initial distribution\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "    return L1, L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input(time, space domain interior, space domain at initial time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Set up network\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "t_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_initial_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_tnsr, x_interior_tnsr, x_initial_tnsr, nSim_t, alpha, beta)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "u = model.call(t_tnsr, x_interior_tnsr)\n",
    "p_unnorm = tf.exp(-u)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14942.644 14880.12 62.523827 0\n",
      "69.86564 18.081434 51.784203 1\n",
      "317.01086 256.78683 60.224045 2\n",
      "72.18333 21.64765 50.535675 3\n",
      "62.610386 3.7408204 58.869564 4\n",
      "37.3634 3.1701863 34.193214 5\n",
      "74.69701 3.5109532 71.18605 6\n",
      "49.255306 3.4794602 45.775845 7\n",
      "56.642075 4.283497 52.358578 8\n",
      "42.673748 2.7630599 39.910686 9\n",
      "34.384174 7.6822343 26.701939 10\n",
      "27.012331 1.0238832 25.988447 11\n",
      "27.871527 3.0359807 24.835546 12\n",
      "26.741451 7.5740175 19.167433 13\n",
      "26.126532 7.447597 18.678934 14\n",
      "23.594131 4.303257 19.290874 15\n",
      "18.910381 6.701488 12.208893 16\n",
      "20.27655 8.785714 11.490837 17\n",
      "23.276794 7.07793 16.198864 18\n",
      "19.351536 5.5473933 13.804142 19\n",
      "12.301167 4.70923 7.591936 20\n",
      "14.520358 3.7326736 10.787684 21\n",
      "16.136045 3.0909193 13.045126 22\n",
      "25.98601 8.686638 17.299372 23\n",
      "16.838873 2.9174469 13.921427 24\n",
      "21.027744 7.300969 13.726775 25\n",
      "10.989891 3.5120866 7.477804 26\n",
      "7.990516 2.9130912 5.077425 27\n",
      "19.950289 4.132112 15.818176 28\n",
      "18.912193 4.649883 14.262311 29\n",
      "10.637106 3.7877576 6.849348 30\n",
      "9.1785145 4.3083777 4.8701363 31\n",
      "4.0853567 2.4841983 1.6011586 32\n",
      "17.07965 5.0748515 12.004799 33\n",
      "21.1683 2.3173375 18.850964 34\n",
      "14.866617 3.884129 10.982488 35\n",
      "11.1138 3.1734266 7.9403734 36\n",
      "8.160222 3.5074036 4.6528187 37\n",
      "11.43048 3.6775517 7.7529283 38\n",
      "2.415597 1.2016435 1.2139536 39\n",
      "12.693186 4.1111827 8.582004 40\n",
      "11.010422 4.9209466 6.0894747 41\n",
      "12.81716 6.900769 5.9163904 42\n",
      "8.202844 4.411741 3.7911031 43\n",
      "17.544746 6.9195647 10.625182 44\n",
      "9.1502495 3.8469236 5.3033257 45\n",
      "10.24074 3.9785497 6.2621903 46\n",
      "13.046071 5.347469 7.698602 47\n",
      "4.9578886 1.1540396 3.803849 48\n",
      "1.4694669 0.81474143 0.65472543 49\n",
      "9.388227 4.7961187 4.5921082 50\n",
      "14.016392 5.364062 8.652329 51\n",
      "8.175716 4.289188 3.8865287 52\n",
      "4.966511 4.140411 0.8260997 53\n",
      "2.819669 2.3260171 0.49365178 54\n",
      "2.013692 1.8702345 0.14345734 55\n",
      "8.711527 4.505128 4.2063985 56\n",
      "5.0167665 3.3874092 1.6293576 57\n",
      "1.5958743 1.1234868 0.47238758 58\n",
      "3.426989 2.3707757 1.0562133 59\n",
      "11.300701 5.6132746 5.6874266 60\n",
      "5.559713 1.7573 3.8024127 61\n",
      "5.312851 3.6409338 1.6719173 62\n",
      "3.1253216 1.3711456 1.754176 63\n",
      "6.108803 4.9082737 1.200529 64\n",
      "9.495591 3.4354928 6.060098 65\n",
      "10.754288 3.3629806 7.391307 66\n",
      "2.844852 2.4618998 0.3829523 67\n",
      "9.363646 2.5863755 6.77727 68\n",
      "10.344092 6.7146196 3.6294727 69\n",
      "12.113222 1.5052509 10.607971 70\n",
      "9.892004 3.5455549 6.346449 71\n",
      "9.500828 4.211246 5.2895813 72\n",
      "8.693934 5.084476 3.6094582 73\n",
      "11.47557 3.7630413 7.7125287 74\n",
      "7.673448 2.1804276 5.4930205 75\n",
      "4.3617005 1.4509953 2.9107053 76\n",
      "11.325213 5.8916087 5.4336047 77\n",
      "3.5692508 1.0111401 2.5581107 78\n",
      "5.9871006 4.583749 1.403352 79\n",
      "4.4333487 2.1022239 2.3311245 80\n",
      "3.362021 2.1776648 1.1843562 81\n",
      "1.9932688 0.9444996 1.0487692 82\n",
      "3.2204366 1.8063669 1.4140695 83\n",
      "3.4180398 2.161329 1.2567106 84\n",
      "1.4614418 0.7310714 0.73037034 85\n",
      "1.3786579 0.84384745 0.5348105 86\n",
      "3.2704182 2.0218523 1.248566 87\n",
      "1.1884848 0.671464 0.51702076 88\n",
      "9.2424965 4.071789 5.170708 89\n",
      "4.670135 3.1798735 1.4902617 90\n",
      "8.432905 4.4924183 3.9404874 91\n",
      "3.08607 1.1108866 1.9751836 92\n",
      "2.517942 1.5683014 0.9496406 93\n",
      "1.540232 0.8450484 0.6951835 94\n",
      "8.917778 4.3976336 4.520145 95\n",
      "4.48038 2.207197 2.2731829 96\n",
      "2.3153982 1.5470917 0.7683065 97\n",
      "4.5679965 2.581571 1.9864253 98\n",
      "2.841947 1.3875996 1.4543475 99\n",
      "2.2411318 1.5541632 0.6869687 100\n",
      "6.0069294 2.496164 3.510765 101\n",
      "3.7092042 1.4694375 2.2397668 102\n",
      "10.420174 6.7234635 3.69671 103\n",
      "3.4281135 1.0571795 2.370934 104\n",
      "7.882057 4.1107163 3.7713408 105\n",
      "7.7635403 3.883382 3.8801584 106\n",
      "8.698618 5.0180182 3.6806 107\n",
      "5.654034 2.844287 2.8097472 108\n",
      "1.5734779 0.64738655 0.9260913 109\n",
      "7.2360497 4.360738 2.8753119 110\n",
      "5.944373 3.0733128 2.8710604 111\n",
      "3.7851627 2.5184157 1.266747 112\n",
      "3.00182 1.9613851 1.0404348 113\n",
      "2.1752791 1.566547 0.608732 114\n",
      "11.077978 6.6495514 4.4284263 115\n",
      "5.6669245 2.0197794 3.647145 116\n",
      "2.8667755 1.9974648 0.8693107 117\n",
      "1.2369189 0.604529 0.63238984 118\n",
      "4.552336 2.3110795 2.2412567 119\n",
      "1.8860626 0.9794001 0.9066626 120\n",
      "2.590314 1.9447607 0.64555323 121\n",
      "5.325433 2.5998037 2.7256289 122\n",
      "5.3478994 3.6834202 1.6644793 123\n",
      "1.876534 0.96224624 0.91428775 124\n",
      "2.4514565 1.7421675 0.70928895 125\n",
      "5.422744 3.2203393 2.2024047 126\n",
      "2.5941749 1.681865 0.91230994 127\n",
      "1.5529467 1.0262748 0.5266719 128\n",
      "8.139815 2.2756112 5.864204 129\n",
      "3.9765644 1.2023098 2.7742546 130\n",
      "2.9918983 2.0940444 0.8978538 131\n",
      "3.39499 1.9667429 1.4282471 132\n",
      "6.736147 3.0564792 3.6796677 133\n",
      "5.0347633 3.0607011 1.9740621 134\n",
      "4.5048723 2.383139 2.1217332 135\n",
      "7.9365387 5.287441 2.649098 136\n",
      "4.8638825 2.8392098 2.0246727 137\n",
      "4.8055935 1.8186667 2.9869266 138\n",
      "3.3545506 1.8948816 1.459669 139\n",
      "7.2422533 3.41465 3.8276033 140\n",
      "2.2883716 0.98183405 1.3065376 141\n",
      "1.0286398 0.4219739 0.6066659 142\n",
      "10.630573 4.3742094 6.2563634 143\n",
      "7.1119576 0.55626464 6.5556927 144\n",
      "3.2845433 1.8252596 1.4592837 145\n",
      "2.40635 1.3630031 1.0433468 146\n",
      "2.0305986 1.2945081 0.7360904 147\n",
      "4.2571507 2.0550196 2.2021313 148\n",
      "7.644095 2.8724961 4.771599 149\n",
      "2.8855882 1.0508288 1.8347594 150\n",
      "1.3735936 0.39268345 0.9809101 151\n",
      "3.3492754 2.433976 0.9152995 152\n",
      "3.4150887 1.269924 2.1451647 153\n",
      "4.002864 1.9598144 2.0430496 154\n",
      "2.163402 0.42495057 1.7384515 155\n",
      "1.8719244 1.1070698 0.7648545 156\n",
      "0.8826444 0.26301464 0.6196298 157\n",
      "8.025023 2.989137 5.0358863 158\n",
      "7.427818 3.6261392 3.8016787 159\n",
      "3.0965884 1.4126273 1.683961 160\n",
      "2.9960833 1.7361752 1.2599082 161\n",
      "3.5746727 1.8497406 1.724932 162\n",
      "6.3100643 2.0617082 4.2483563 163\n",
      "3.6247377 1.845588 1.7791498 164\n",
      "1.8755944 0.9177885 0.9578058 165\n",
      "1.6640211 0.88500494 0.7790162 166\n",
      "6.914733 2.399411 4.5153217 167\n",
      "3.350243 0.5426985 2.8075445 168\n",
      "3.7236977 2.4181855 1.3055121 169\n",
      "3.4131083 1.3960938 2.0170145 170\n",
      "1.7232025 0.5372996 1.1859028 171\n",
      "2.541339 1.6052303 0.9361085 172\n",
      "4.1633453 1.6537945 2.509551 173\n",
      "2.9211836 1.6573657 1.263818 174\n",
      "3.4657364 1.7468103 1.718926 175\n",
      "1.7255177 0.4595309 1.2659869 176\n",
      "2.5311177 1.4133637 1.117754 177\n",
      "1.7258422 0.5853558 1.1404865 178\n",
      "1.5359434 0.84065133 0.69529206 179\n",
      "1.0342833 0.3385516 0.69573164 180\n",
      "3.3463023 1.5321783 1.814124 181\n",
      "1.4870527 0.4520753 1.0349774 182\n",
      "4.887415 2.191218 2.6961973 183\n",
      "1.3253908 0.48181954 0.84357125 184\n",
      "8.0538025 2.6711726 5.38263 185\n",
      "3.8062725 0.6974905 3.108782 186\n",
      "3.9168005 2.1584988 1.7583019 187\n",
      "3.4738727 2.0706203 1.4032522 188\n",
      "1.4445937 0.5525854 0.8920083 189\n",
      "2.0383127 1.1527299 0.88558275 190\n",
      "1.8117156 1.1499511 0.6617645 191\n",
      "1.6570351 1.0149983 0.6420368 192\n",
      "1.0577276 0.5206757 0.5370518 193\n",
      "0.65314305 0.25242612 0.40071696 194\n",
      "1.7056648 0.9359073 0.76975745 195\n",
      "1.0297103 0.49405384 0.53565645 196\n",
      "3.9632432 1.7962303 2.167013 197\n",
      "3.918159 2.1812646 1.7368945 198\n",
      "1.3956287 0.89061016 0.5050186 199\n",
      "7.988452 3.9663675 4.022084 200\n",
      "3.6134648 0.69297504 2.9204898 201\n",
      "3.2886515 1.6426893 1.6459622 202\n",
      "2.1751194 0.7864527 1.3886666 203\n",
      "3.715743 1.9055487 1.8101945 204\n",
      "2.9175048 1.8454965 1.0720083 205\n",
      "1.8537238 0.5484508 1.3052729 206\n",
      "2.3059568 0.91135466 1.3946022 207\n",
      "2.403203 1.4439029 0.9593002 208\n",
      "1.7329202 0.6574686 1.0754516 209\n",
      "0.944389 0.4006401 0.5437489 210\n",
      "0.54400456 0.238729 0.30527556 211\n",
      "3.2264066 1.7437025 1.4827042 212\n",
      "1.0802333 0.46081802 0.6194153 213\n",
      "0.82305664 0.48541823 0.3376384 214\n",
      "0.8696656 0.62029797 0.24936762 215\n",
      "2.8782115 1.4715562 1.4066554 216\n",
      "1.1270635 0.45348454 0.673579 217\n",
      "0.90160835 0.4489092 0.45269915 218\n",
      "0.9682959 0.6656566 0.30263925 219\n",
      "3.2304103 1.251261 1.9791493 220\n",
      "1.577822 0.29736313 1.2804588 221\n",
      "4.8244567 2.4575493 2.3669074 222\n",
      "4.696682 2.8528173 1.8438646 223\n",
      "1.168938 0.46643415 0.7025039 224\n",
      "4.384383 2.7957141 1.588669 225\n",
      "2.3565154 1.3127846 1.0437307 226\n",
      "1.0961046 0.3519778 0.7441268 227\n",
      "1.0750842 0.6009211 0.47416306 228\n",
      "0.9671275 0.5826461 0.38448143 229\n",
      "1.3251425 0.70587623 0.6192663 230\n",
      "0.88370264 0.50537187 0.37833074 231\n",
      "0.8428737 0.40710488 0.4357688 232\n",
      "1.1361564 0.89543456 0.24072184 233\n",
      "11.109108 2.8135233 8.295585 234\n",
      "4.544799 0.9080982 3.6367004 235\n",
      "5.0450163 3.2870896 1.7579265 236\n",
      "1.6451001 0.42532822 1.2197719 237\n",
      "0.8173426 0.3066002 0.51074237 238\n",
      "4.354966 1.9767741 2.378192 239\n",
      "1.4624872 0.20627318 1.256214 240\n",
      "3.3762262 1.9481947 1.4280314 241\n",
      "1.8713872 1.0608897 0.8104975 242\n",
      "3.0874817 1.8134823 1.2739995 243\n",
      "2.1283445 0.62746924 1.5008754 244\n",
      "2.077835 0.9262806 1.1515543 245\n",
      "1.7592511 0.81694204 0.94230914 246\n",
      "1.2219963 0.48632604 0.7356703 247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4406872 1.0342064 1.4064809 248\n",
      "1.3269916 0.63248366 0.69450796 249\n",
      "3.4607441 1.8839397 1.5768044 250\n",
      "1.2820255 0.3860027 0.8960228 251\n",
      "0.93951344 0.40708938 0.53242403 252\n",
      "0.78828156 0.1851381 0.60314345 253\n",
      "0.626389 0.23865819 0.38773087 254\n",
      "0.60376394 0.26404878 0.33971512 255\n",
      "4.054513 2.7288988 1.3256142 256\n",
      "1.3824065 0.47158605 0.9108205 257\n",
      "1.1651658 0.6341917 0.5309741 258\n",
      "1.6435934 1.145217 0.4983765 259\n",
      "2.0747485 1.3467638 0.72798467 260\n",
      "0.7435994 0.39207613 0.35152328 261\n",
      "0.69522 0.49837938 0.19684063 262\n",
      "0.6846077 0.35501686 0.32959083 263\n",
      "0.59557474 0.2683575 0.32721728 264\n",
      "0.40760174 0.25913915 0.1484626 265\n",
      "0.76310813 0.4997312 0.26337695 266\n",
      "6.2658215 1.1580429 5.1077785 267\n",
      "2.5129523 1.6564983 0.85645413 268\n",
      "1.7169738 1.1079017 0.6090721 269\n",
      "0.8760558 0.38830745 0.48774832 270\n",
      "1.0260636 0.6597936 0.36626992 271\n",
      "1.6359886 0.975552 0.6604366 272\n",
      "0.84268975 0.37627813 0.46641162 273\n",
      "2.0015724 0.9454867 1.0560856 274\n",
      "1.0203508 0.41401514 0.6063357 275\n",
      "0.5510338 0.1332874 0.4177464 276\n",
      "0.55705345 0.21528304 0.34177044 277\n",
      "1.0371608 0.8360942 0.20106651 278\n",
      "0.7172067 0.4166774 0.30052936 279\n",
      "0.4457678 0.29519328 0.1505745 280\n",
      "0.64885247 0.38485146 0.26400098 281\n",
      "5.0410976 1.9234349 3.117663 282\n",
      "1.868479 1.2129107 0.65556836 283\n",
      "1.3274038 0.78794664 0.5394571 284\n",
      "0.6330017 0.34968624 0.28331542 285\n",
      "0.5460555 0.2492467 0.2968088 286\n",
      "0.48075813 0.27629685 0.20446128 287\n",
      "0.67688894 0.46790954 0.20897938 288\n",
      "0.6624665 0.42306933 0.2393972 289\n",
      "0.8631247 0.75342304 0.10970172 290\n",
      "0.44457084 0.37249944 0.07207139 291\n",
      "14.118374 2.3792524 11.739121 292\n",
      "4.796112 2.091942 2.70417 293\n",
      "3.6046762 2.7677653 0.8369111 294\n",
      "1.4590409 0.65355456 0.8054863 295\n",
      "1.1290617 0.42497382 0.7040879 296\n",
      "4.9899745 2.246504 2.7434704 297\n",
      "3.6602385 0.9151146 2.7451239 298\n",
      "1.5644604 0.8745171 0.6899433 299\n",
      "1.0120516 0.483255 0.52879655 300\n",
      "0.5943804 0.19342104 0.40095937 301\n",
      "2.3639874 1.1958432 1.1681443 302\n",
      "3.487782 2.1743202 1.3134619 303\n",
      "1.4310935 0.7635351 0.6675584 304\n",
      "1.9849308 0.9214892 1.0634416 305\n",
      "0.78565747 0.24579386 0.5398636 306\n",
      "0.9158751 0.46965224 0.44622287 307\n",
      "1.0757675 0.5368399 0.5389276 308\n",
      "0.85821414 0.49339905 0.36481506 309\n",
      "0.5865617 0.25848398 0.32807773 310\n",
      "0.6778048 0.39726672 0.28053808 311\n",
      "1.1445682 0.8994506 0.24511766 312\n",
      "3.2156193 1.1958425 2.0197768 313\n",
      "1.6787114 1.0148624 0.66384894 314\n",
      "1.4619505 0.9340106 0.52794 315\n",
      "0.8658638 0.44787517 0.41798863 316\n",
      "3.3302097 1.6037 1.7265098 317\n",
      "1.4409409 0.79836243 0.64257836 318\n",
      "0.7005424 0.26642367 0.43411872 319\n",
      "4.2733517 3.465405 0.8079467 320\n",
      "0.758953 0.3376771 0.4212759 321\n",
      "1.528636 0.9371187 0.5915173 322\n",
      "0.88076967 0.47193438 0.4088353 323\n",
      "0.57439387 0.23723073 0.33716315 324\n",
      "1.0330372 0.5251014 0.5079357 325\n",
      "0.99660766 0.8088156 0.18779203 326\n",
      "1.4776496 0.96986645 0.5077831 327\n",
      "1.6896836 1.0479755 0.641708 328\n",
      "1.1139365 0.72271824 0.39121833 329\n",
      "1.7563004 0.7278045 1.028496 330\n",
      "1.031729 0.57942975 0.4522992 331\n",
      "1.2817575 0.7970355 0.48472196 332\n",
      "0.94030535 0.47115317 0.46915218 333\n",
      "2.5838346 1.5740489 1.0097857 334\n",
      "1.2861068 0.7045214 0.5815854 335\n",
      "0.9817225 0.7516624 0.23006012 336\n",
      "2.418992 1.438092 0.98090005 337\n",
      "1.6657188 0.9223079 0.7434109 338\n",
      "1.0981797 0.626295 0.47188476 339\n",
      "0.6636899 0.4597704 0.2039195 340\n",
      "0.865649 0.4535708 0.4120782 341\n",
      "0.7092897 0.34241772 0.366872 342\n",
      "0.7299279 0.43628874 0.29363915 343\n",
      "0.4806025 0.29533792 0.18526457 344\n",
      "1.7198856 1.4914106 0.22847503 345\n",
      "0.53887725 0.2654946 0.27338263 346\n",
      "0.52376544 0.34401906 0.17974636 347\n",
      "0.50333965 0.37719345 0.12614621 348\n",
      "1.443367 1.2747425 0.16862454 349\n",
      "0.3426376 0.22511981 0.1175178 350\n",
      "5.4649434 1.7460774 3.7188659 351\n",
      "1.1120324 0.5495143 0.5625181 352\n",
      "1.0556641 0.4751075 0.5805565 353\n",
      "1.1361701 0.66003305 0.47613716 354\n",
      "0.66466403 0.39003092 0.2746331 355\n",
      "0.312349 0.19090633 0.12144266 356\n",
      "0.20517206 0.1299675 0.07520457 357\n",
      "0.77482533 0.65001905 0.124806315 358\n",
      "0.34768897 0.1968426 0.15084636 359\n",
      "6.257532 3.3177128 2.939819 360\n",
      "1.4974082 0.35085887 1.1465493 361\n",
      "3.6156602 2.8997543 0.71590596 362\n",
      "1.7144738 1.1795343 0.5349395 363\n",
      "0.57762027 0.30586192 0.27175832 364\n",
      "2.7617404 1.7687062 0.9930343 365\n",
      "1.5917002 1.1405797 0.4511205 366\n",
      "0.56260586 0.36872572 0.19388014 367\n",
      "4.275642 2.4556897 1.8199521 368\n",
      "0.8979273 0.6723847 0.22554263 369\n",
      "1.4974077 1.2772547 0.22015303 370\n",
      "0.6227249 0.28350776 0.33921716 371\n",
      "1.1294062 0.91054434 0.21886191 372\n",
      "0.3380707 0.2648618 0.07320889 373\n",
      "0.5249863 0.3507533 0.17423302 374\n",
      "0.21286438 0.13452849 0.078335896 375\n",
      "0.26897007 0.20041573 0.06855434 376\n",
      "0.21104392 0.17965011 0.031393804 377\n",
      "0.7694614 0.5667586 0.20270279 378\n",
      "0.31379873 0.18908808 0.124710664 379\n",
      "0.29108137 0.2402801 0.050801266 380\n",
      "0.21723776 0.15440966 0.062828094 381\n",
      "0.2763252 0.24671833 0.02960686 382\n",
      "7.210083 3.2960324 3.9140508 383\n",
      "0.83897597 0.49300885 0.3459671 384\n",
      "0.8272573 0.5510582 0.27619907 385\n",
      "3.552184 2.7445247 0.8076595 386\n",
      "2.0710301 1.6082013 0.46282881 387\n",
      "0.38696098 0.17978314 0.20717783 388\n",
      "0.24756026 0.09624234 0.15131792 389\n",
      "2.6485214 1.9189268 0.7295946 390\n",
      "1.7442936 1.4092758 0.33501777 391\n",
      "0.9095378 0.63475347 0.27478433 392\n",
      "1.2033029 0.8739607 0.32934213 393\n",
      "1.1872745 0.64283 0.5444445 394\n",
      "0.53070086 0.2625762 0.26812467 395\n",
      "4.385825 2.6047864 1.7810385 396\n",
      "4.689244 1.9408273 2.7484167 397\n",
      "1.8585865 1.2277589 0.63082767 398\n",
      "0.74462265 0.2070564 0.53756624 399\n",
      "0.7017691 0.4039494 0.2978197 400\n",
      "0.41730824 0.16726404 0.2500442 401\n",
      "0.99239475 0.68534756 0.3070472 402\n",
      "0.46000987 0.1897833 0.27022657 403\n",
      "0.49714813 0.29130095 0.20584717 404\n",
      "1.1487105 0.9410578 0.20765266 405\n",
      "0.5877774 0.37802586 0.20975153 406\n",
      "0.29330534 0.13197418 0.16133116 407\n",
      "0.6363863 0.32306 0.3133263 408\n",
      "0.61625785 0.43596655 0.18029131 409\n",
      "0.43837306 0.33485076 0.10352229 410\n",
      "0.52118635 0.41777545 0.10341093 411\n",
      "0.48251015 0.32531556 0.15719457 412\n",
      "5.972368 2.511923 3.4604447 413\n",
      "3.339113 2.8098152 0.5292978 414\n",
      "5.1766553 3.2220478 1.9546075 415\n",
      "1.3595657 0.32923245 1.0303333 416\n",
      "1.4916098 0.9324724 0.5591374 417\n",
      "4.323818 2.518266 1.805552 418\n",
      "3.697405 2.1361172 1.5612879 419\n",
      "1.5578892 1.0048105 0.55307883 420\n",
      "0.67855084 0.25953692 0.41901395 421\n",
      "0.8481604 0.4394989 0.40866148 422\n",
      "1.1458856 0.5708454 0.57504016 423\n",
      "0.81441355 0.39283648 0.42157707 424\n",
      "0.50532806 0.20086955 0.3044585 425\n",
      "0.7967232 0.4620211 0.33470207 426\n",
      "0.4400829 0.24620433 0.19387859 427\n",
      "0.37527063 0.19935124 0.1759194 428\n",
      "2.0432558 0.76550364 1.277752 429\n",
      "2.3493207 1.6273426 0.7219781 430\n",
      "4.064727 2.4836977 1.5810294 431\n",
      "0.5877764 0.2367001 0.3510763 432\n",
      "0.5922595 0.20406507 0.38819447 433\n",
      "0.32392192 0.14896676 0.17495516 434\n",
      "3.3671703 2.0997944 1.2673761 435\n",
      "2.1777623 1.7094685 0.46829385 436\n",
      "0.8849615 0.5170377 0.36792383 437\n",
      "0.872344 0.57042736 0.3019167 438\n",
      "1.7645413 0.92568034 0.8388609 439\n",
      "0.9485522 0.7140586 0.23449361 440\n",
      "0.5390227 0.31244704 0.22657563 441\n",
      "1.7490921 0.61007255 1.1390196 442\n",
      "0.38272804 0.18726368 0.19546436 443\n",
      "0.49002826 0.1893792 0.30064905 444\n",
      "0.7438194 0.51680964 0.22700979 445\n",
      "0.46060443 0.2868618 0.17374264 446\n",
      "0.3133392 0.14544983 0.16788937 447\n",
      "0.39731026 0.23816569 0.15914455 448\n",
      "0.2266584 0.15159695 0.07506145 449\n",
      "1.8219087 1.3105853 0.5113234 450\n",
      "0.6433496 0.3609061 0.2824435 451\n",
      "0.43644723 0.18883705 0.24761018 452\n",
      "0.46290112 0.24800129 0.21489982 453\n",
      "0.34221655 0.21014567 0.13207087 454\n",
      "0.19046663 0.10739452 0.08307211 455\n",
      "2.3210015 1.5347149 0.7862866 456\n",
      "1.0406364 0.7353093 0.30532715 457\n",
      "1.1843615 0.8194227 0.36493874 458\n",
      "0.31670243 0.11913407 0.19756836 459\n",
      "1.1422207 0.621459 0.5207617 460\n",
      "0.37220764 0.22536571 0.14684191 461\n",
      "0.6447677 0.48914656 0.15562113 462\n",
      "0.81611586 0.63252324 0.18359259 463\n",
      "0.9803661 0.62104267 0.35932347 464\n",
      "0.37649894 0.19408982 0.18240911 465\n",
      "0.53904766 0.29984066 0.23920701 466\n",
      "0.47412142 0.3718643 0.10225714 467\n",
      "0.18325898 0.0938601 0.08939889 468\n",
      "0.44014543 0.36146566 0.078679755 469\n",
      "0.39915115 0.33122692 0.06792424 470\n",
      "0.63662225 0.6001099 0.036512364 471\n",
      "0.89242035 0.5005018 0.39191854 472\n",
      "0.76416236 0.68768466 0.07647771 473\n",
      "0.36979493 0.2810031 0.08879185 474\n",
      "0.4120383 0.31120357 0.10083472 475\n",
      "0.1594984 0.09152653 0.067971855 476\n",
      "0.6537708 0.5479501 0.10582072 477\n",
      "0.23206912 0.18193597 0.050133158 478\n",
      "2.2375205 0.5579653 1.6795552 479\n",
      "1.6626365 1.3630143 0.29962215 480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61436117 0.35718313 0.257178 481\n",
      "1.1484486 0.8241683 0.32428023 482\n",
      "1.258877 0.82901996 0.42985713 483\n",
      "0.31248578 0.18459249 0.1278933 484\n",
      "0.5516671 0.3723816 0.17928553 485\n",
      "0.25206012 0.14424111 0.10781901 486\n",
      "0.7035058 0.6644462 0.039059646 487\n",
      "0.28118846 0.18192565 0.099262804 488\n",
      "0.34774333 0.18938029 0.15836306 489\n",
      "2.2115037 1.5007322 0.7107715 490\n",
      "0.18897127 0.044304233 0.14466703 491\n",
      "0.77002615 0.61512035 0.15490578 492\n",
      "1.8547279 1.3745437 0.48018417 493\n",
      "1.1885451 0.92866385 0.25988126 494\n",
      "0.7184005 0.51837003 0.20003045 495\n",
      "0.4280553 0.32557312 0.102482185 496\n",
      "0.92829967 0.7192505 0.20904917 497\n",
      "0.36880365 0.19261716 0.17618649 498\n",
      "0.2883331 0.16587643 0.122456655 499\n"
     ]
    }
   ],
   "source": [
    "#Train network\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t, x_interior, x_initial = sampler(nSim_t, nSim_x_interior, nSim_x_initial)\n",
    "    \n",
    "    for j in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_tnsr:t, x_interior_tnsr:x_interior, x_initial_tnsr:x_initial})\n",
    "        \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.save(sess, './FokkerPlack/' + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow1.7",
   "language": "python",
   "name": "tensorflow1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
