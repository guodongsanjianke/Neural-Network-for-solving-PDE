{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method (DGM)\n",
    "### S1 = sigma(w1*x + b1)  Z(l) = sigma(u*x + w*S + b) l=1,...,L  G(l) = sigma(u*x + w*S + b) l=1,...,L  \n",
    "### R(l) = sigma(u*x + w*S + b) l=1,...,L   H(l) = sigma(u*x + w*(S Hadamard R) + b)  l=1,...,L  \n",
    "### S(L+1) = (1-G) Hadamard H + Z Hadamard S  f = w*S(L+1) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of imput data  \n",
    "**output_dim:** number of output for LSTM layer  \n",
    "**trans1, trans2 (str):** activation functions used inside the layer;  \n",
    "one of: \"tanh\"(default), \"relu\" or \"sigmoid\"  \n",
    "**u vectors:** weighting vectors for inputs original inputs x  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**S:** output of previous layer  \n",
    "**X:** data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-like layer used in DGM\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        \n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    def call(self, S, X):\n",
    "\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of input data  \n",
    "**output_dim:** number of outputs for dense layer  \n",
    "**transformation:** activation function used inside the layer; using None is equivalent to the identity map  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**X:** input to layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer(dense)\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        \n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    def call(self,X):\n",
    "        \n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_layers:** number of intermediate LSTM layers  \n",
    "**input_dim:** spaital dimension of input data (excludes time dimension)  \n",
    "**final_trans:** transformation used in final layer\n",
    "define initial layer as fully connected\n",
    "to account for time inputs we use input_dim+1 as the input dimension\n",
    "**t:** sampled time inputs  \n",
    "**x:** sampled space inputs  \n",
    "Run the DGM model and obtain fitted function value at the inputs (t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture used in DGM\n",
    "\n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        \n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1))\n",
    "        \n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    def call(self,t,x):\n",
    "\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process parameters（Ornstein-Uhlenbeck Process）\n",
    "kappa = 0\n",
    "theta = 0.5\n",
    "sigma = 2\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "# tenminal time\n",
    "T = 1.0\n",
    "\n",
    "# bounds of sampling region for space dimension, i.e. sampling will be done on\n",
    "# [multipliter*Xlow, multiplier*Xhigh]\n",
    "Xlow = -4.0\n",
    "Xhigh = 4.0\n",
    "x_multiplier = 2.0\n",
    "t_multiplier = 1.5\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages = 500\n",
    "steps_per_sample = 10\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_t = 5\n",
    "nSim_x_interior = 50\n",
    "nSim_x_initial = 50\n",
    "\n",
    "# Save options\n",
    "saveName = 'FokkerPlanck'\n",
    "saveFigure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OU Simulation function(Ornstein-Uhlenbeck Process)**  \n",
    "Simulate end point of Ornstein-Uhlenbeck process with normally distributed random starting value  \n",
    "**alpha:** mean of random starting value  \n",
    "**beta:** standard deviation of random starting value   \n",
    "**theta:** mean reversion level    \n",
    "**kappa:** mean reversion rate  \n",
    "**sigma:** volatility  \n",
    "**nSim:** number of simulations  \n",
    "**T:** terminal time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(alpha, beta, theta, kappa, sigma, nSim, T):\n",
    "    \n",
    "    # simulate initial point based on normal distribution\n",
    "    X0 = np.random.normal(loc = alpha, scale = beta, size = nSim)\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta + (X0 - theta) * np.exp(-kappa * T)\n",
    "    v = np.sqrt(sigma**2 / (2 * kappa) * (1 - np.exp(-2*kappa*T)))\n",
    "    \n",
    "    # simulate endpoint\n",
    "    Xt = np.random.normal(m,v)    \n",
    "    \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample time-space points from the function's domain;  \n",
    "point are sampled uniformly on the interior of the domain, at the initial/terminal time points  \n",
    "and along the spatial boundary at different time points.  \n",
    "**nSim_t:** number of (interior) time points to sample  \n",
    "**nSim_x_interior:** number of space points in the interior of the function's domain to sample  \n",
    "**nSim_x_initial:** number of space points at initial time to sample (initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function - random sample time-space pairs\n",
    "def sampler(nSim_t, nSim_x_interior, nSim_x_initial):\n",
    "    \n",
    "    # Sampler1: domain interior\n",
    "    t = np.random.uniform(low=0, high=T*t_multiplier, size=[nSim_t, 1])\n",
    "    x_interior = np.random.uniform(low=Xlow*x_multiplier, high=Xhigh*x_multiplier, size=[nSim_x_interior, 1])\n",
    "    \n",
    "    # Sampler: spatial boundary\n",
    "    # no spatial boundary condition for this problem \n",
    "    \n",
    "    # Sampler3: initial/terminal condition\n",
    "    x_initial = np.random.uniform(low=Xlow*1.5, high=Xhigh*1.5, size = [nSim_x_initial, 1])\n",
    "    \n",
    "    return t, x_interior, x_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute total loss for training.     \n",
    "The loss is based o the PDE satisfied by the negative-exponential of the density and NOT the density   \n",
    "itself, i.e. the u(t,x) in p(t,x) = exp(-u(t,x)) / c(t) where p is the density and c is the normalization constant.    \n",
    "**model:** DGM model object   \n",
    "**t:** sampled (interior) time points  \n",
    "**x_interior:** sampled space points in the interior of the function's domain   \n",
    "**x_initial:** sampled space points at initial time   \n",
    "**nSim_t:** number of (interior) time points sampled (size of t)  \n",
    "**alpha:** mean of normal distribution for process staring value  \n",
    "**beta:** standard deviation of normal distribution for process starting value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t, x_interior, x_initial, nSim_t, alpha, beta):\n",
    "    \n",
    "    # Loss term1: PDE\n",
    "    \n",
    "    # initialize vector of losses\n",
    "    losses_u = []\n",
    "    \n",
    "    # for each simulated interior time point\n",
    "    for tIndex in range(nSim_t):\n",
    "        \n",
    "        curr_t = t[tIndex]\n",
    "        t_vector = curr_t * tf.ones_like(x_interior)\n",
    "        \n",
    "        u    = model.call(t_vector, x_interior)\n",
    "        u_t  = tf.gradients(u, t_vector)[0]\n",
    "        u_x  = tf.gradients(u, x_interior)[0]\n",
    "        u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "        psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "        psi = tf.reduce_sum( u_t*tf.exp(-u) ) / psi_denominator\n",
    "\n",
    "        # PDE differential operator\n",
    "        diff_f = -u_t + kappa - kappa*(x_interior- theta)*u_x - 0.5*sigma**2*(-u_xx + u_x**2) + psi\n",
    "        \n",
    "        # compute L2-norm of differential operator and attach to vector of losses\n",
    "        currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "        losses_u.append(currLoss)\n",
    "    \n",
    "    # average losses across sample time points \n",
    "    L1 = tf.add_n(losses_u) / nSim_t\n",
    "    \n",
    "    # Loss term2: boundary condition\n",
    "    # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term3: initial condition\n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0 i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    fitted_pdf = model.call(0*tf.ones_like(x_initial), x_initial)\n",
    "    \n",
    "    target_pdf  = 0.5*(x_initial - alpha)**2 / (beta**2)\n",
    "    \n",
    "    # average L2 error for initial distribution\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "    return L1, L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input(time, space domain interior, space domain at initial time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Set up network\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "t_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_initial_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_tnsr, x_interior_tnsr, x_initial_tnsr, nSim_t, alpha, beta)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "u = model.call(t_tnsr, x_interior_tnsr)\n",
    "p_unnorm = tf.exp(-u)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.964574 1.9752108 7.989363 0\n",
      "7.558194 2.5371182 5.021076 1\n",
      "6.140235 1.9708465 4.1693883 2\n",
      "2.6498854 0.35510942 2.294776 3\n",
      "4.247659 1.5360676 2.7115917 4\n",
      "2.940885 0.7515363 2.1893487 5\n",
      "1.2956846 0.32190165 0.97378296 6\n",
      "4.532461 1.6162363 2.916225 7\n",
      "3.4336998 0.9740605 2.4596393 8\n",
      "1.8899381 0.63668245 1.2532556 9\n",
      "1.421838 0.29477286 1.1270652 10\n",
      "2.6204557 1.5101446 1.1103112 11\n",
      "2.302884 1.2689656 1.0339186 12\n",
      "1.2147251 0.4158629 0.7988622 13\n",
      "0.42135438 0.16873729 0.2526171 14\n",
      "0.46387473 0.1352252 0.32864952 15\n",
      "3.347396 2.161321 1.186075 16\n",
      "1.2096891 0.47464666 0.7350425 17\n",
      "0.5784742 0.26971483 0.3087594 18\n",
      "0.5177029 0.1217124 0.3959905 19\n",
      "0.5338627 0.323698 0.21016468 20\n",
      "0.3608293 0.15026066 0.21056862 21\n",
      "0.22447376 0.104209594 0.120264165 22\n",
      "0.11324502 0.022282617 0.0909624 23\n",
      "0.07513886 0.024736999 0.050401863 24\n",
      "0.7040769 0.32685435 0.3772225 25\n",
      "0.27144504 0.056406666 0.21503837 26\n",
      "0.2845407 0.20239297 0.08214776 27\n",
      "0.13123968 0.08004018 0.051199503 28\n",
      "0.09274319 0.028800456 0.06394273 29\n",
      "0.080109216 0.054023754 0.026085462 30\n",
      "0.18340895 0.150175 0.033233937 31\n",
      "0.054468036 0.023676982 0.030791054 32\n",
      "0.034797937 0.019969292 0.014828643 33\n",
      "0.079049304 0.040918652 0.038130656 34\n",
      "0.04114496 0.019086959 0.022058003 35\n",
      "0.062283754 0.037851367 0.024432385 36\n",
      "0.10237002 0.08176809 0.020601926 37\n",
      "0.033341877 0.0161883 0.017153576 38\n",
      "0.064781904 0.029636908 0.035145 39\n",
      "0.024828494 0.010252285 0.014576209 40\n",
      "0.040960208 0.026642282 0.014317927 41\n",
      "0.040484563 0.019629223 0.020855341 42\n",
      "0.025753666 0.014016491 0.011737175 43\n",
      "0.027170587 0.022780737 0.00438985 44\n",
      "0.05447459 0.03162656 0.022848029 45\n",
      "0.06982767 0.03756017 0.0322675 46\n",
      "0.013762955 0.006366712 0.0073962426 47\n",
      "0.46408096 0.40345445 0.060626496 48\n",
      "0.17297775 0.1245841 0.048393644 49\n",
      "0.05941377 0.02780427 0.031609498 50\n",
      "0.13697882 0.065762125 0.071216695 51\n",
      "0.11314522 0.046931487 0.066213734 52\n",
      "0.057933763 0.012248727 0.045685038 53\n",
      "0.029231433 0.008222102 0.021009332 54\n",
      "0.09607962 0.07890784 0.017171774 55\n",
      "0.03686951 0.023573086 0.013296425 56\n",
      "0.01868527 0.01229916 0.0063861106 57\n",
      "0.8164709 0.47192088 0.34455004 58\n",
      "0.23752554 0.17018643 0.06733911 59\n",
      "0.11584441 0.06554518 0.05029923 60\n",
      "0.089296326 0.028078547 0.06121778 61\n",
      "0.28994742 0.21467502 0.075272396 62\n",
      "0.06733123 0.009717241 0.05761399 63\n",
      "1.423861 0.49164173 0.9322193 64\n",
      "0.4150316 0.06123027 0.35380134 65\n",
      "0.24878454 0.089443795 0.15934074 66\n",
      "1.0353512 0.7172728 0.31807837 67\n",
      "0.40099138 0.13563527 0.2653561 68\n",
      "0.23394808 0.103516735 0.13043135 69\n",
      "0.18067501 0.036768433 0.14390658 70\n",
      "0.27173004 0.17198095 0.099749096 71\n",
      "0.69671285 0.44551346 0.25119936 72\n",
      "0.4424085 0.18731083 0.2550977 73\n",
      "0.31679112 0.20484257 0.111948535 74\n",
      "0.6059119 0.35793695 0.24797495 75\n",
      "0.3223278 0.12687977 0.19544804 76\n",
      "0.090119705 0.026349101 0.06377061 77\n",
      "0.11939277 0.057233553 0.062159214 78\n",
      "0.23201081 0.17490743 0.057103377 79\n",
      "0.107007384 0.020572176 0.086435206 80\n",
      "0.4046221 0.31692344 0.08769866 81\n",
      "1.0030129 0.6656998 0.33731315 82\n",
      "0.49763525 0.36648095 0.1311543 83\n",
      "0.25962058 0.12063861 0.13898197 84\n",
      "0.11447178 0.029482031 0.08498975 85\n",
      "0.106506445 0.03226067 0.07424577 86\n",
      "0.03594799 0.0077917413 0.028156247 87\n",
      "0.62279475 0.21155477 0.41124 88\n",
      "0.542057 0.30352315 0.23853382 89\n",
      "0.334601 0.22610235 0.10849867 90\n",
      "0.14382604 0.05001008 0.09381595 91\n",
      "0.10711826 0.058752287 0.04836597 92\n",
      "0.08584774 0.04859842 0.037249323 93\n",
      "0.02520347 0.015983544 0.009219926 94\n",
      "0.18548058 0.10567404 0.07980654 95\n",
      "0.16360584 0.041174095 0.12243175 96\n",
      "0.0857321 0.045757204 0.039974894 97\n",
      "0.09776934 0.047136895 0.050632447 98\n",
      "0.05779788 0.030674374 0.027123505 99\n",
      "0.053107657 0.027059847 0.026047809 100\n",
      "0.043126494 0.021562835 0.02156366 101\n",
      "0.048212547 0.026414448 0.021798098 102\n",
      "0.03175628 0.013241555 0.018514726 103\n",
      "0.044756413 0.020490427 0.024265988 104\n",
      "0.070423834 0.04803353 0.0223903 105\n",
      "0.026884018 0.0070306496 0.019853368 106\n",
      "0.023365103 0.006622349 0.016742755 107\n",
      "0.42303377 0.2510235 0.17201027 108\n",
      "0.2554389 0.19899447 0.05644443 109\n",
      "0.08367123 0.028105443 0.055565786 110\n",
      "0.03603177 0.009336746 0.026695024 111\n",
      "0.04764407 0.012599835 0.035044238 112\n",
      "0.039480597 0.029149426 0.01033117 113\n",
      "0.029425308 0.011257404 0.018167904 114\n",
      "0.06494085 0.05679963 0.008141215 115\n",
      "0.14069752 0.085855596 0.054841932 116\n",
      "0.066352814 0.019843774 0.046509042 117\n",
      "0.026567783 0.010458347 0.016109435 118\n",
      "0.026075546 0.014758884 0.011316663 119\n",
      "0.017113904 0.0019374762 0.015176427 120\n",
      "0.020131465 0.003991989 0.016139476 121\n",
      "0.013947201 0.00610336 0.007843841 122\n",
      "0.00788402 0.004255506 0.0036285135 123\n",
      "0.04592953 0.036068507 0.009861021 124\n",
      "0.19179107 0.14524762 0.046543445 125\n",
      "0.04158919 0.017613696 0.023975493 126\n",
      "0.51413184 0.19714893 0.3169829 127\n",
      "0.33181167 0.12913875 0.20267291 128\n",
      "0.35815617 0.2971022 0.06105397 129\n",
      "0.11625688 0.07146461 0.044792265 130\n",
      "0.040405788 0.01398347 0.026422316 131\n",
      "0.17622457 0.14592902 0.030295556 132\n",
      "0.1012091 0.045296837 0.05591226 133\n",
      "0.10714355 0.08232292 0.024820633 134\n",
      "0.057564735 0.024943879 0.03262086 135\n",
      "0.020652715 0.008136894 0.012515821 136\n",
      "0.066518486 0.038599808 0.027918674 137\n",
      "0.0169467 0.007319588 0.009627112 138\n",
      "0.02071147 0.0049777417 0.015733728 139\n",
      "0.57070833 0.5290032 0.041705146 140\n",
      "0.09301771 0.036151905 0.056865808 141\n",
      "0.092241265 0.058896005 0.03334526 142\n",
      "0.04827003 0.022750413 0.025519619 143\n",
      "0.025535723 0.013383972 0.012151751 144\n",
      "0.025642443 0.013283375 0.01235907 145\n",
      "0.089521326 0.043732863 0.045788463 146\n",
      "0.07912752 0.038931735 0.040195785 147\n",
      "0.074865475 0.044487603 0.03037787 148\n",
      "0.09119983 0.061878707 0.02932112 149\n",
      "0.016926128 0.008233246 0.008692882 150\n",
      "0.054040134 0.042346746 0.011693388 151\n",
      "0.016246289 0.011695561 0.0045507285 152\n",
      "0.012343433 0.0074036852 0.004939748 153\n",
      "0.02124932 0.007830833 0.013418488 154\n",
      "0.054787353 0.03539501 0.019392341 155\n",
      "0.021472204 0.01352667 0.007945534 156\n",
      "0.097136915 0.089596316 0.007540601 157\n",
      "0.1240309 0.053329606 0.0707013 158\n",
      "0.0671586 0.04499754 0.022161061 159\n",
      "0.023353186 0.007382496 0.01597069 160\n",
      "0.02132508 0.005769416 0.015555664 161\n",
      "0.04784038 0.036638845 0.011201536 162\n",
      "0.044741377 0.033559114 0.011182264 163\n",
      "0.096251756 0.08452384 0.011727918 164\n",
      "0.5664394 0.41935655 0.14708284 165\n",
      "0.46297204 0.27041116 0.1925609 166\n",
      "0.07729016 0.025079368 0.052210793 167\n",
      "0.09921856 0.04740944 0.05180912 168\n",
      "0.044571247 0.0357503 0.008820948 169\n",
      "0.09217731 0.06137803 0.030799279 170\n",
      "0.074252725 0.066004775 0.008247951 171\n",
      "0.10579546 0.06330968 0.04248578 172\n",
      "0.03014121 0.012837008 0.017304203 173\n",
      "0.12296797 0.07804374 0.04492424 174\n",
      "0.061337616 0.051259644 0.010077972 175\n",
      "0.05385635 0.030416712 0.023439636 176\n",
      "0.03539948 0.003462603 0.03193688 177\n",
      "0.11400228 0.08910779 0.02489449 178\n",
      "0.05129089 0.008943099 0.04234779 179\n",
      "0.013943844 0.009086812 0.004857031 180\n",
      "0.013005973 0.0036923059 0.009313667 181\n",
      "0.030308157 0.010358412 0.019949745 182\n",
      "0.046581946 0.029114638 0.017467309 183\n",
      "0.011669584 0.0076160217 0.0040535615 184\n",
      "0.00868753 0.0014211938 0.007266336 185\n",
      "0.03863067 0.02877199 0.009858681 186\n",
      "0.01777357 0.005750824 0.012022746 187\n",
      "0.039796796 0.033904698 0.0058920984 188\n",
      "0.01693672 0.010324067 0.006612652 189\n",
      "0.024064394 0.017408123 0.00665627 190\n",
      "0.12543789 0.10092405 0.02451383 191\n",
      "0.02469617 0.011123179 0.01357299 192\n",
      "0.13915542 0.07773136 0.06142405 193\n",
      "0.061016604 0.028796941 0.032219663 194\n",
      "0.1759519 0.1336718 0.042280097 195\n",
      "0.2119897 0.15253295 0.059456743 196\n",
      "0.08605554 0.060928907 0.025126634 197\n",
      "0.02613919 0.015282646 0.010856545 198\n",
      "0.11441979 0.05085128 0.0635685 199\n",
      "0.034268048 0.01286899 0.021399058 200\n",
      "0.3043925 0.13016956 0.17422293 201\n",
      "0.06516033 0.04633322 0.018827109 202\n",
      "0.034289025 0.015196839 0.019092184 203\n",
      "0.050718833 0.041689824 0.009029009 204\n",
      "0.08726748 0.05868593 0.028581552 205\n",
      "0.061796088 0.032714125 0.029081963 206\n",
      "0.025385968 0.014533484 0.010852483 207\n",
      "0.020526709 0.0083363345 0.012190375 208\n",
      "0.014552442 0.010162306 0.004390136 209\n",
      "0.07788478 0.061485272 0.01639951 210\n",
      "0.015515053 0.007684429 0.007830624 211\n",
      "0.011610339 0.0021874448 0.009422895 212\n",
      "0.021197874 0.0060468176 0.015151056 213\n",
      "0.07210744 0.05560137 0.01650607 214\n",
      "0.35092616 0.284762 0.06616418 215\n",
      "0.06112451 0.024790702 0.03633381 216\n",
      "0.021117732 0.012875534 0.008242197 217\n",
      "0.21055989 0.14188246 0.068677425 218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3952098 0.2592 0.1360098 219\n",
      "0.095554054 0.05615523 0.03939882 220\n",
      "0.12713535 0.0877558 0.039379552 221\n",
      "0.042312276 0.028957838 0.013354437 222\n",
      "0.018804459 0.007921945 0.010882514 223\n",
      "0.0526113 0.036474776 0.016136521 224\n",
      "0.019459233 0.006611896 0.012847337 225\n",
      "0.088387676 0.0743924 0.013995277 226\n",
      "0.037566934 0.026237205 0.011329728 227\n",
      "0.033574544 0.014671594 0.018902952 228\n",
      "0.012020422 0.006501447 0.0055189747 229\n",
      "0.057590246 0.047848094 0.009742153 230\n",
      "0.047537927 0.035218336 0.012319592 231\n",
      "0.0358931 0.032557543 0.0033355572 232\n",
      "0.030280977 0.018624753 0.011656225 233\n",
      "0.016398013 0.010526792 0.00587122 234\n",
      "0.0690191 0.058154594 0.010864506 235\n",
      "0.06748232 0.04723693 0.020245388 236\n",
      "0.05630513 0.03108415 0.025220979 237\n",
      "0.025405817 0.013770938 0.011634879 238\n",
      "1.0990931 0.66152114 0.43757194 239\n",
      "0.26982474 0.17359026 0.09623449 240\n",
      "1.5879974 0.9678168 0.6201806 241\n",
      "0.8178815 0.22363822 0.5942433 242\n",
      "0.1881536 0.055411465 0.13274214 243\n",
      "0.089475796 0.02185307 0.06762272 244\n",
      "0.20541522 0.1389434 0.06647181 245\n",
      "0.09703097 0.06158316 0.035447802 246\n",
      "0.08347358 0.040055674 0.0434179 247\n",
      "0.06751786 0.039429106 0.028088756 248\n",
      "0.022321217 0.00845392 0.013867298 249\n",
      "0.028909309 0.0070181317 0.021891177 250\n",
      "0.017194588 0.0119061405 0.0052884473 251\n",
      "0.23841251 0.13305984 0.10535267 252\n",
      "0.16962701 0.06528085 0.10434617 253\n",
      "0.046948377 0.026612764 0.020335613 254\n",
      "0.030868165 0.009123555 0.02174461 255\n",
      "1.3484718 0.94060725 0.40786454 256\n",
      "0.50051534 0.31829762 0.18221773 257\n",
      "0.49982065 0.30162993 0.19819073 258\n",
      "0.19073115 0.0718149 0.11891625 259\n",
      "0.16716258 0.08890912 0.07825346 260\n",
      "0.066765875 0.026407978 0.040357895 261\n",
      "0.16748688 0.12753896 0.039947916 262\n",
      "0.06103368 0.010453917 0.050579764 263\n",
      "0.070278496 0.013688549 0.056589946 264\n",
      "0.10609649 0.037238896 0.068857595 265\n",
      "0.47645175 0.38134333 0.095108405 266\n",
      "0.17809132 0.12923567 0.04885564 267\n",
      "0.10402527 0.05449756 0.04952771 268\n",
      "0.047477655 0.029598309 0.017879348 269\n",
      "0.040431295 0.01023474 0.030196555 270\n",
      "0.026470516 0.005933653 0.020536862 271\n",
      "0.013903824 0.0035109574 0.010392866 272\n",
      "0.08959634 0.06773466 0.021861682 273\n",
      "0.04414299 0.018723562 0.025419429 274\n",
      "0.02756463 0.012846864 0.014717767 275\n",
      "0.060921222 0.040304292 0.020616932 276\n",
      "0.049156815 0.026299039 0.022857778 277\n",
      "0.013595432 0.008682056 0.004913376 278\n",
      "0.05344147 0.03266896 0.02077251 279\n",
      "0.020188766 0.006438914 0.013749852 280\n",
      "0.029749844 0.014928115 0.014821731 281\n",
      "0.026470477 0.021869106 0.004601371 282\n",
      "0.032953113 0.020076213 0.012876902 283\n",
      "0.013489716 0.004311649 0.009178067 284\n",
      "0.036817055 0.029624885 0.0071921707 285\n",
      "0.031226099 0.022208383 0.009017715 286\n",
      "0.020289708 0.009957264 0.010332444 287\n",
      "0.024756994 0.01300476 0.011752234 288\n",
      "0.08403235 0.0617473 0.022285046 289\n",
      "0.1018695 0.08810903 0.013760469 290\n",
      "0.10097489 0.086889386 0.014085504 291\n",
      "0.04862503 0.04171978 0.0069052493 292\n",
      "0.06592548 0.055064797 0.010860684 293\n",
      "0.031718537 0.027055895 0.004662643 294\n",
      "0.018238802 0.012013177 0.0062256237 295\n",
      "0.027871832 0.023264369 0.004607464 296\n",
      "0.0666572 0.057691474 0.008965728 297\n",
      "0.059022244 0.047644343 0.0113779 298\n",
      "0.049479283 0.018852472 0.03062681 299\n",
      "0.017907402 0.0069137835 0.010993618 300\n",
      "0.13182198 0.09129021 0.04053176 301\n",
      "0.085288666 0.068258636 0.017030032 302\n",
      "0.05597571 0.038530935 0.017444775 303\n",
      "0.033910032 0.021530762 0.01237927 304\n",
      "0.053557575 0.029171878 0.024385696 305\n",
      "0.010998818 0.0079492135 0.0030496048 306\n",
      "0.016531967 0.0098246215 0.0067073447 307\n",
      "0.021659877 0.013108362 0.008551514 308\n",
      "0.06620802 0.05686425 0.009343773 309\n",
      "0.03317571 0.020510662 0.012665047 310\n",
      "0.033240885 0.016694931 0.016545953 311\n",
      "0.013857999 0.0071499683 0.0067080306 312\n",
      "0.066891596 0.0389938 0.027897794 313\n",
      "0.013156447 0.004987997 0.00816845 314\n",
      "0.045015126 0.023610873 0.021404251 315\n",
      "0.037129365 0.017776577 0.019352788 316\n",
      "0.014633051 0.009128102 0.005504949 317\n",
      "0.012714548 0.0101255225 0.002589026 318\n",
      "0.01953002 0.013603336 0.0059266835 319\n",
      "0.0382628 0.02908042 0.00918238 320\n",
      "0.024915885 0.018796923 0.0061189616 321\n",
      "0.019270133 0.014010151 0.005259983 322\n",
      "0.06564273 0.052865233 0.012777493 323\n",
      "0.06047129 0.04285834 0.017612949 324\n",
      "0.023429658 0.012911341 0.010518318 325\n",
      "0.03426396 0.018547073 0.015716888 326\n",
      "0.07200534 0.058963127 0.013042209 327\n",
      "0.032761574 0.013126946 0.019634629 328\n",
      "0.022201864 0.015278253 0.0069236117 329\n",
      "0.05776262 0.04636031 0.011402308 330\n",
      "0.09078696 0.0808432 0.009943765 331\n",
      "0.02328658 0.018482288 0.0048042904 332\n",
      "0.012618813 0.005209189 0.007409624 333\n",
      "0.03265898 0.019509574 0.013149403 334\n",
      "0.01248551 0.009204431 0.0032810778 335\n",
      "0.022613024 0.013687539 0.008925485 336\n",
      "0.0074715544 0.0041323504 0.003339204 337\n",
      "0.049843557 0.0373273 0.012516257 338\n",
      "0.04944635 0.041272175 0.008174177 339\n",
      "0.020347651 0.011544402 0.008803248 340\n",
      "0.04675449 0.03105073 0.01570376 341\n",
      "0.10973746 0.08640218 0.023335285 342\n",
      "0.023147503 0.017570531 0.0055769724 343\n",
      "0.022583019 0.017641647 0.0049413713 344\n",
      "0.011289442 0.0058921264 0.005397316 345\n",
      "0.013345629 0.0096632745 0.0036823547 346\n",
      "0.005797124 0.004723421 0.0010737033 347\n",
      "0.0080474345 0.0073122107 0.0007352241 348\n",
      "0.006638308 0.0055068405 0.0011314676 349\n",
      "0.07681916 0.07342737 0.0033917893 350\n",
      "0.021159532 0.01645876 0.004700773 351\n",
      "0.03311366 0.027312119 0.00580154 352\n",
      "0.024522569 0.017003465 0.007519103 353\n",
      "0.011877281 0.009969496 0.0019077844 354\n",
      "0.09258535 0.05511197 0.037473377 355\n",
      "0.05702337 0.04824105 0.008782318 356\n",
      "0.024297662 0.017260382 0.00703728 357\n",
      "0.05682586 0.053727027 0.0030988348 358\n",
      "0.038225897 0.023554586 0.014671309 359\n",
      "0.021103092 0.014129152 0.006973941 360\n",
      "0.10736074 0.06255484 0.0448059 361\n",
      "0.07034507 0.060959827 0.009385241 362\n",
      "0.055773947 0.033732455 0.022041492 363\n",
      "0.03647155 0.023283897 0.013187652 364\n",
      "0.03627788 0.02469791 0.011579969 365\n",
      "0.00788088 0.005231625 0.0026492549 366\n",
      "0.01836973 0.013644052 0.004725678 367\n",
      "0.0066718594 0.005117472 0.0015543876 368\n",
      "0.029194856 0.02663185 0.0025630053 369\n",
      "0.009373707 0.005971782 0.0034019246 370\n",
      "0.0056107906 0.003987437 0.0016233535 371\n",
      "0.0115494905 0.008897456 0.0026520342 372\n",
      "0.018231045 0.011546116 0.0066849296 373\n",
      "0.013592524 0.01208651 0.0015060145 374\n",
      "0.0055787615 0.004032823 0.0015459387 375\n",
      "0.08821294 0.07629343 0.011919508 376\n",
      "0.030515675 0.024041316 0.006474359 377\n",
      "0.01132779 0.006545222 0.0047825687 378\n",
      "0.024629531 0.02137334 0.0032561913 379\n",
      "0.015258234 0.011005292 0.004252942 380\n",
      "0.022152303 0.015748201 0.0064041014 381\n",
      "0.018825747 0.01563774 0.0031880063 382\n",
      "0.0056269094 0.0042652413 0.001361668 383\n",
      "0.15101436 0.13902347 0.011990884 384\n",
      "0.029983329 0.004035653 0.025947675 385\n",
      "0.11994745 0.04965742 0.07029003 386\n",
      "0.33004215 0.1470269 0.18301526 387\n",
      "0.0762887 0.029250586 0.047038116 388\n",
      "0.023373751 0.00968084 0.013692911 389\n",
      "0.69840497 0.5694789 0.12892602 390\n",
      "0.4973755 0.3571434 0.1402321 391\n",
      "0.06470455 0.0059547373 0.058749817 392\n",
      "0.035198957 0.017550543 0.017648414 393\n",
      "0.79677916 0.5735737 0.22320548 394\n",
      "0.17627798 0.03958289 0.13669509 395\n",
      "0.14170216 0.044641577 0.09706058 396\n",
      "0.07650569 0.035974514 0.040531177 397\n",
      "0.06300206 0.036128115 0.02687394 398\n",
      "0.050576996 0.030385515 0.020191481 399\n",
      "0.038659003 0.035034087 0.0036249151 400\n",
      "0.0635331 0.04505042 0.018482676 401\n",
      "0.024329055 0.010470837 0.013858217 402\n",
      "0.012769561 0.007368315 0.005401246 403\n",
      "0.062710226 0.046900548 0.015809676 404\n",
      "0.02257207 0.011830031 0.0107420385 405\n",
      "0.014993066 0.003260641 0.011732426 406\n",
      "0.005797885 0.0018527902 0.0039450945 407\n",
      "0.18659766 0.092150435 0.094447225 408\n",
      "0.11334708 0.077935 0.03541208 409\n",
      "0.0114752855 0.0041245404 0.007350745 410\n",
      "0.013717549 0.0074364743 0.006281075 411\n",
      "0.02503637 0.017295891 0.007740479 412\n",
      "0.008409252 0.0061127567 0.0022964953 413\n",
      "0.21979353 0.047227543 0.17256598 414\n",
      "0.046323754 0.011499849 0.034823906 415\n",
      "0.02377979 0.0075676376 0.016212154 416\n",
      "0.05640494 0.04454397 0.01186097 417\n",
      "0.032522976 0.025309805 0.007213171 418\n",
      "0.017636962 0.005988963 0.011648 419\n",
      "0.01238252 0.002205636 0.010176884 420\n",
      "0.010599669 0.00615028 0.0044493885 421\n",
      "0.014469722 0.009337993 0.0051317294 422\n",
      "0.038033538 0.031220004 0.0068135317 423\n",
      "0.02289238 0.015343867 0.0075485134 424\n",
      "0.023856098 0.013779667 0.010076431 425\n",
      "0.042029224 0.029237077 0.01279215 426\n",
      "0.01421676 0.009576337 0.0046404223 427\n",
      "0.09276547 0.08295181 0.009813666 428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041137993 0.034935128 0.0062028654 429\n",
      "0.0056737885 0.0035497795 0.0021240092 430\n",
      "0.031928323 0.02561959 0.0063087307 431\n",
      "0.0075649535 0.0058937436 0.0016712099 432\n",
      "0.06278678 0.03909015 0.023696633 433\n",
      "0.029082328 0.013456878 0.01562545 434\n",
      "0.060715146 0.03240586 0.028309284 435\n",
      "0.0456945 0.03557747 0.010117032 436\n",
      "0.014492616 0.009146318 0.005346298 437\n",
      "0.0064986474 0.0031657224 0.003332925 438\n",
      "0.004741301 0.003754323 0.000986978 439\n",
      "0.00853553 0.0077443453 0.00079118507 440\n",
      "0.012188656 0.010391766 0.0017968899 441\n",
      "0.0030435165 0.0012014344 0.001842082 442\n",
      "0.13523147 0.10774424 0.027487233 443\n",
      "0.025954753 0.010736883 0.015217871 444\n",
      "0.012495943 0.0058275037 0.0066684396 445\n",
      "0.0076598637 0.0034328622 0.004227001 446\n",
      "0.004415671 0.0013823673 0.0030333034 447\n",
      "0.0049046115 0.0038963032 0.0010083084 448\n",
      "0.09872846 0.07656217 0.022166286 449\n",
      "0.018305298 0.010794403 0.0075108944 450\n",
      "0.101342075 0.083813675 0.017528398 451\n",
      "0.19511795 0.16396517 0.031152783 452\n",
      "0.052327953 0.023906868 0.028421085 453\n",
      "0.02156611 0.012289723 0.009276386 454\n",
      "0.07132661 0.04537189 0.025954725 455\n",
      "0.01701195 0.01243227 0.00457968 456\n",
      "0.020424293 0.016156498 0.0042677955 457\n",
      "0.092216864 0.07521672 0.017000148 458\n",
      "0.23154053 0.13039123 0.1011493 459\n",
      "0.028626963 0.0071377154 0.021489248 460\n",
      "0.046088383 0.010283241 0.035805143 461\n",
      "0.41686225 0.08370049 0.33316174 462\n",
      "0.044526916 0.020212898 0.024314018 463\n",
      "0.11059973 0.023826841 0.08677288 464\n",
      "0.05398742 0.012734947 0.041252475 465\n",
      "0.1666884 0.091707945 0.07498045 466\n",
      "0.07205761 0.04724941 0.024808204 467\n",
      "0.035584882 0.009021657 0.026563225 468\n",
      "0.05317671 0.020692958 0.032483753 469\n",
      "0.08776721 0.056336798 0.03143042 470\n",
      "0.01648499 0.0059737624 0.010511229 471\n",
      "0.059713047 0.03213339 0.027579658 472\n",
      "0.05283628 0.042349305 0.010486974 473\n",
      "0.009673307 0.005145136 0.0045281714 474\n",
      "0.015665326 0.009178703 0.006486624 475\n",
      "0.026162704 0.018213874 0.007948831 476\n",
      "0.012003507 0.0034545348 0.008548973 477\n",
      "0.07512753 0.030984584 0.04414294 478\n",
      "0.0127361305 0.003442265 0.009293865 479\n",
      "0.15183139 0.095217556 0.056613833 480\n",
      "0.044403233 0.018565807 0.025837423 481\n",
      "0.08498861 0.064289354 0.020699253 482\n",
      "0.026088258 0.01914571 0.0069425483 483\n",
      "0.011162702 0.006019666 0.005143036 484\n",
      "0.05607755 0.048240334 0.007837216 485\n",
      "0.036234893 0.028519502 0.007715392 486\n",
      "0.04032957 0.03338744 0.00694213 487\n",
      "0.011720557 0.0077348016 0.0039857547 488\n",
      "0.017900106 0.013949043 0.0039510624 489\n",
      "0.022691162 0.01905461 0.003636552 490\n",
      "0.028537706 0.026570026 0.0019676802 491\n",
      "0.012229905 0.0072130756 0.00501683 492\n",
      "0.02281176 0.018348824 0.0044629346 493\n",
      "0.033336587 0.026444767 0.0068918215 494\n",
      "0.047316477 0.024836635 0.022479843 495\n",
      "0.018495882 0.008776004 0.0097198775 496\n",
      "0.008518966 0.0036148976 0.004904068 497\n",
      "0.0072317426 0.005831967 0.0013997756 498\n",
      "0.03325592 0.028287059 0.004968863 499\n"
     ]
    }
   ],
   "source": [
    "#Train network\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t, x_interior, x_initial = sampler(nSim_t, nSim_x_interior, nSim_x_initial)\n",
    "    \n",
    "    for j in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_tnsr:t, x_interior_tnsr:x_interior, x_initial_tnsr:x_initial})\n",
    "        \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./FokkerPlack/FokkerPlanck'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './FokkerPlack/' + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow1.7",
   "language": "python",
   "name": "tensorflow1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
