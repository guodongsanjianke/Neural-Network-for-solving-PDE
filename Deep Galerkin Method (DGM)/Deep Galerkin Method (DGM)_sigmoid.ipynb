{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method (DGM)\n",
    "### S1 = sigma(w1*x + b1)  Z(l) = sigma(u*x + w*S + b) l=1,...,L  G(l) = sigma(u*x + w*S + b) l=1,...,L  \n",
    "### R(l) = sigma(u*x + w*S + b) l=1,...,L   H(l) = sigma(u*x + w*(S Hadamard R) + b)  l=1,...,L  \n",
    "### S(L+1) = (1-G) Hadamard H + Z Hadamard S  f = w*S(L+1) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of imput data  \n",
    "**output_dim:** number of output for LSTM layer  \n",
    "**trans1, trans2 (str):** activation functions used inside the layer;  \n",
    "one of: \"tanh\"(default), \"relu\" or \"sigmoid\"  \n",
    "**u vectors:** weighting vectors for inputs original inputs x  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**S:** output of previous layer  \n",
    "**X:** data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-like layer used in DGM\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        \n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    def call(self, S, X):\n",
    "\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of input data  \n",
    "**output_dim:** number of outputs for dense layer  \n",
    "**transformation:** activation function used inside the layer; using None is equivalent to the identity map  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**X:** input to layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer(dense)\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        \n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    def call(self,X):\n",
    "        \n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_layers:** number of intermediate LSTM layers  \n",
    "**input_dim:** spaital dimension of input data (excludes time dimension)  \n",
    "**final_trans:** transformation used in final layer\n",
    "define initial layer as fully connected\n",
    "to account for time inputs we use input_dim+1 as the input dimension\n",
    "**t:** sampled time inputs  \n",
    "**x:** sampled space inputs  \n",
    "Run the DGM model and obtain fitted function value at the inputs (t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture used in DGM\n",
    "\n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        \n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1,trans1 = \"sigmoid\", trans2 = \"sigmoid\"))\n",
    "        \n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    def call(self,t,x):\n",
    "\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process parameters（Ornstein-Uhlenbeck Process）\n",
    "kappa = 0\n",
    "theta = 0.5\n",
    "sigma = 2\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "# tenminal time\n",
    "T = 1.0\n",
    "\n",
    "# bounds of sampling region for space dimension, i.e. sampling will be done on\n",
    "# [multipliter*Xlow, multiplier*Xhigh]\n",
    "Xlow = -4.0\n",
    "Xhigh = 4.0\n",
    "x_multiplier = 2.0\n",
    "t_multiplier = 1.5\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages = 500\n",
    "steps_per_sample = 10\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_t = 5\n",
    "nSim_x_interior = 50\n",
    "nSim_x_initial = 50\n",
    "\n",
    "# Save options\n",
    "saveName = 'FokkerPlanck'\n",
    "saveFigure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OU Simulation function(Ornstein-Uhlenbeck Process)**  \n",
    "Simulate end point of Ornstein-Uhlenbeck process with normally distributed random starting value  \n",
    "**alpha:** mean of random starting value  \n",
    "**beta:** standard deviation of random starting value   \n",
    "**theta:** mean reversion level    \n",
    "**kappa:** mean reversion rate  \n",
    "**sigma:** volatility  \n",
    "**nSim:** number of simulations  \n",
    "**T:** terminal time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(alpha, beta, theta, kappa, sigma, nSim, T):\n",
    "    \n",
    "    # simulate initial point based on normal distribution\n",
    "    X0 = np.random.normal(loc = alpha, scale = beta, size = nSim)\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta + (X0 - theta) * np.exp(-kappa * T)\n",
    "    v = np.sqrt(sigma**2 / (2 * kappa) * (1 - np.exp(-2*kappa*T)))\n",
    "    \n",
    "    # simulate endpoint\n",
    "    Xt = np.random.normal(m,v)    \n",
    "    \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample time-space points from the function's domain;  \n",
    "point are sampled uniformly on the interior of the domain, at the initial/terminal time points  \n",
    "and along the spatial boundary at different time points.  \n",
    "**nSim_t:** number of (interior) time points to sample  \n",
    "**nSim_x_interior:** number of space points in the interior of the function's domain to sample  \n",
    "**nSim_x_initial:** number of space points at initial time to sample (initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function - random sample time-space pairs\n",
    "def sampler(nSim_t, nSim_x_interior, nSim_x_initial):\n",
    "    \n",
    "    # Sampler1: domain interior\n",
    "    t = np.random.uniform(low=0, high=T*t_multiplier, size=[nSim_t, 1])\n",
    "    x_interior = np.random.uniform(low=Xlow*x_multiplier, high=Xhigh*x_multiplier, size=[nSim_x_interior, 1])\n",
    "    \n",
    "    # Sampler: spatial boundary\n",
    "    # no spatial boundary condition for this problem \n",
    "    \n",
    "    # Sampler3: initial/terminal condition\n",
    "    x_initial = np.random.uniform(low=Xlow*1.5, high=Xhigh*1.5, size = [nSim_x_initial, 1])\n",
    "    \n",
    "    return t, x_interior, x_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute total loss for training.     \n",
    "The loss is based o the PDE satisfied by the negative-exponential of the density and NOT the density   \n",
    "itself, i.e. the u(t,x) in p(t,x) = exp(-u(t,x)) / c(t) where p is the density and c is the normalization constant.    \n",
    "**model:** DGM model object   \n",
    "**t:** sampled (interior) time points  \n",
    "**x_interior:** sampled space points in the interior of the function's domain   \n",
    "**x_initial:** sampled space points at initial time   \n",
    "**nSim_t:** number of (interior) time points sampled (size of t)  \n",
    "**alpha:** mean of normal distribution for process staring value  \n",
    "**beta:** standard deviation of normal distribution for process starting value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t, x_interior, x_initial, nSim_t, alpha, beta):\n",
    "    \n",
    "    # Loss term1: PDE\n",
    "    \n",
    "    # initialize vector of losses\n",
    "    losses_u = []\n",
    "    \n",
    "    # for each simulated interior time point\n",
    "    for tIndex in range(nSim_t):\n",
    "        \n",
    "        curr_t = t[tIndex]\n",
    "        t_vector = curr_t * tf.ones_like(x_interior)\n",
    "        \n",
    "        u    = model.call(t_vector, x_interior)\n",
    "        u_t  = tf.gradients(u, t_vector)[0]\n",
    "        u_x  = tf.gradients(u, x_interior)[0]\n",
    "        u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "        psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "        psi = tf.reduce_sum( u_t*tf.exp(-u) ) / psi_denominator\n",
    "\n",
    "        # PDE differential operator\n",
    "        diff_f = -u_t + kappa - kappa*(x_interior- theta)*u_x - 0.5*sigma**2*(-u_xx + u_x**2) + psi\n",
    "        \n",
    "        # compute L2-norm of differential operator and attach to vector of losses\n",
    "        currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "        losses_u.append(currLoss)\n",
    "    \n",
    "    # average losses across sample time points \n",
    "    L1 = tf.add_n(losses_u) / nSim_t\n",
    "    \n",
    "    # Loss term2: boundary condition\n",
    "    # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term3: initial condition\n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0 i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    fitted_pdf = model.call(0*tf.ones_like(x_initial), x_initial)\n",
    "    \n",
    "    target_pdf  = 0.5*(x_initial - alpha)**2 / (beta**2)\n",
    "    \n",
    "    # average L2 error for initial distribution\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "    return L1, L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input(time, space domain interior, space domain at initial time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Set up network\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "t_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_initial_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_tnsr, x_interior_tnsr, x_initial_tnsr, nSim_t, alpha, beta)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "u = model.call(t_tnsr, x_interior_tnsr)\n",
    "p_unnorm = tf.exp(-u)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.4826 0.008439033 73.47416 0\n",
      "68.18187 0.0853854 68.09648 1\n",
      "41.786182 1.7137364 40.072445 2\n",
      "33.70224 15.797106 17.905136 3\n",
      "20.781979 6.2021227 14.579856 4\n",
      "10.431985 3.7189782 6.7130065 5\n",
      "18.163464 7.9541025 10.20936 6\n",
      "13.556261 6.647526 6.9087358 7\n",
      "7.455699 2.2466536 5.2090454 8\n",
      "7.50115 2.7377284 4.763422 9\n",
      "9.193243 4.064962 5.1282806 10\n",
      "3.195266 1.1663725 2.0288935 11\n",
      "3.0303988 1.4645867 1.5658121 12\n",
      "2.6905417 1.3327177 1.3578241 13\n",
      "4.199729 2.173051 2.0266776 14\n",
      "4.525466 1.9911137 2.534352 15\n",
      "2.02034 0.6110878 1.409252 16\n",
      "5.64937 3.736843 1.912527 17\n",
      "3.1833458 1.4122771 1.7710686 18\n",
      "2.6054003 1.3424715 1.2629288 19\n",
      "2.0859494 0.89260244 1.1933471 20\n",
      "1.2211018 0.26892966 0.9521721 21\n",
      "1.2220523 0.51662385 0.70542854 22\n",
      "1.6252003 0.8972977 0.7279026 23\n",
      "1.4670928 0.25470927 1.2123835 24\n",
      "3.1237059 1.7628838 1.3608222 25\n",
      "0.6438482 0.21053044 0.43331772 26\n",
      "1.0900034 0.6622417 0.42776164 27\n",
      "2.0043597 1.024061 0.98029864 28\n",
      "0.7902868 0.14463393 0.64565283 29\n",
      "0.973114 0.25533226 0.7177818 30\n",
      "0.84370244 0.36610028 0.47760215 31\n",
      "0.6883967 0.3036372 0.3847595 32\n",
      "0.2933665 0.08374562 0.20962088 33\n",
      "1.3623898 0.60217917 0.7602106 34\n",
      "0.8388665 0.40237617 0.43649033 35\n",
      "0.9876374 0.43344703 0.5541904 36\n",
      "0.61179423 0.114749305 0.49704492 37\n",
      "0.9355202 0.47935542 0.45616478 38\n",
      "1.0355635 0.50277233 0.5327912 39\n",
      "0.40386286 0.07254801 0.33131486 40\n",
      "0.51861894 0.17201667 0.34660226 41\n",
      "0.35542905 0.06106892 0.29436013 42\n",
      "0.18947417 0.055776715 0.13369745 43\n",
      "0.21404725 0.07089809 0.14314915 44\n",
      "0.40135995 0.25711176 0.14424819 45\n",
      "0.23304042 0.07272285 0.16031757 46\n",
      "0.837271 0.5458277 0.29144326 47\n",
      "0.15793091 0.024331879 0.13359903 48\n",
      "0.15332659 0.04999465 0.10333193 49\n",
      "0.10072939 0.026168419 0.07456097 50\n",
      "0.18402475 0.03471979 0.14930496 51\n",
      "0.43655685 0.29948214 0.13707471 52\n",
      "0.24658327 0.08613773 0.16044554 53\n",
      "0.30692807 0.14920127 0.1577268 54\n",
      "0.13032268 0.012190045 0.11813263 55\n",
      "0.29900393 0.13417786 0.16482607 56\n",
      "0.12817532 0.045445193 0.08273012 57\n",
      "0.27669808 0.21681455 0.05988354 58\n",
      "0.37226954 0.22847621 0.14379333 59\n",
      "0.18285334 0.071556024 0.11129731 60\n",
      "0.86319894 0.62881917 0.23437975 61\n",
      "0.56990945 0.32186458 0.24804488 62\n",
      "0.29645523 0.051074028 0.2453812 63\n",
      "0.1630182 0.021262309 0.1417559 64\n",
      "0.16867554 0.06911703 0.0995585 65\n",
      "0.24162346 0.15502469 0.08659876 66\n",
      "0.14220707 0.022504333 0.11970274 67\n",
      "0.11702644 0.018918343 0.0981081 68\n",
      "0.07148737 0.031104749 0.04038262 69\n",
      "0.20013317 0.12387184 0.076261334 70\n",
      "0.13524142 0.06117013 0.07407129 71\n",
      "0.17548974 0.124411955 0.05107778 72\n",
      "0.072562136 0.017462006 0.055100128 73\n",
      "0.2816707 0.10087885 0.18079185 74\n",
      "0.06276944 0.009547312 0.053222135 75\n",
      "0.2913608 0.07522394 0.21613686 76\n",
      "0.17656581 0.09399636 0.08256945 77\n",
      "0.050144784 0.0077251247 0.04241966 78\n",
      "0.14591542 0.051898193 0.09401722 79\n",
      "0.08137235 0.024492357 0.056879997 80\n",
      "0.07542923 0.036117893 0.03931134 81\n",
      "0.37229848 0.23977707 0.1325214 82\n",
      "0.20287254 0.09772787 0.10514468 83\n",
      "0.06981767 0.025815798 0.044001874 84\n",
      "0.0638644 0.028528279 0.035336126 85\n",
      "0.10137479 0.06504387 0.036330923 86\n",
      "0.6344266 0.38366958 0.250757 87\n",
      "0.45802683 0.39661965 0.061407164 88\n",
      "0.21106717 0.07492115 0.13614602 89\n",
      "0.13585186 0.022702787 0.11314907 90\n",
      "0.04652697 0.020650674 0.025876293 91\n",
      "0.08449062 0.028069926 0.056420695 92\n",
      "0.070251346 0.03523053 0.03502082 93\n",
      "0.06905245 0.045652755 0.023399694 94\n",
      "0.04438573 0.012106547 0.032279182 95\n",
      "0.39428857 0.29889354 0.09539503 96\n",
      "0.2632904 0.12963955 0.13365084 97\n",
      "0.37766546 0.2668174 0.110848054 98\n",
      "0.18013634 0.052474704 0.12766163 99\n",
      "0.106550805 0.026760632 0.079790175 100\n",
      "0.06747693 0.034934815 0.032542113 101\n",
      "0.07443016 0.045911085 0.028519077 102\n",
      "0.2131111 0.1689831 0.04412801 103\n",
      "0.11331137 0.035976753 0.07733462 104\n",
      "0.07577117 0.02429512 0.05147605 105\n",
      "0.28477272 0.19558786 0.089184865 106\n",
      "0.117847234 0.01697139 0.10087585 107\n",
      "0.06627274 0.03172347 0.034549274 108\n",
      "0.33730727 0.12564899 0.21165827 109\n",
      "0.1881802 0.05251317 0.13566703 110\n",
      "0.15888861 0.08131238 0.077576235 111\n",
      "0.18509533 0.14240503 0.04269029 112\n",
      "0.12986608 0.042453215 0.087412864 113\n",
      "0.08145699 0.04359742 0.037859574 114\n",
      "0.13635036 0.054728527 0.08162184 115\n",
      "0.09587829 0.03836088 0.05751741 116\n",
      "0.24941555 0.15768103 0.091734506 117\n",
      "0.19804162 0.1342883 0.06375333 118\n",
      "0.12563626 0.048349675 0.07728659 119\n",
      "0.092209786 0.03895993 0.053249855 120\n",
      "0.09529059 0.063403346 0.03188724 121\n",
      "0.0779154 0.036903754 0.041011643 122\n",
      "0.065864034 0.031190136 0.034673896 123\n",
      "0.387825 0.32850322 0.0593218 124\n",
      "0.07526896 0.0071364352 0.06813253 125\n",
      "0.36776173 0.27158046 0.09618126 126\n",
      "0.19612667 0.11942722 0.07669946 127\n",
      "0.094113395 0.033893984 0.06021941 128\n",
      "0.059118 0.02857229 0.03054571 129\n",
      "0.17512618 0.13324633 0.041879844 130\n",
      "0.12985517 0.083255574 0.046599597 131\n",
      "0.084354796 0.023874396 0.0604804 132\n",
      "0.03134946 0.006623489 0.024725972 133\n",
      "0.41922098 0.2934392 0.12578179 134\n",
      "0.31702098 0.10406585 0.21295513 135\n",
      "0.24876004 0.13294132 0.11581872 136\n",
      "0.11824775 0.057354964 0.060892783 137\n",
      "0.10446979 0.070762746 0.03370705 138\n",
      "0.057743363 0.040231906 0.017511457 139\n",
      "0.26062867 0.17007966 0.090549 140\n",
      "0.10747701 0.063283585 0.04419342 141\n",
      "0.0706632 0.026344657 0.044318546 142\n",
      "0.11749316 0.085780546 0.031712614 143\n",
      "0.07213582 0.022990132 0.049145687 144\n",
      "0.03825316 0.011419759 0.026833398 145\n",
      "0.13881353 0.106557086 0.03225644 146\n",
      "0.23436956 0.055134147 0.17923541 147\n",
      "0.0465831 0.035492357 0.011090742 148\n",
      "0.05941514 0.028924868 0.030490274 149\n",
      "0.03170122 0.021790797 0.009910421 150\n",
      "0.030040316 0.0139056 0.016134717 151\n",
      "0.04469944 0.023911556 0.020787885 152\n",
      "0.11530382 0.07751719 0.037786633 153\n",
      "0.06630285 0.023183081 0.043119766 154\n",
      "0.07970175 0.043234423 0.036467325 155\n",
      "0.03150564 0.02008837 0.011417269 156\n",
      "0.020761486 0.0062589757 0.014502511 157\n",
      "0.114132464 0.088943206 0.025189254 158\n",
      "0.052339338 0.027506595 0.024832742 159\n",
      "0.18464613 0.16269521 0.02195092 160\n",
      "0.066370204 0.03672739 0.029642813 161\n",
      "0.054839857 0.030057017 0.024782842 162\n",
      "0.26259354 0.22006312 0.042530406 163\n",
      "0.13207139 0.05104796 0.08102343 164\n",
      "0.36717564 0.29916713 0.0680085 165\n",
      "0.07216357 0.033249088 0.03891448 166\n",
      "0.12672949 0.03746844 0.089261055 167\n",
      "0.047258936 0.014539889 0.032719046 168\n",
      "0.19309926 0.1278804 0.06521886 169\n",
      "0.13757932 0.07333415 0.06424518 170\n",
      "0.1428901 0.108250104 0.034639996 171\n",
      "0.21834432 0.15522288 0.06312144 172\n",
      "0.11030343 0.05383506 0.056468368 173\n",
      "0.08609074 0.05253756 0.033553187 174\n",
      "0.07183439 0.05581485 0.016019544 175\n",
      "0.093439184 0.06604638 0.027392806 176\n",
      "0.12429123 0.05587969 0.068411544 177\n",
      "0.07534265 0.046068426 0.02927422 178\n",
      "0.04789397 0.02709704 0.020796932 179\n",
      "0.05768966 0.035853654 0.021836007 180\n",
      "0.056046188 0.039280374 0.016765814 181\n",
      "0.030537963 0.005303146 0.025234817 182\n",
      "0.083755344 0.058537483 0.025217863 183\n",
      "0.029485185 0.009457166 0.02002802 184\n",
      "0.036619246 0.013401813 0.023217432 185\n",
      "0.02630011 0.014459404 0.011840705 186\n",
      "0.017916702 0.006902095 0.011014606 187\n",
      "0.18006614 0.16517948 0.014886663 188\n",
      "0.06048615 0.013561835 0.046924315 189\n",
      "0.15139376 0.035751436 0.11564232 190\n",
      "0.13683751 0.0960089 0.040828623 191\n",
      "0.036910012 0.01700904 0.01990097 192\n",
      "0.56471 0.45124838 0.11346162 193\n",
      "0.299671 0.14593421 0.15373679 194\n",
      "0.08951708 0.029199822 0.060317256 195\n",
      "0.07566682 0.014212402 0.06145442 196\n",
      "0.042775724 0.021678343 0.02109738 197\n",
      "0.17554727 0.14253305 0.033014216 198\n",
      "0.1310873 0.09858077 0.03250654 199\n",
      "0.07650043 0.02367931 0.052821122 200\n",
      "0.08351668 0.060222246 0.023294434 201\n",
      "0.036849916 0.018545717 0.018304197 202\n",
      "0.036486093 0.02576363 0.010722461 203\n",
      "0.050237507 0.021530587 0.02870692 204\n",
      "0.018221885 0.005353314 0.012868572 205\n",
      "0.023785021 0.010968491 0.012816531 206\n",
      "0.13926378 0.115029864 0.02423392 207\n",
      "0.05731573 0.016006814 0.041308917 208\n",
      "0.032452106 0.015826633 0.016625475 209\n",
      "0.026323695 0.009071414 0.017252281 210\n",
      "0.109707765 0.09861779 0.011089971 211\n",
      "0.11990453 0.072997004 0.046907533 212\n",
      "0.054142963 0.034953464 0.0191895 213\n",
      "0.17197347 0.12705502 0.044918455 214\n",
      "0.3498397 0.28629577 0.06354391 215\n",
      "0.09550288 0.046091106 0.049411777 216\n",
      "0.062044926 0.01579454 0.046250388 217\n",
      "0.053037703 0.01734188 0.03569582 218\n",
      "0.024998233 0.010829953 0.014168279 219\n",
      "0.017311497 0.006776869 0.010534628 220\n",
      "0.16069964 0.11811413 0.042585514 221\n",
      "0.06800881 0.027364183 0.040644627 222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.076515116 0.062641166 0.013873949 223\n",
      "0.056668073 0.030200735 0.02646734 224\n",
      "0.05327245 0.030575572 0.022696875 225\n",
      "0.06403866 0.045439776 0.018598882 226\n",
      "0.031840526 0.010631098 0.021209426 227\n",
      "0.05383329 0.042430084 0.011403209 228\n",
      "0.02041185 0.011571287 0.008840564 229\n",
      "0.039809857 0.025804257 0.014005599 230\n",
      "0.023638356 0.011014424 0.012623933 231\n",
      "0.04920395 0.03761542 0.0115885325 232\n",
      "0.02866356 0.013379934 0.0152836265 233\n",
      "0.023125488 0.012829683 0.0102958055 234\n",
      "0.017744906 0.012190331 0.0055545755 235\n",
      "0.0077626184 0.00431502 0.0034475985 236\n",
      "0.024054116 0.019823248 0.004230868 237\n",
      "0.033907965 0.019299094 0.014608872 238\n",
      "0.03214226 0.022672286 0.009469975 239\n",
      "0.009138125 0.0057854825 0.0033526428 240\n",
      "0.03512437 0.031246299 0.0038780684 241\n",
      "0.026134605 0.012601267 0.013533338 242\n",
      "0.090958044 0.08342494 0.007533102 243\n",
      "0.06807136 0.053339448 0.01473191 244\n",
      "0.0375949 0.029188706 0.008406193 245\n",
      "0.01690924 0.010493933 0.006415307 246\n",
      "0.017775252 0.0092175845 0.008557667 247\n",
      "0.013286235 0.008602737 0.0046834974 248\n",
      "0.009075591 0.0051295864 0.003946005 249\n",
      "0.026487382 0.020566303 0.0059210793 250\n",
      "0.03837491 0.02866432 0.009710587 251\n",
      "0.027500138 0.022761775 0.0047383625 252\n",
      "0.021166563 0.014359439 0.0068071233 253\n",
      "0.017170254 0.008785011 0.008385242 254\n",
      "0.056197222 0.046144605 0.010052618 255\n",
      "0.018858492 0.013203097 0.0056553953 256\n",
      "0.014720229 0.007664703 0.007055527 257\n",
      "0.009807117 0.005776975 0.004030143 258\n",
      "1.6116232 0.76133823 0.85028493 259\n",
      "1.1473427 0.34767586 0.7996669 260\n",
      "0.61180675 0.087502524 0.5243042 261\n",
      "0.23899975 0.08036991 0.15862983 262\n",
      "0.2747846 0.17573822 0.09904638 263\n",
      "0.4086851 0.18727653 0.22140855 264\n",
      "0.22762632 0.13505304 0.09257329 265\n",
      "0.17665155 0.063169055 0.1134825 266\n",
      "0.39363554 0.31733194 0.0763036 267\n",
      "0.1694496 0.1266961 0.042753503 268\n",
      "0.1950737 0.09928589 0.09578781 269\n",
      "0.08514582 0.026143376 0.059002448 270\n",
      "0.07969619 0.04120671 0.038489483 271\n",
      "0.08282012 0.040756557 0.042063557 272\n",
      "0.29323098 0.2329558 0.060275186 273\n",
      "0.08128226 0.017623642 0.06365862 274\n",
      "0.09490688 0.057864327 0.03704255 275\n",
      "0.089122504 0.037606012 0.051516496 276\n",
      "0.33569038 0.22136259 0.11432777 277\n",
      "0.094732985 0.030387256 0.064345725 278\n",
      "0.15939495 0.07930823 0.08008673 279\n",
      "0.12934816 0.08189091 0.04745725 280\n",
      "0.06160795 0.015001955 0.046605993 281\n",
      "0.08296853 0.06443361 0.018534923 282\n",
      "0.07288349 0.028092071 0.044791415 283\n",
      "0.049639754 0.010021728 0.039618026 284\n",
      "0.04962133 0.030754963 0.018866368 285\n",
      "0.021138921 0.012552874 0.008586048 286\n",
      "0.24613273 0.07813079 0.16800195 287\n",
      "0.097762205 0.065981604 0.0317806 288\n",
      "0.17572492 0.061458495 0.114266425 289\n",
      "0.028709576 0.011021641 0.017687935 290\n",
      "0.054256856 0.019134572 0.035122287 291\n",
      "0.07281227 0.05765451 0.015157758 292\n",
      "0.040402096 0.017907286 0.02249481 293\n",
      "0.15573964 0.10060705 0.05513259 294\n",
      "0.122378245 0.08653497 0.035843275 295\n",
      "0.09172694 0.075437196 0.016289743 296\n",
      "0.0336575 0.009439271 0.02421823 297\n",
      "0.035609543 0.01661529 0.018994255 298\n",
      "0.016954744 0.006553224 0.010401521 299\n",
      "0.10012701 0.06854212 0.031584892 300\n",
      "0.047807105 0.03409935 0.013707753 301\n",
      "0.121216945 0.078080505 0.04313644 302\n",
      "0.09072708 0.07773427 0.012992817 303\n",
      "0.06998902 0.018834224 0.051154796 304\n",
      "0.02600717 0.016038343 0.009968827 305\n",
      "0.047459472 0.030693937 0.016765535 306\n",
      "0.06800591 0.05289792 0.01510799 307\n",
      "0.107632115 0.072291344 0.03534077 308\n",
      "0.05247744 0.034176465 0.018300975 309\n",
      "0.07899812 0.057618957 0.021379162 310\n",
      "0.025414752 0.017417068 0.007997684 311\n",
      "0.03823882 0.03221954 0.006019279 312\n",
      "0.054546602 0.02355272 0.030993884 313\n",
      "0.034122832 0.023257634 0.0108651975 314\n",
      "0.039082386 0.03439085 0.0046915337 315\n",
      "0.020593613 0.013513329 0.0070802844 316\n",
      "0.011111163 0.006789069 0.004322093 317\n",
      "0.016176097 0.008936795 0.0072393017 318\n",
      "0.021486428 0.016484877 0.005001551 319\n",
      "0.011221793 0.008451769 0.002770023 320\n",
      "0.011253726 0.005523671 0.0057300543 321\n",
      "0.0079220105 0.0062841955 0.0016378148 322\n",
      "0.20951359 0.12563094 0.083882645 323\n",
      "0.34463146 0.25172135 0.09291012 324\n",
      "0.3401996 0.18215564 0.15804397 325\n",
      "0.08751638 0.039863493 0.047652893 326\n",
      "0.039941195 0.02222811 0.017713085 327\n",
      "0.050390825 0.032987647 0.01740318 328\n",
      "0.09336075 0.077511296 0.01584946 329\n",
      "0.12580314 0.07554271 0.050260436 330\n",
      "0.059263267 0.02820521 0.031058056 331\n",
      "0.110439874 0.09880736 0.011632518 332\n",
      "0.036921218 0.010690054 0.026231164 333\n",
      "0.020010952 0.009550738 0.010460214 334\n",
      "0.12600464 0.06820033 0.057804316 335\n",
      "0.05382996 0.02604564 0.027784321 336\n",
      "0.028365016 0.013762645 0.0146023715 337\n",
      "0.049853966 0.026470572 0.023383396 338\n",
      "0.021346714 0.010487323 0.010859393 339\n",
      "0.048064526 0.02986748 0.018197047 340\n",
      "0.030722756 0.017094513 0.0136282435 341\n",
      "0.10618549 0.08398991 0.022195576 342\n",
      "0.02252476 0.009397867 0.013126893 343\n",
      "0.05965147 0.049324 0.010327472 344\n",
      "0.04174917 0.026514543 0.015234624 345\n",
      "0.019808259 0.012573855 0.007234403 346\n",
      "0.030698083 0.014371717 0.016326366 347\n",
      "0.099992156 0.06745737 0.03253478 348\n",
      "0.032317534 0.016800234 0.015517298 349\n",
      "0.018609129 0.007182479 0.011426649 350\n",
      "0.15243919 0.12465534 0.027783852 351\n",
      "0.059329823 0.017857615 0.041472208 352\n",
      "0.65643525 0.20045868 0.4559766 353\n",
      "0.19921046 0.10406275 0.09514771 354\n",
      "0.12580456 0.051396083 0.07440847 355\n",
      "0.087941974 0.056853373 0.031088598 356\n",
      "0.057354882 0.021809652 0.03554523 357\n",
      "0.042571105 0.013764064 0.028807042 358\n",
      "0.1164399 0.08831501 0.028124891 359\n",
      "0.051409323 0.039477635 0.011931687 360\n",
      "0.041532457 0.018568981 0.022963475 361\n",
      "0.01617287 0.010838874 0.005333996 362\n",
      "0.026780011 0.014372681 0.01240733 363\n",
      "0.021894682 0.015661227 0.0062334556 364\n",
      "0.01765117 0.012027581 0.0056235893 365\n",
      "0.07699887 0.065729186 0.011269679 366\n",
      "0.047712706 0.03698344 0.010729266 367\n",
      "0.04063177 0.018220467 0.022411302 368\n",
      "0.026096089 0.008584976 0.017511113 369\n",
      "0.11785885 0.108358815 0.009500036 370\n",
      "0.026535973 0.0116194505 0.0149165215 371\n",
      "0.046469066 0.038407326 0.0080617415 372\n",
      "0.017983213 0.010526004 0.0074572098 373\n",
      "0.023738097 0.015406015 0.008332081 374\n",
      "0.012363019 0.008995485 0.0033675348 375\n",
      "0.046040997 0.04070406 0.0053369375 376\n",
      "0.022828607 0.014979305 0.007849302 377\n",
      "0.010208968 0.0036889159 0.0065200524 378\n",
      "0.023375826 0.01706175 0.006314076 379\n",
      "0.025190204 0.019082604 0.006107601 380\n",
      "0.01697514 0.013794156 0.0031809842 381\n",
      "0.066759616 0.059548028 0.0072115893 382\n",
      "0.03283287 0.022019897 0.010812972 383\n",
      "0.03142869 0.018455466 0.012973225 384\n",
      "0.043360442 0.0328455 0.010514942 385\n",
      "0.026806619 0.021110252 0.0056963675 386\n",
      "0.017141152 0.01469979 0.0024413634 387\n",
      "0.03547078 0.032251436 0.003219343 388\n",
      "0.0583975 0.041004907 0.017392596 389\n",
      "0.026993047 0.009809269 0.017183779 390\n",
      "0.026773142 0.02373976 0.0030333814 391\n",
      "0.020506565 0.016381498 0.0041250666 392\n",
      "0.00788258 0.0046753176 0.0032072624 393\n",
      "0.043322276 0.037576478 0.005745797 394\n",
      "0.03125035 0.02180532 0.009445029 395\n",
      "0.016566843 0.0079077 0.008659143 396\n",
      "0.014239134 0.0032989574 0.010940177 397\n",
      "0.007661176 0.0030364662 0.0046247095 398\n",
      "0.066237584 0.045345243 0.020892337 399\n",
      "0.06615622 0.0061063296 0.060049895 400\n",
      "0.023803487 0.010797511 0.013005976 401\n",
      "0.44880056 0.32737795 0.12142263 402\n",
      "0.11305916 0.019089958 0.0939692 403\n",
      "0.2107985 0.110186435 0.100612074 404\n",
      "0.056800008 0.029348707 0.027451301 405\n",
      "0.029709447 0.016101737 0.01360771 406\n",
      "0.20635012 0.11632904 0.09002109 407\n",
      "0.09675339 0.062437892 0.0343155 408\n",
      "0.049406566 0.028441006 0.020965558 409\n",
      "0.04054011 0.032576784 0.007963326 410\n",
      "0.0136462655 0.0064002126 0.0072460533 411\n",
      "0.020933703 0.010837372 0.010096329 412\n",
      "0.022484243 0.017811738 0.0046725054 413\n",
      "0.0077805175 0.004855445 0.0029250726 414\n",
      "0.0147917345 0.009010407 0.005781327 415\n",
      "0.012644996 0.0067922506 0.0058527454 416\n",
      "0.01162327 0.007959286 0.003663984 417\n",
      "0.6610627 0.5556897 0.105372995 418\n",
      "0.2423846 0.04521918 0.19716541 419\n",
      "0.050043553 0.018816963 0.03122659 420\n",
      "0.023606103 0.0070422525 0.016563851 421\n",
      "0.042013716 0.020874752 0.021138964 422\n",
      "0.04881951 0.03666198 0.0121575305 423\n",
      "0.04056645 0.019368647 0.021197807 424\n",
      "0.034895066 0.02289606 0.011999005 425\n",
      "0.035598375 0.027249876 0.0083485 426\n",
      "0.031344388 0.023036147 0.008308241 427\n",
      "0.014013506 0.0071825595 0.006830946 428\n",
      "0.012026446 0.006213066 0.0058133793 429\n",
      "0.038523547 0.019906122 0.018617425 430\n",
      "0.03377513 0.026818788 0.006956341 431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16356455 0.1488144 0.01475015 432\n",
      "0.03965421 0.010992849 0.028661363 433\n",
      "0.0556326 0.048138775 0.0074938238 434\n",
      "0.026843708 0.023962488 0.0028812194 435\n",
      "0.016848724 0.011427338 0.005421387 436\n",
      "0.026288053 0.013374435 0.012913618 437\n",
      "0.022283444 0.012310221 0.009973223 438\n",
      "0.026763067 0.0220056 0.0047574653 439\n",
      "0.053520046 0.017413875 0.03610617 440\n",
      "0.025608122 0.0089024445 0.016705677 441\n",
      "0.025697883 0.017954478 0.007743406 442\n",
      "0.008327722 0.002758351 0.005569371 443\n",
      "0.24535592 0.19178818 0.053567737 444\n",
      "0.06725332 0.02861079 0.038642533 445\n",
      "0.051720954 0.03266162 0.019059332 446\n",
      "0.013949752 0.0061171567 0.007832595 447\n",
      "0.011187845 0.003287887 0.007899958 448\n",
      "0.021935247 0.014436403 0.0074988427 449\n",
      "0.03501305 0.028323298 0.0066897525 450\n",
      "0.011103753 0.0055646137 0.005539139 451\n",
      "0.0075365226 0.0053300182 0.0022065043 452\n",
      "0.032497093 0.02435097 0.008146122 453\n",
      "0.04470765 0.04068151 0.0040261387 454\n",
      "0.070820905 0.042850647 0.02797026 455\n",
      "0.058365926 0.049717095 0.008648832 456\n",
      "0.020420996 0.01211819 0.008302807 457\n",
      "0.01775568 0.010914507 0.0068411715 458\n",
      "0.017728789 0.01369626 0.0040325294 459\n",
      "0.014581863 0.008530971 0.0060508917 460\n",
      "0.017986542 0.014683539 0.0033030028 461\n",
      "0.036824953 0.03184479 0.0049801623 462\n",
      "0.018834585 0.0071208077 0.011713778 463\n",
      "0.014083617 0.010421192 0.0036624253 464\n",
      "0.04670544 0.043437522 0.0032679175 465\n",
      "0.042413224 0.037936278 0.004476945 466\n",
      "0.05080276 0.04369099 0.00711177 467\n",
      "0.009247646 0.0053668916 0.0038807543 468\n",
      "0.03194011 0.024208436 0.0077316724 469\n",
      "0.008687306 0.004325831 0.0043614754 470\n",
      "0.027668297 0.017194523 0.010473774 471\n",
      "0.044647254 0.037468404 0.0071788523 472\n",
      "0.021086005 0.006245674 0.014840332 473\n",
      "0.03689486 0.02156355 0.01533131 474\n",
      "0.023741556 0.02163369 0.0021078666 475\n",
      "0.055143733 0.040150985 0.014992748 476\n",
      "0.028844073 0.019452846 0.009391229 477\n",
      "0.021320362 0.016338699 0.004981663 478\n",
      "0.043677203 0.028340328 0.015336874 479\n",
      "0.01616366 0.008393993 0.0077696675 480\n",
      "0.04171928 0.018242506 0.023476774 481\n",
      "0.023432894 0.018052224 0.0053806696 482\n",
      "0.048387356 0.040613297 0.0077740587 483\n",
      "0.020168979 0.0116241425 0.008544837 484\n",
      "0.12382951 0.11769863 0.0061308746 485\n",
      "1.1949 0.9787715 0.21612857 486\n",
      "0.17849213 0.06348253 0.11500959 487\n",
      "0.24772424 0.16602963 0.081694596 488\n",
      "0.12953006 0.066140935 0.06338912 489\n",
      "0.07648477 0.03859925 0.03788552 490\n",
      "0.052089665 0.039570525 0.01251914 491\n",
      "0.028223291 0.01881116 0.00941213 492\n",
      "0.03171467 0.011277767 0.020436902 493\n",
      "0.028665818 0.008798822 0.019866996 494\n",
      "0.018898712 0.0069383816 0.011960331 495\n",
      "0.01901462 0.008390185 0.010624433 496\n",
      "0.084839456 0.061471965 0.023367489 497\n",
      "0.21834832 0.17541096 0.04293737 498\n",
      "0.0985243 0.050917577 0.04760673 499\n"
     ]
    }
   ],
   "source": [
    "#Train network\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t, x_interior, x_initial = sampler(nSim_t, nSim_x_interior, nSim_x_initial)\n",
    "    \n",
    "    for j in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_tnsr:t, x_interior_tnsr:x_interior, x_initial_tnsr:x_initial})\n",
    "        \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./FokkerPlack/FokkerPlanck'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './FokkerPlack/' + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow1.7",
   "language": "python",
   "name": "tensorflow1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
