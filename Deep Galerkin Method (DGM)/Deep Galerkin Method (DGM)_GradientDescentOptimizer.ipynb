{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method (DGM)\n",
    "### S1 = sigma(w1*x + b1)  Z(l) = sigma(u*x + w*S + b) l=1,...,L  G(l) = sigma(u*x + w*S + b) l=1,...,L  \n",
    "### R(l) = sigma(u*x + w*S + b) l=1,...,L   H(l) = sigma(u*x + w*(S Hadamard R) + b)  l=1,...,L  \n",
    "### S(L+1) = (1-G) Hadamard H + Z Hadamard S  f = w*S(L+1) + b  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import needed packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of imput data  \n",
    "**output_dim:** number of output for LSTM layer  \n",
    "**trans1, trans2 (str):** activation functions used inside the layer;  \n",
    "one of: \"tanh\"(default), \"relu\" or \"sigmoid\"  \n",
    "**u vectors:** weighting vectors for inputs original inputs x  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**S:** output of previous layer  \n",
    "**X:** data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-like layer used in DGM\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        \n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    def call(self, S, X):\n",
    "\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_dim:** dimensionality of input data  \n",
    "**output_dim:** number of outputs for dense layer  \n",
    "**transformation:** activation function used inside the layer; using None is equivalent to the identity map  \n",
    "**w vectors:** weighting vectors for output of previous layer  \n",
    "**X:** input to layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer(dense)\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        \n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    def call(self,X):\n",
    "        \n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_layers:** number of intermediate LSTM layers  \n",
    "**input_dim:** spaital dimension of input data (excludes time dimension)  \n",
    "**final_trans:** transformation used in final layer\n",
    "define initial layer as fully connected\n",
    "to account for time inputs we use input_dim+1 as the input dimension\n",
    "**t:** sampled time inputs  \n",
    "**x:** sampled space inputs  \n",
    "Run the DGM model and obtain fitted function value at the inputs (t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture used in DGM\n",
    "\n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        \n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1))\n",
    "        \n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    def call(self,t,x):\n",
    "\n",
    "        X = tf.concat([t,x],1)\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process parameters（Ornstein-Uhlenbeck Process）\n",
    "kappa = 0\n",
    "theta = 0.5\n",
    "sigma = 2\n",
    "\n",
    "# mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = 0.0\n",
    "beta = 1\n",
    "\n",
    "# tenminal time\n",
    "T = 1.0\n",
    "\n",
    "# bounds of sampling region for space dimension, i.e. sampling will be done on\n",
    "# [multipliter*Xlow, multiplier*Xhigh]\n",
    "Xlow = -4.0\n",
    "Xhigh = 4.0\n",
    "x_multiplier = 2.0\n",
    "t_multiplier = 1.5\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages = 500\n",
    "steps_per_sample = 10\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_t = 5\n",
    "nSim_x_interior = 50\n",
    "nSim_x_initial = 50\n",
    "\n",
    "# Save options\n",
    "saveName = 'FokkerPlanck'\n",
    "saveFigure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OU Simulation function(Ornstein-Uhlenbeck Process)**  \n",
    "Simulate end point of Ornstein-Uhlenbeck process with normally distributed random starting value  \n",
    "**alpha:** mean of random starting value  \n",
    "**beta:** standard deviation of random starting value   \n",
    "**theta:** mean reversion level    \n",
    "**kappa:** mean reversion rate  \n",
    "**sigma:** volatility  \n",
    "**nSim:** number of simulations  \n",
    "**T:** terminal time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(alpha, beta, theta, kappa, sigma, nSim, T):\n",
    "    \n",
    "    # simulate initial point based on normal distribution\n",
    "    X0 = np.random.normal(loc = alpha, scale = beta, size = nSim)\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta + (X0 - theta) * np.exp(-kappa * T)\n",
    "    v = np.sqrt(sigma**2 / (2 * kappa) * (1 - np.exp(-2*kappa*T)))\n",
    "    \n",
    "    # simulate endpoint\n",
    "    Xt = np.random.normal(m,v)    \n",
    "    \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample time-space points from the function's domain;  \n",
    "point are sampled uniformly on the interior of the domain, at the initial/terminal time points  \n",
    "and along the spatial boundary at different time points.  \n",
    "**nSim_t:** number of (interior) time points to sample  \n",
    "**nSim_x_interior:** number of space points in the interior of the function's domain to sample  \n",
    "**nSim_x_initial:** number of space points at initial time to sample (initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function - random sample time-space pairs\n",
    "def sampler(nSim_t, nSim_x_interior, nSim_x_initial):\n",
    "    \n",
    "    # Sampler1: domain interior\n",
    "    t = np.random.uniform(low=0, high=T*t_multiplier, size=[nSim_t, 1])\n",
    "    x_interior = np.random.uniform(low=Xlow*x_multiplier, high=Xhigh*x_multiplier, size=[nSim_x_interior, 1])\n",
    "    \n",
    "    # Sampler: spatial boundary\n",
    "    # no spatial boundary condition for this problem \n",
    "    \n",
    "    # Sampler3: initial/terminal condition\n",
    "    x_initial = np.random.uniform(low=Xlow*1.5, high=Xhigh*1.5, size = [nSim_x_initial, 1])\n",
    "    \n",
    "    return t, x_interior, x_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute total loss for training.     \n",
    "The loss is based o the PDE satisfied by the negative-exponential of the density and NOT the density   \n",
    "itself, i.e. the u(t,x) in p(t,x) = exp(-u(t,x)) / c(t) where p is the density and c is the normalization constant.    \n",
    "**model:** DGM model object   \n",
    "**t:** sampled (interior) time points  \n",
    "**x_interior:** sampled space points in the interior of the function's domain   \n",
    "**x_initial:** sampled space points at initial time   \n",
    "**nSim_t:** number of (interior) time points sampled (size of t)  \n",
    "**alpha:** mean of normal distribution for process staring value  \n",
    "**beta:** standard deviation of normal distribution for process starting value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t, x_interior, x_initial, nSim_t, alpha, beta):\n",
    "    \n",
    "    # Loss term1: PDE\n",
    "    \n",
    "    # initialize vector of losses\n",
    "    losses_u = []\n",
    "    \n",
    "    # for each simulated interior time point\n",
    "    for tIndex in range(nSim_t):\n",
    "        \n",
    "        curr_t = t[tIndex]\n",
    "        t_vector = curr_t * tf.ones_like(x_interior)\n",
    "        \n",
    "        u    = model.call(t_vector, x_interior)\n",
    "        u_t  = tf.gradients(u, t_vector)[0]\n",
    "        u_x  = tf.gradients(u, x_interior)[0]\n",
    "        u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "        psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "        psi = tf.reduce_sum( u_t*tf.exp(-u) ) / psi_denominator\n",
    "\n",
    "        # PDE differential operator\n",
    "        diff_f = -u_t + kappa - kappa*(x_interior- theta)*u_x - 0.5*sigma**2*(-u_xx + u_x**2) + psi\n",
    "        \n",
    "        # compute L2-norm of differential operator and attach to vector of losses\n",
    "        currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "        losses_u.append(currLoss)\n",
    "    \n",
    "    # average losses across sample time points \n",
    "    L1 = tf.add_n(losses_u) / nSim_t\n",
    "    \n",
    "    # Loss term2: boundary condition\n",
    "    # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term3: initial condition\n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0 i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    fitted_pdf = model.call(0*tf.ones_like(x_initial), x_initial)\n",
    "    \n",
    "    target_pdf  = 0.5*(x_initial - alpha)**2 / (beta**2)\n",
    "    \n",
    "    # average L2 error for initial distribution\n",
    "    L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "    return L1, L3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input(time, space domain interior, space domain at initial time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyaohong/.conda/envs/tensorflow1.7/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# Set up network\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "t_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "x_initial_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_tnsr, x_interior_tnsr, x_initial_tnsr, nSim_t, alpha, beta)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "u = model.call(t_tnsr, x_interior_tnsr)\n",
    "p_unnorm = tf.exp(-u)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.937588 3.8644593 15.073129 0\n",
      "10.027338 4.849501 5.1778364 1\n",
      "12.299571 3.3551521 8.944419 2\n",
      "10.802689 2.863058 7.93963 3\n",
      "4.1894174 1.140113 3.0493042 4\n",
      "5.4668903 1.3257658 4.1411242 5\n",
      "4.374291 0.84861356 3.5256774 6\n",
      "2.4595947 0.6368637 1.822731 7\n",
      "9.843922 2.3518443 7.492078 8\n",
      "8.904815 3.1673772 5.7374372 9\n",
      "1.7591333 0.3112379 1.4478954 10\n",
      "7.239501 2.3386216 4.9008794 11\n",
      "2.0181632 0.29453894 1.7236242 12\n",
      "6.9758015 1.7907552 5.185046 13\n",
      "1.8025029 0.32934654 1.4731563 14\n",
      "40.733463 4.804336 35.929127 15\n",
      "8.508402 2.3762238 6.132178 16\n",
      "3.2199883 0.57747877 2.6425097 17\n",
      "1.3749571 0.28014523 1.0948119 18\n",
      "2.326943 0.79491067 1.5320324 19\n",
      "12.707598 3.6068783 9.100719 20\n",
      "2.8677764 0.26044747 2.607329 21\n",
      "2.9381666 0.53055704 2.4076097 22\n",
      "10.91075 1.7424071 9.168344 23\n",
      "6.6800613 1.4299843 5.250077 24\n",
      "3.523921 0.62621444 2.8977065 25\n",
      "3.1201153 0.8549717 2.2651436 26\n",
      "5.168741 1.390326 3.7784152 27\n",
      "0.83053917 0.08140282 0.7491363 28\n",
      "0.8510183 0.05287074 0.79814756 29\n",
      "2.078226 0.23184319 1.8463829 30\n",
      "8.092592 2.1948745 5.897718 31\n",
      "3.6743884 0.7451863 2.929202 32\n",
      "7.8447566 5.036156 2.8086004 33\n",
      "4.183216 1.1420764 3.0411398 34\n",
      "4.6382003 1.4542578 3.1839426 35\n",
      "6.7428765 1.6216668 5.1212096 36\n",
      "5.3461537 1.1288859 4.217268 37\n",
      "2.1577277 0.16289817 1.9948295 38\n",
      "4.791169 1.3651631 3.4260058 39\n",
      "2.483663 0.59152424 1.8921387 40\n",
      "1.7685052 0.27238527 1.49612 41\n",
      "10.510063 1.9785637 8.5315 42\n",
      "5.717279 1.1021694 4.6151094 43\n",
      "7.4371147 1.5709689 5.8661456 44\n",
      "3.605474 0.5182985 3.0871754 45\n",
      "6.4248247 1.982589 4.4422355 46\n",
      "6.1272936 2.106096 4.021198 47\n",
      "4.268269 1.6115426 2.6567266 48\n",
      "2.2397902 0.86529046 1.3744998 49\n",
      "2.7075386 0.763504 1.9440346 50\n",
      "1.8434141 0.33585057 1.5075635 51\n",
      "0.9080332 0.121699765 0.78633344 52\n",
      "9.647253 3.0194716 6.6277814 53\n",
      "2.2459002 1.1224055 1.1234947 54\n",
      "0.9415566 0.16705202 0.77450454 55\n",
      "2.2799232 0.91654694 1.3633763 56\n",
      "2.4603033 0.7549445 1.7053589 57\n",
      "1.9452226 0.65741956 1.287803 58\n",
      "0.8669144 0.12925765 0.7376568 59\n",
      "0.50974375 0.08604993 0.4236938 60\n",
      "0.95439124 0.25154302 0.7028482 61\n",
      "5.8615837 1.3194973 4.5420866 62\n",
      "4.337072 1.1183218 3.21875 63\n",
      "3.5519726 0.85214806 2.6998246 64\n",
      "4.803273 1.5587465 3.2445269 65\n",
      "1.1189575 0.18445249 0.934505 66\n",
      "0.69982195 0.08619222 0.6136297 67\n",
      "3.2905684 0.77837175 2.5121965 68\n",
      "4.147201 1.1051235 3.0420775 69\n",
      "1.5626482 0.32004142 1.2426068 70\n",
      "3.2763658 0.8106748 2.465691 71\n",
      "0.49516284 0.058931064 0.4362318 72\n",
      "3.5515456 1.2579354 2.2936103 73\n",
      "2.5245144 0.6222488 1.9022657 74\n",
      "0.5280397 0.14565317 0.3823865 75\n",
      "0.55864704 0.16205885 0.39658818 76\n",
      "1.7594702 0.70948195 1.0499883 77\n",
      "0.91957176 0.4469637 0.47260806 78\n",
      "5.4263964 2.6931548 2.7332416 79\n",
      "0.8249888 0.23084283 0.59414595 80\n",
      "1.7769367 0.8201195 0.95681715 81\n",
      "2.3062959 1.043895 1.2624009 82\n",
      "4.9510336 2.1209207 2.8301127 83\n",
      "0.49726644 0.11884733 0.3784191 84\n",
      "0.37043518 0.071374856 0.2990603 85\n",
      "0.41448843 0.14073275 0.27375567 86\n",
      "1.5764225 1.0063366 0.57008594 87\n",
      "0.29513186 0.06933378 0.22579809 88\n",
      "0.28977478 0.036894705 0.25288007 89\n",
      "5.9077883 2.654124 3.253664 90\n",
      "2.3195581 0.42302656 1.8965316 91\n",
      "5.284319 2.1702275 3.1140914 92\n",
      "3.2302294 1.4841417 1.7460878 93\n",
      "5.2767034 3.3770282 1.899675 94\n",
      "2.7010913 1.0596122 1.641479 95\n",
      "2.3493009 0.74669087 1.6026099 96\n",
      "0.560084 0.07486526 0.48521873 97\n",
      "0.43517202 0.12846492 0.3067071 98\n",
      "0.29621512 0.071305595 0.22490953 99\n",
      "0.13564016 0.03041465 0.1052255 100\n",
      "1.9646313 0.86985797 1.0947733 101\n",
      "1.2314126 0.46094465 0.77046794 102\n",
      "0.59920526 0.121169284 0.47803596 103\n",
      "1.1358091 0.2771986 0.85861045 104\n",
      "3.7332106 1.1952759 2.5379348 105\n",
      "3.3248248 1.9731435 1.3516814 106\n",
      "3.0605183 1.816656 1.2438624 107\n",
      "0.48852158 0.09451198 0.3940096 108\n",
      "0.422411 0.1624254 0.2599856 109\n",
      "4.543626 3.0913963 1.4522297 110\n",
      "0.3694017 0.052395355 0.31700635 111\n",
      "4.51944 0.86023283 3.6592073 112\n",
      "1.2473903 0.09788692 1.1495034 113\n",
      "1.9528325 0.4963471 1.4564854 114\n",
      "2.0007112 0.6546715 1.3460398 115\n",
      "2.188367 0.61442995 1.5739368 116\n",
      "2.2899766 0.5815511 1.7084254 117\n",
      "1.7360482 0.7738878 0.96216047 118\n",
      "0.3976222 0.037657592 0.3599646 119\n",
      "0.308231 0.04560665 0.26262435 120\n",
      "1.5072072 0.39510757 1.1120995 121\n",
      "0.49224275 0.10008653 0.3921562 122\n",
      "0.5081742 0.124540925 0.38363326 123\n",
      "1.2919533 0.27421805 1.0177352 124\n",
      "1.2853291 0.34316975 0.9421593 125\n",
      "3.0065544 1.3225697 1.6839846 126\n",
      "1.0783569 0.2815991 0.79675776 127\n",
      "0.9094189 0.18899441 0.7204245 128\n",
      "1.4744844 0.6374085 0.8370759 129\n",
      "0.64543766 0.41395256 0.2314851 130\n",
      "0.7619112 0.20391713 0.55799407 131\n",
      "0.79240596 0.2933922 0.49901378 132\n",
      "1.0937413 0.49158594 0.6021553 133\n",
      "0.6304718 0.27255017 0.35792163 134\n",
      "0.39560926 0.09176667 0.30384257 135\n",
      "0.91109747 0.14376383 0.7673336 136\n",
      "0.83111024 0.36325485 0.46785536 137\n",
      "0.2591011 0.021903696 0.2371974 138\n",
      "1.1958445 0.49805823 0.69778633 139\n",
      "7.9271784 1.3965131 6.5306654 140\n",
      "3.3718314 0.5081913 2.86364 141\n",
      "2.929644 0.413968 2.515676 142\n",
      "1.9376723 0.28134367 1.6563286 143\n",
      "2.6922486 0.6565334 2.035715 144\n",
      "1.9791636 0.45340234 1.5257614 145\n",
      "2.152025 0.9074165 1.2446084 146\n",
      "0.3204223 0.091555744 0.22886653 147\n",
      "0.9301393 0.5224964 0.40764287 148\n",
      "2.5310488 0.7499316 1.7811171 149\n",
      "1.4191331 0.31196454 1.1071686 150\n",
      "0.6134449 0.0907348 0.52271014 151\n",
      "3.477666 1.8271614 1.6505044 152\n",
      "1.3889456 0.2799119 1.1090337 153\n",
      "3.9152803 1.7325372 2.1827433 154\n",
      "2.4860537 0.48427176 2.001782 155\n",
      "0.7254688 0.12941967 0.59604913 156\n",
      "0.97136104 0.28274986 0.6886112 157\n",
      "0.3207363 0.033356834 0.28737944 158\n",
      "2.3951817 1.1500245 1.2451571 159\n",
      "0.44456932 0.061965305 0.382604 160\n",
      "4.8647885 1.5041777 3.360611 161\n",
      "3.406311 0.5908533 2.8154578 162\n",
      "2.0609636 0.30036783 1.7605958 163\n",
      "1.1051365 0.17079237 0.9343442 164\n",
      "0.36664888 0.08902589 0.277623 165\n",
      "4.666505 0.8373012 3.8292036 166\n",
      "4.4565086 1.1030291 3.3534796 167\n",
      "1.9524027 0.33774868 1.6146541 168\n",
      "3.0691216 0.69521934 2.3739023 169\n",
      "2.0108328 0.35819918 1.6526335 170\n",
      "0.95415807 0.14910029 0.80505776 171\n",
      "2.0776696 0.849271 1.2283986 172\n",
      "1.2234647 0.30785173 0.91561294 173\n",
      "0.47991312 0.056326937 0.4235862 174\n",
      "9.4314995 1.3024075 8.129092 175\n",
      "4.549316 0.5829932 3.9663227 176\n",
      "2.5510168 0.35352978 2.197487 177\n",
      "5.1002893 1.2035455 3.896744 178\n",
      "3.174645 0.7144477 2.4601972 179\n",
      "2.7775528 0.6956775 2.0818753 180\n",
      "1.2113575 0.22295032 0.9884072 181\n",
      "0.6940057 0.15499154 0.5390142 182\n",
      "0.2785039 0.060982842 0.21752104 183\n",
      "1.237272 0.27199388 0.96527815 184\n",
      "2.3862088 0.5791027 1.807106 185\n",
      "5.045287 2.5949667 2.4503202 186\n",
      "1.6615219 1.017788 0.6437339 187\n",
      "0.7792784 0.08534061 0.6939378 188\n",
      "0.4218272 0.035673678 0.38615352 189\n",
      "1.1940526 0.12827758 1.065775 190\n",
      "1.8947295 1.4315511 0.4631784 191\n",
      "0.82070386 0.29280832 0.52789557 192\n",
      "0.5545911 0.23343514 0.32115597 193\n",
      "0.42030624 0.03539141 0.38491482 194\n",
      "0.1550245 0.015751973 0.13927253 195\n",
      "0.6474275 0.12968172 0.5177458 196\n",
      "0.24949643 0.03152896 0.21796747 197\n",
      "0.963598 0.26721582 0.69638216 198\n",
      "1.1140997 0.5223398 0.5917599 199\n",
      "0.12690215 0.019755242 0.10714691 200\n",
      "0.16293344 0.01842433 0.1445091 201\n",
      "1.0831947 0.7823159 0.30087882 202\n",
      "3.874508 2.5434418 1.3310663 203\n",
      "2.074214 1.1144117 0.95980215 204\n",
      "2.0431275 0.9888897 1.0542377 205\n",
      "1.66804 0.8303755 0.83766454 206\n",
      "2.182409 0.3351491 1.8472599 207\n",
      "1.2396964 0.145804 1.0938923 208\n",
      "0.51124185 0.08224494 0.4289969 209\n",
      "0.44909564 0.08996025 0.3591354 210\n",
      "3.8938842 0.80112803 3.0927563 211\n",
      "1.7936738 0.35542056 1.4382532 212\n",
      "1.4750551 0.5526425 0.9224126 213\n",
      "2.4796736 1.541638 0.93803567 214\n",
      "0.91224426 0.072495684 0.83974856 215\n",
      "1.0084096 0.13932708 0.86908257 216\n",
      "1.094258 0.3566428 0.7376151 217\n",
      "1.7654607 0.6513457 1.1141151 218\n",
      "0.45027104 0.061338224 0.38893282 219\n",
      "3.0889692 1.8566312 1.232338 220\n",
      "2.22445 1.2577107 0.9667395 221\n",
      "0.9632826 0.30041406 0.6628685 222\n",
      "0.7266048 0.11738081 0.609224 223\n",
      "0.368403 0.048633505 0.31976947 224\n",
      "1.4952096 0.83829737 0.6569122 225\n",
      "1.7738211 0.9100323 0.8637889 226\n",
      "0.37064207 0.03187312 0.33876896 227\n",
      "0.26076737 0.0179006 0.24286678 228\n",
      "5.6159368 1.4038767 4.21206 229\n",
      "3.123569 0.51974267 2.6038263 230\n",
      "2.4815745 0.6423214 1.8392531 231\n",
      "1.6817651 0.21075907 1.471006 232\n",
      "1.4269294 0.19489218 1.2320372 233\n",
      "0.9673843 0.19725895 0.7701253 234\n",
      "0.30548462 0.03152128 0.27396333 235\n",
      "0.78333217 0.13032937 0.6530028 236\n",
      "0.6561286 0.34506527 0.31106332 237\n",
      "1.0621654 0.44548827 0.6166771 238\n",
      "0.47879258 0.042216856 0.4365757 239\n",
      "0.7204707 0.10786801 0.6126027 240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9875968 0.5719928 1.415604 241\n",
      "0.9244403 0.05411005 0.8703303 242\n",
      "2.0844421 0.57086796 1.5135741 243\n",
      "1.1673054 0.22651735 0.940788 244\n",
      "0.5757694 0.0430241 0.5327453 245\n",
      "0.5593158 0.113426186 0.44588962 246\n",
      "0.5498834 0.09747253 0.45241088 247\n",
      "0.8822335 0.44606835 0.43616515 248\n",
      "0.39512855 0.029089643 0.36603892 249\n",
      "1.4019375 0.9124978 0.4894396 250\n",
      "0.26900637 0.034841154 0.2341652 251\n",
      "3.1481888 0.62486744 2.5233214 252\n",
      "0.9561177 0.06762793 0.8884898 253\n",
      "0.8145617 0.1342334 0.6803283 254\n",
      "0.30235058 0.055934336 0.24641624 255\n",
      "1.0550745 0.30459955 0.75047493 256\n",
      "0.6442038 0.040736586 0.6034672 257\n",
      "1.3347646 0.78486955 0.54989505 258\n",
      "0.3142415 0.046741065 0.26750043 259\n",
      "0.55433995 0.117305495 0.43703443 260\n",
      "0.54500574 0.0829049 0.46210086 261\n",
      "1.954236 1.2626966 0.6915394 262\n",
      "0.26841894 0.027208297 0.24121064 263\n",
      "2.0107918 1.4407723 0.57001936 264\n",
      "0.37950748 0.036421634 0.34308586 265\n",
      "0.20565383 0.02767225 0.17798159 266\n",
      "0.38743293 0.1424992 0.24493375 267\n",
      "0.75734043 0.47335806 0.28398237 268\n",
      "0.4547764 0.0733845 0.3813919 269\n",
      "0.79524714 0.5268211 0.26842606 270\n",
      "0.87832135 0.24824512 0.6300762 271\n",
      "2.3248615 1.7830852 0.5417764 272\n",
      "0.42923057 0.054520078 0.3747105 273\n",
      "1.5473533 0.398578 1.1487752 274\n",
      "0.763911 0.10979471 0.6541163 275\n",
      "1.630635 1.0133213 0.6173138 276\n",
      "0.41554797 0.033942744 0.38160524 277\n",
      "0.18237722 0.015813693 0.16656353 278\n",
      "0.40172416 0.13908152 0.26264265 279\n",
      "0.21138014 0.012916744 0.1984634 280\n",
      "0.59257805 0.07705411 0.51552397 281\n",
      "0.3295622 0.049829543 0.27973264 282\n",
      "0.6663797 0.36476955 0.30161017 283\n",
      "0.22971298 0.014565003 0.21514797 284\n",
      "0.16780084 0.013697221 0.15410362 285\n",
      "1.1027358 0.6442467 0.45848912 286\n",
      "8.099726 6.8288026 1.2709234 287\n",
      "0.37817052 0.106176846 0.27199367 288\n",
      "0.5910684 0.29456708 0.29650128 289\n",
      "0.1383885 0.014063494 0.12432501 290\n",
      "0.26829964 0.08781134 0.1804883 291\n",
      "1.3783154 0.34799618 1.0303193 292\n",
      "0.38019517 0.047250304 0.33294487 293\n",
      "0.11006049 0.01876164 0.09129885 294\n",
      "1.6795609 0.30512938 1.3744315 295\n",
      "0.6218393 0.052612938 0.5692263 296\n",
      "0.866259 0.3767879 0.48947108 297\n",
      "2.0963886 1.1267834 0.96960527 298\n",
      "0.16239312 0.0089167 0.15347642 299\n",
      "1.3332336 0.5417572 0.79147637 300\n",
      "1.1844683 0.7980364 0.3864318 301\n",
      "0.25591725 0.024946889 0.23097037 302\n",
      "0.17971496 0.0112713715 0.16844359 303\n",
      "0.10173988 0.013567689 0.0881722 304\n",
      "2.8813055 0.98655987 1.8947456 305\n",
      "0.7691832 0.18449275 0.58469045 306\n",
      "0.5740016 0.10775866 0.46624294 307\n",
      "0.2692072 0.033377156 0.23583005 308\n",
      "1.07254 0.344377 0.72816306 309\n",
      "0.4381328 0.08001395 0.35811883 310\n",
      "0.3162074 0.019467935 0.2967395 311\n",
      "1.2238059 0.8710929 0.35271302 312\n",
      "0.21710347 0.017481176 0.19962229 313\n",
      "0.17607747 0.023662789 0.15241468 314\n",
      "0.09618098 0.007823556 0.088357426 315\n",
      "1.0902443 0.685576 0.40466824 316\n",
      "0.21111362 0.018371752 0.19274187 317\n",
      "0.12079631 0.015023154 0.10577315 318\n",
      "2.0551608 0.76289874 1.292262 319\n",
      "0.6577605 0.3248618 0.33289868 320\n",
      "0.2355598 0.028535033 0.20702477 321\n",
      "0.13149196 0.00995263 0.121539325 322\n",
      "0.17315273 0.015753672 0.15739906 323\n",
      "1.6867707 1.0099882 0.6767824 324\n",
      "0.14440711 0.017695555 0.12671155 325\n",
      "0.64523673 0.26022264 0.38501412 326\n",
      "0.15655214 0.023701835 0.1328503 327\n",
      "0.13754725 0.0055931443 0.1319541 328\n",
      "1.6920933 0.22914386 1.4629494 329\n",
      "0.6974282 0.08217123 0.615257 330\n",
      "0.7943671 0.32785976 0.46650732 331\n",
      "0.2798865 0.028693175 0.25119334 332\n",
      "0.32788518 0.08020937 0.2476758 333\n",
      "0.11595237 0.0072964104 0.10865596 334\n",
      "0.9555193 0.4229072 0.53261214 335\n",
      "0.1594648 0.013776575 0.14568824 336\n",
      "0.067849234 0.008225569 0.059623666 337\n",
      "0.33766094 0.0696994 0.26796153 338\n",
      "0.13500063 0.0064777383 0.12852289 339\n",
      "5.7074485 1.4381366 4.269312 340\n",
      "2.8482008 0.30305454 2.5451462 341\n",
      "2.422334 0.4254069 1.996927 342\n",
      "1.8992493 0.24852298 1.6507263 343\n",
      "0.8577217 0.066891275 0.79083043 344\n",
      "0.9667342 0.19538198 0.7713522 345\n",
      "0.68265396 0.067634866 0.6150191 346\n",
      "2.9599974 0.6296491 2.3303483 347\n",
      "2.0100455 0.56003046 1.4500151 348\n",
      "1.4487249 0.26468173 1.1840432 349\n",
      "1.3259941 0.51303196 0.8129622 350\n",
      "1.0031235 0.25808144 0.7450421 351\n",
      "0.8710513 0.05430867 0.81674266 352\n",
      "0.9038162 0.20481193 0.6990043 353\n",
      "0.39846975 0.028807005 0.36966273 354\n",
      "0.2849512 0.05947131 0.22547989 355\n",
      "0.36355683 0.050385546 0.3131713 356\n",
      "0.25271043 0.014516647 0.23819378 357\n",
      "0.87046534 0.44852287 0.42194247 358\n",
      "0.8224736 0.48915336 0.33332023 359\n",
      "0.20827761 0.029902494 0.17837512 360\n",
      "0.27916697 0.035504688 0.24366228 361\n",
      "0.16621827 0.015140468 0.15107779 362\n",
      "0.13905054 0.01172577 0.12732477 363\n",
      "0.5544095 0.2140131 0.34039637 364\n",
      "0.2433703 0.014637574 0.22873272 365\n",
      "0.8694162 0.41962895 0.44978723 366\n",
      "0.33176908 0.08085162 0.25091746 367\n",
      "0.41936356 0.20355764 0.21580592 368\n",
      "1.8127902 1.079926 0.7328642 369\n",
      "0.24948677 0.021631828 0.22785495 370\n",
      "0.6090907 0.36523226 0.24385846 371\n",
      "0.21115233 0.05536766 0.15578467 372\n",
      "0.5431674 0.2728847 0.27028272 373\n",
      "5.9941854 2.9507287 3.0434568 374\n",
      "1.4729847 0.14380088 1.3291838 375\n",
      "2.1825979 0.4317614 1.7508364 376\n",
      "1.3319081 0.3227398 1.0091683 377\n",
      "1.2603338 0.33410043 0.9262333 378\n",
      "6.0787067 4.1539307 1.9247762 379\n",
      "0.530953 0.068045214 0.46290776 380\n",
      "0.6364174 0.07000451 0.56641287 381\n",
      "1.3753403 0.58882505 0.7865153 382\n",
      "0.34748086 0.023537597 0.32394326 383\n",
      "0.36828277 0.03709864 0.33118412 384\n",
      "0.86630195 0.46900734 0.3972946 385\n",
      "0.26961166 0.01857171 0.25103995 386\n",
      "1.9263687 0.9250886 1.0012802 387\n",
      "0.39918435 0.08236774 0.3168166 388\n",
      "0.4928488 0.09058767 0.40226114 389\n",
      "1.358101 0.90941733 0.44868365 390\n",
      "0.435366 0.032704603 0.4026614 391\n",
      "0.6617514 0.33657148 0.32517987 392\n",
      "1.1540778 0.45375356 0.70032424 393\n",
      "0.25506827 0.025215788 0.22985248 394\n",
      "0.8820462 0.5588855 0.32316074 395\n",
      "0.18760964 0.018523399 0.16908625 396\n",
      "0.18772744 0.019678673 0.16804877 397\n",
      "0.1803895 0.01863248 0.161757 398\n",
      "0.23204173 0.015149498 0.21689223 399\n",
      "0.14802083 0.031204829 0.11681601 400\n",
      "0.7207868 0.4733862 0.24740058 401\n",
      "0.26803094 0.055258818 0.21277212 402\n",
      "0.34769022 0.08333179 0.26435843 403\n",
      "0.18600914 0.02059456 0.16541459 404\n",
      "0.108405255 0.009941026 0.09846423 405\n",
      "0.08025839 0.013083011 0.06717538 406\n",
      "0.5694639 0.22613297 0.34333092 407\n",
      "0.5227791 0.22064435 0.30213472 408\n",
      "0.076617636 0.013678598 0.06293904 409\n",
      "0.09477977 0.022560978 0.07221879 410\n",
      "0.057201944 0.017381275 0.039820667 411\n",
      "0.042976838 0.00926849 0.03370835 412\n",
      "0.08617174 0.013483668 0.07268807 413\n",
      "0.047876675 0.015843611 0.032033063 414\n",
      "0.08523229 0.009322102 0.07591019 415\n",
      "0.3340252 0.14603755 0.18798767 416\n",
      "0.38520345 0.23513828 0.15006517 417\n",
      "0.07087931 0.0091904355 0.061688878 418\n",
      "0.43265122 0.3473635 0.08528771 419\n",
      "0.12012145 0.022550765 0.09757069 420\n",
      "0.18304867 0.07484073 0.10820793 421\n",
      "4.306983 4.079004 0.22797918 422\n",
      "0.23653498 0.06837142 0.16816355 423\n",
      "0.08432551 0.01179953 0.07252598 424\n",
      "7.5096393 5.8651423 1.644497 425\n",
      "0.4536306 0.042451333 0.41117927 426\n",
      "1.0257939 0.2808241 0.74496984 427\n",
      "0.6068235 0.20526016 0.40156335 428\n",
      "5.5267057 3.7932975 1.7334083 429\n",
      "0.47576934 0.10130786 0.37446147 430\n",
      "0.5381313 0.0893832 0.44874808 431\n",
      "0.3339017 0.035871718 0.29803 432\n",
      "0.19477995 0.020279873 0.17450008 433\n",
      "0.07687628 0.024913812 0.051962472 434\n",
      "1.4865639 0.9974443 0.48911965 435\n",
      "0.19180049 0.047106218 0.14469427 436\n",
      "0.17361158 0.017715067 0.15589651 437\n",
      "0.14007601 0.012589695 0.12748632 438\n",
      "0.11969486 0.013924928 0.10576993 439\n",
      "0.22235799 0.06252024 0.15983774 440\n",
      "0.34439564 0.21041858 0.13397704 441\n",
      "0.11914335 0.04989083 0.06925252 442\n",
      "0.23612547 0.1075724 0.12855306 443\n",
      "0.060847525 0.009308276 0.05153925 444\n",
      "1.1779296 0.72910607 0.4488235 445\n",
      "0.07196705 0.0063307113 0.06563634 446\n",
      "2.6142457 2.0061364 0.6081093 447\n",
      "0.17258573 0.019663054 0.15292268 448\n",
      "0.10812812 0.0074087954 0.100719325 449\n",
      "0.36723584 0.20425259 0.16298327 450\n",
      "0.061040953 0.008300569 0.052740384 451\n",
      "0.07188201 0.003603705 0.068278305 452\n",
      "0.13163826 0.04916417 0.0824741 453\n",
      "0.11311176 0.020299276 0.09281248 454\n",
      "0.085814014 0.012414853 0.073399164 455\n",
      "0.07193834 0.026798522 0.045139823 456\n",
      "0.32108516 0.12874465 0.19234052 457\n",
      "0.062157966 0.01540144 0.04675653 458\n",
      "0.053552635 0.006629616 0.04692302 459\n",
      "0.5499695 0.16251032 0.38745916 460\n",
      "0.16479841 0.0149654765 0.14983293 461\n",
      "0.068529814 0.021231052 0.047298767 462\n",
      "0.2874254 0.16230266 0.12512276 463\n",
      "0.10134634 0.025446454 0.075899884 464\n",
      "0.070602484 0.012768711 0.057833776 465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056195088 0.020494757 0.035700332 466\n",
      "0.096398905 0.022737022 0.073661886 467\n",
      "0.05140037 0.011340152 0.04006022 468\n",
      "0.18606348 0.05895582 0.12710766 469\n",
      "0.40890524 0.26259458 0.14631066 470\n",
      "5.059734 4.6661754 0.39355838 471\n",
      "0.16466534 0.023959642 0.1407057 472\n",
      "0.1209842 0.041183293 0.079800904 473\n",
      "0.07295567 0.016250951 0.056704715 474\n",
      "0.07192585 0.022146452 0.049779393 475\n",
      "0.69712687 0.47122994 0.22589695 476\n",
      "0.08842762 0.033516645 0.054910973 477\n",
      "0.46572337 0.23839195 0.22733141 478\n",
      "0.1261138 0.019390753 0.10672305 479\n",
      "0.61070806 0.40401077 0.20669727 480\n",
      "0.11702576 0.03491381 0.08211195 481\n",
      "0.06687567 0.0073445695 0.059531096 482\n",
      "0.44174522 0.304157 0.13758822 483\n",
      "0.085908845 0.0059168995 0.079991944 484\n",
      "0.09675239 0.02168144 0.075070955 485\n",
      "0.040427044 0.003725974 0.03670107 486\n",
      "0.040003702 0.0036581561 0.036345545 487\n",
      "0.060612787 0.010557175 0.050055612 488\n",
      "0.04020961 0.020155942 0.020053668 489\n",
      "0.24898514 0.091113985 0.15787116 490\n",
      "0.024818318 0.005932163 0.018886155 491\n",
      "0.94126296 0.6544236 0.28683937 492\n",
      "0.68698406 0.35241273 0.3345713 493\n",
      "0.73842704 0.52612203 0.21230498 494\n",
      "0.49849626 0.38214275 0.116353504 495\n",
      "0.0878173 0.0045292033 0.083288096 496\n",
      "0.088276565 0.038456704 0.04981986 497\n",
      "0.0894775 0.018006546 0.07147095 498\n",
      "1.7333086 0.82489234 0.90841615 499\n"
     ]
    }
   ],
   "source": [
    "#Train network\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t, x_interior, x_initial = sampler(nSim_t, nSim_x_interior, nSim_x_initial)\n",
    "    \n",
    "    for j in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_tnsr:t, x_interior_tnsr:x_interior, x_initial_tnsr:x_initial})\n",
    "        \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./FokkerPlack/FokkerPlanck'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './FokkerPlack/' + saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow1.7",
   "language": "python",
   "name": "tensorflow1.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
